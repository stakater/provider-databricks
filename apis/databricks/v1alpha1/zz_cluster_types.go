// SPDX-FileCopyrightText: 2023 The Crossplane Authors <https://crossplane.io>
//
// SPDX-License-Identifier: Apache-2.0

/*
Copyright 2022 Upbound Inc.
*/

// Code generated by upjet. DO NOT EDIT.

package v1alpha1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type AbfssInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type AbfssObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type AbfssParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

type AutoscaleInitParameters struct {

	// The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.
	MaxWorkers *float64 `json:"maxWorkers,omitempty" tf:"max_workers,omitempty"`

	// The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.
	MinWorkers *float64 `json:"minWorkers,omitempty" tf:"min_workers,omitempty"`
}

type AutoscaleObservation struct {

	// The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.
	MaxWorkers *float64 `json:"maxWorkers,omitempty" tf:"max_workers,omitempty"`

	// The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.
	MinWorkers *float64 `json:"minWorkers,omitempty" tf:"min_workers,omitempty"`
}

type AutoscaleParameters struct {

	// The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.
	// +kubebuilder:validation:Optional
	MaxWorkers *float64 `json:"maxWorkers,omitempty" tf:"max_workers,omitempty"`

	// The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.
	// +kubebuilder:validation:Optional
	MinWorkers *float64 `json:"minWorkers,omitempty" tf:"min_workers,omitempty"`
}

type AwsAttributesInitParameters struct {

	// Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.
	EBSVolumeCount *float64 `json:"ebsVolumeCount,omitempty" tf:"ebs_volume_count,omitempty"`

	EBSVolumeIops *float64 `json:"ebsVolumeIops,omitempty" tf:"ebs_volume_iops,omitempty"`

	// The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).
	EBSVolumeSize *float64 `json:"ebsVolumeSize,omitempty" tf:"ebs_volume_size,omitempty"`

	EBSVolumeThroughput *float64 `json:"ebsVolumeThroughput,omitempty" tf:"ebs_volume_throughput,omitempty"`

	// The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you're not picking Delta Optimized  node types.
	EBSVolumeType *string `json:"ebsVolumeType,omitempty" tf:"ebs_volume_type,omitempty"`

	// The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is 1 and could change in the future
	FirstOnDemand *float64 `json:"firstOnDemand,omitempty" tf:"first_on_demand,omitempty"`

	// Nodes for this cluster will only be placed on AWS instances with this instance profile.
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.
	SpotBidPricePercent *float64 `json:"spotBidPricePercent,omitempty" tf:"spot_bid_price_percent,omitempty"`

	// Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.
	ZoneID *string `json:"zoneId,omitempty" tf:"zone_id,omitempty"`
}

type AwsAttributesObservation struct {

	// Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.
	EBSVolumeCount *float64 `json:"ebsVolumeCount,omitempty" tf:"ebs_volume_count,omitempty"`

	EBSVolumeIops *float64 `json:"ebsVolumeIops,omitempty" tf:"ebs_volume_iops,omitempty"`

	// The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).
	EBSVolumeSize *float64 `json:"ebsVolumeSize,omitempty" tf:"ebs_volume_size,omitempty"`

	EBSVolumeThroughput *float64 `json:"ebsVolumeThroughput,omitempty" tf:"ebs_volume_throughput,omitempty"`

	// The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you're not picking Delta Optimized  node types.
	EBSVolumeType *string `json:"ebsVolumeType,omitempty" tf:"ebs_volume_type,omitempty"`

	// The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is 1 and could change in the future
	FirstOnDemand *float64 `json:"firstOnDemand,omitempty" tf:"first_on_demand,omitempty"`

	// Nodes for this cluster will only be placed on AWS instances with this instance profile.
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.
	SpotBidPricePercent *float64 `json:"spotBidPricePercent,omitempty" tf:"spot_bid_price_percent,omitempty"`

	// Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.
	ZoneID *string `json:"zoneId,omitempty" tf:"zone_id,omitempty"`
}

type AwsAttributesParameters struct {

	// Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future
	// +kubebuilder:validation:Optional
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.
	// +kubebuilder:validation:Optional
	EBSVolumeCount *float64 `json:"ebsVolumeCount,omitempty" tf:"ebs_volume_count,omitempty"`

	// +kubebuilder:validation:Optional
	EBSVolumeIops *float64 `json:"ebsVolumeIops,omitempty" tf:"ebs_volume_iops,omitempty"`

	// The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).
	// +kubebuilder:validation:Optional
	EBSVolumeSize *float64 `json:"ebsVolumeSize,omitempty" tf:"ebs_volume_size,omitempty"`

	// +kubebuilder:validation:Optional
	EBSVolumeThroughput *float64 `json:"ebsVolumeThroughput,omitempty" tf:"ebs_volume_throughput,omitempty"`

	// The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you're not picking Delta Optimized  node types.
	// +kubebuilder:validation:Optional
	EBSVolumeType *string `json:"ebsVolumeType,omitempty" tf:"ebs_volume_type,omitempty"`

	// The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is 1 and could change in the future
	// +kubebuilder:validation:Optional
	FirstOnDemand *float64 `json:"firstOnDemand,omitempty" tf:"first_on_demand,omitempty"`

	// Nodes for this cluster will only be placed on AWS instances with this instance profile.
	// +kubebuilder:validation:Optional
	InstanceProfileArn *string `json:"instanceProfileArn,omitempty" tf:"instance_profile_arn,omitempty"`

	// The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.
	// +kubebuilder:validation:Optional
	SpotBidPricePercent *float64 `json:"spotBidPricePercent,omitempty" tf:"spot_bid_price_percent,omitempty"`

	// Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.
	// +kubebuilder:validation:Optional
	ZoneID *string `json:"zoneId,omitempty" tf:"zone_id,omitempty"`
}

type AzureAttributesInitParameters struct {

	// Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.
	FirstOnDemand *float64 `json:"firstOnDemand,omitempty" tf:"first_on_demand,omitempty"`

	LogAnalyticsInfo []LogAnalyticsInfoInitParameters `json:"logAnalyticsInfo,omitempty" tf:"log_analytics_info,omitempty"`

	// The max price for Azure spot instances.  Use -1 to specify the lowest price.
	SpotBidMaxPrice *float64 `json:"spotBidMaxPrice,omitempty" tf:"spot_bid_max_price,omitempty"`
}

type AzureAttributesObservation struct {

	// Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.
	FirstOnDemand *float64 `json:"firstOnDemand,omitempty" tf:"first_on_demand,omitempty"`

	LogAnalyticsInfo []LogAnalyticsInfoObservation `json:"logAnalyticsInfo,omitempty" tf:"log_analytics_info,omitempty"`

	// The max price for Azure spot instances.  Use -1 to specify the lowest price.
	SpotBidMaxPrice *float64 `json:"spotBidMaxPrice,omitempty" tf:"spot_bid_max_price,omitempty"`
}

type AzureAttributesParameters struct {

	// Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.
	// +kubebuilder:validation:Optional
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.
	// +kubebuilder:validation:Optional
	FirstOnDemand *float64 `json:"firstOnDemand,omitempty" tf:"first_on_demand,omitempty"`

	// +kubebuilder:validation:Optional
	LogAnalyticsInfo []LogAnalyticsInfoParameters `json:"logAnalyticsInfo,omitempty" tf:"log_analytics_info,omitempty"`

	// The max price for Azure spot instances.  Use -1 to specify the lowest price.
	// +kubebuilder:validation:Optional
	SpotBidMaxPrice *float64 `json:"spotBidMaxPrice,omitempty" tf:"spot_bid_max_price,omitempty"`
}

type BasicAuthInitParameters struct {
	Username *string `json:"username,omitempty" tf:"username,omitempty"`
}

type BasicAuthObservation struct {
	Username *string `json:"username,omitempty" tf:"username,omitempty"`
}

type BasicAuthParameters struct {

	// +kubebuilder:validation:Required
	PasswordSecretRef v1.SecretKeySelector `json:"passwordSecretRef" tf:"-"`

	// +kubebuilder:validation:Optional
	Username *string `json:"username" tf:"username,omitempty"`
}

type ClientsInitParameters struct {

	// boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: true.
	Jobs *bool `json:"jobs,omitempty" tf:"jobs,omitempty"`

	// boolean flag defining if it's possible to run notebooks on this cluster. Default: true.
	Notebooks *bool `json:"notebooks,omitempty" tf:"notebooks,omitempty"`
}

type ClientsObservation struct {

	// boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: true.
	Jobs *bool `json:"jobs,omitempty" tf:"jobs,omitempty"`

	// boolean flag defining if it's possible to run notebooks on this cluster. Default: true.
	Notebooks *bool `json:"notebooks,omitempty" tf:"notebooks,omitempty"`
}

type ClientsParameters struct {

	// boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: true.
	// +kubebuilder:validation:Optional
	Jobs *bool `json:"jobs,omitempty" tf:"jobs,omitempty"`

	// boolean flag defining if it's possible to run notebooks on this cluster. Default: true.
	// +kubebuilder:validation:Optional
	Notebooks *bool `json:"notebooks,omitempty" tf:"notebooks,omitempty"`
}

type ClusterInitParameters struct {

	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues *bool `json:"applyPolicyDefaultValues,omitempty" tf:"apply_policy_default_values,omitempty"`

	Autoscale []AutoscaleInitParameters `json:"autoscale,omitempty" tf:"autoscale,omitempty"`

	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters.
	AutoterminationMinutes *float64 `json:"autoterminationMinutes,omitempty" tf:"autotermination_minutes,omitempty"`

	AwsAttributes []AwsAttributesInitParameters `json:"awsAttributes,omitempty" tf:"aws_attributes,omitempty"`

	AzureAttributes []AzureAttributesInitParameters `json:"azureAttributes,omitempty" tf:"azure_attributes,omitempty"`

	// Canonical unique identifier for the cluster.
	ClusterID *string `json:"clusterId,omitempty" tf:"cluster_id,omitempty"`

	ClusterLogConf []ClusterLogConfInitParameters `json:"clusterLogConf,omitempty" tf:"cluster_log_conf,omitempty"`

	ClusterMountInfo []ClusterMountInfoInitParameters `json:"clusterMountInfo,omitempty" tf:"cluster_mount_info,omitempty"`

	// Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName *string `json:"clusterName,omitempty" tf:"cluster_name,omitempty"`

	// Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated.
	CustomTags map[string]*string `json:"customTags,omitempty" tf:"custom_tags,omitempty"`

	// Select the security features of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed Access Mode and USER_ISOLATION has been renamed Shared, but use these terms here.
	DataSecurityMode *string `json:"dataSecurityMode,omitempty" tf:"data_security_mode,omitempty"`

	DockerImage []DockerImageInitParameters `json:"dockerImage,omitempty" tf:"docker_image,omitempty"`

	// similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
	DriverInstancePoolID *string `json:"driverInstancePoolId,omitempty" tf:"driver_instance_pool_id,omitempty"`

	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above.
	DriverNodeTypeID *string `json:"driverNodeTypeId,omitempty" tf:"driver_node_type_id,omitempty"`

	// If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page.
	EnableElasticDisk *bool `json:"enableElasticDisk,omitempty" tf:"enable_elastic_disk,omitempty"`

	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.
	EnableLocalDiskEncryption *bool `json:"enableLocalDiskEncryption,omitempty" tf:"enable_local_disk_encryption,omitempty"`

	GCPAttributes []GCPAttributesInitParameters `json:"gcpAttributes,omitempty" tf:"gcp_attributes,omitempty"`

	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken *string `json:"idempotencyToken,omitempty" tf:"idempotency_token,omitempty"`

	InitScripts []InitScriptsInitParameters `json:"initScripts,omitempty" tf:"init_scripts,omitempty"`

	// required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolID *string `json:"instancePoolId,omitempty" tf:"instance_pool_id,omitempty"`

	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned *bool `json:"isPinned,omitempty" tf:"is_pinned,omitempty"`

	Library []LibraryInitParameters `json:"library,omitempty" tf:"library,omitempty"`

	// Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed.
	NodeTypeID *string `json:"nodeTypeId,omitempty" tf:"node_type_id,omitempty"`

	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes.
	NumWorkers *float64 `json:"numWorkers,omitempty" tf:"num_workers,omitempty"`

	// Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.
	PolicyID *string `json:"policyId,omitempty" tf:"policy_id,omitempty"`

	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD.
	RuntimeEngine *string `json:"runtimeEngine,omitempty" tf:"runtime_engine,omitempty"`

	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SSHPublicKeys []*string `json:"sshPublicKeys,omitempty" tf:"ssh_public_keys,omitempty"`

	// The optional user name of the user to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName *string `json:"singleUserName,omitempty" tf:"single_user_name,omitempty"`

	// Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration.
	SparkConf map[string]*string `json:"sparkConf,omitempty" tf:"spark_conf,omitempty"`

	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars map[string]*string `json:"sparkEnvVars,omitempty" tf:"spark_env_vars,omitempty"`

	// Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion *string `json:"sparkVersion,omitempty" tf:"spark_version,omitempty"`

	WorkloadType []WorkloadTypeInitParameters `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ClusterLogConfInitParameters struct {
	Dbfs []DbfsInitParameters `json:"dbfs,omitempty" tf:"dbfs,omitempty"`

	S3 []S3InitParameters `json:"s3,omitempty" tf:"s3,omitempty"`
}

type ClusterLogConfObservation struct {
	Dbfs []DbfsObservation `json:"dbfs,omitempty" tf:"dbfs,omitempty"`

	S3 []S3Observation `json:"s3,omitempty" tf:"s3,omitempty"`
}

type ClusterLogConfParameters struct {

	// +kubebuilder:validation:Optional
	Dbfs []DbfsParameters `json:"dbfs,omitempty" tf:"dbfs,omitempty"`

	// +kubebuilder:validation:Optional
	S3 []S3Parameters `json:"s3,omitempty" tf:"s3,omitempty"`
}

type ClusterMountInfoInitParameters struct {

	// path inside the Spark container.
	LocalMountDirPath *string `json:"localMountDirPath,omitempty" tf:"local_mount_dir_path,omitempty"`

	// block specifying connection. It consists of:
	NetworkFilesystemInfo []NetworkFilesystemInfoInitParameters `json:"networkFilesystemInfo,omitempty" tf:"network_filesystem_info,omitempty"`

	// string specifying path to mount on the remote service.
	RemoteMountDirPath *string `json:"remoteMountDirPath,omitempty" tf:"remote_mount_dir_path,omitempty"`
}

type ClusterMountInfoObservation struct {

	// path inside the Spark container.
	LocalMountDirPath *string `json:"localMountDirPath,omitempty" tf:"local_mount_dir_path,omitempty"`

	// block specifying connection. It consists of:
	NetworkFilesystemInfo []NetworkFilesystemInfoObservation `json:"networkFilesystemInfo,omitempty" tf:"network_filesystem_info,omitempty"`

	// string specifying path to mount on the remote service.
	RemoteMountDirPath *string `json:"remoteMountDirPath,omitempty" tf:"remote_mount_dir_path,omitempty"`
}

type ClusterMountInfoParameters struct {

	// path inside the Spark container.
	// +kubebuilder:validation:Optional
	LocalMountDirPath *string `json:"localMountDirPath" tf:"local_mount_dir_path,omitempty"`

	// block specifying connection. It consists of:
	// +kubebuilder:validation:Optional
	NetworkFilesystemInfo []NetworkFilesystemInfoParameters `json:"networkFilesystemInfo" tf:"network_filesystem_info,omitempty"`

	// string specifying path to mount on the remote service.
	// +kubebuilder:validation:Optional
	RemoteMountDirPath *string `json:"remoteMountDirPath,omitempty" tf:"remote_mount_dir_path,omitempty"`
}

type ClusterObservation struct {

	// Whether to use policy default values for missing cluster attributes.
	ApplyPolicyDefaultValues *bool `json:"applyPolicyDefaultValues,omitempty" tf:"apply_policy_default_values,omitempty"`

	Autoscale []AutoscaleObservation `json:"autoscale,omitempty" tf:"autoscale,omitempty"`

	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters.
	AutoterminationMinutes *float64 `json:"autoterminationMinutes,omitempty" tf:"autotermination_minutes,omitempty"`

	AwsAttributes []AwsAttributesObservation `json:"awsAttributes,omitempty" tf:"aws_attributes,omitempty"`

	AzureAttributes []AzureAttributesObservation `json:"azureAttributes,omitempty" tf:"azure_attributes,omitempty"`

	// Canonical unique identifier for the cluster.
	ClusterID *string `json:"clusterId,omitempty" tf:"cluster_id,omitempty"`

	ClusterLogConf []ClusterLogConfObservation `json:"clusterLogConf,omitempty" tf:"cluster_log_conf,omitempty"`

	ClusterMountInfo []ClusterMountInfoObservation `json:"clusterMountInfo,omitempty" tf:"cluster_mount_info,omitempty"`

	// Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.
	ClusterName *string `json:"clusterName,omitempty" tf:"cluster_name,omitempty"`

	ClusterSource *string `json:"clusterSource,omitempty" tf:"cluster_source,omitempty"`

	// Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated.
	CustomTags map[string]*string `json:"customTags,omitempty" tf:"custom_tags,omitempty"`

	// Select the security features of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed Access Mode and USER_ISOLATION has been renamed Shared, but use these terms here.
	DataSecurityMode *string `json:"dataSecurityMode,omitempty" tf:"data_security_mode,omitempty"`

	// (map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: , and any workspace and pool tags.
	DefaultTags map[string]*string `json:"defaultTags,omitempty" tf:"default_tags,omitempty"`

	DockerImage []DockerImageObservation `json:"dockerImage,omitempty" tf:"docker_image,omitempty"`

	// similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
	DriverInstancePoolID *string `json:"driverInstancePoolId,omitempty" tf:"driver_instance_pool_id,omitempty"`

	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above.
	DriverNodeTypeID *string `json:"driverNodeTypeId,omitempty" tf:"driver_node_type_id,omitempty"`

	// If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page.
	EnableElasticDisk *bool `json:"enableElasticDisk,omitempty" tf:"enable_elastic_disk,omitempty"`

	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.
	EnableLocalDiskEncryption *bool `json:"enableLocalDiskEncryption,omitempty" tf:"enable_local_disk_encryption,omitempty"`

	GCPAttributes []GCPAttributesObservation `json:"gcpAttributes,omitempty" tf:"gcp_attributes,omitempty"`

	// Canonical unique identifier for the cluster.
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	IdempotencyToken *string `json:"idempotencyToken,omitempty" tf:"idempotency_token,omitempty"`

	InitScripts []InitScriptsObservation `json:"initScripts,omitempty" tf:"init_scripts,omitempty"`

	// required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
	InstancePoolID *string `json:"instancePoolId,omitempty" tf:"instance_pool_id,omitempty"`

	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	IsPinned *bool `json:"isPinned,omitempty" tf:"is_pinned,omitempty"`

	Library []LibraryObservation `json:"library,omitempty" tf:"library,omitempty"`

	// Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed.
	NodeTypeID *string `json:"nodeTypeId,omitempty" tf:"node_type_id,omitempty"`

	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes.
	NumWorkers *float64 `json:"numWorkers,omitempty" tf:"num_workers,omitempty"`

	// Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.
	PolicyID *string `json:"policyId,omitempty" tf:"policy_id,omitempty"`

	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD.
	RuntimeEngine *string `json:"runtimeEngine,omitempty" tf:"runtime_engine,omitempty"`

	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	SSHPublicKeys []*string `json:"sshPublicKeys,omitempty" tf:"ssh_public_keys,omitempty"`

	// The optional user name of the user to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	SingleUserName *string `json:"singleUserName,omitempty" tf:"single_user_name,omitempty"`

	// Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration.
	SparkConf map[string]*string `json:"sparkConf,omitempty" tf:"spark_conf,omitempty"`

	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	SparkEnvVars map[string]*string `json:"sparkEnvVars,omitempty" tf:"spark_env_vars,omitempty"`

	// Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	SparkVersion *string `json:"sparkVersion,omitempty" tf:"spark_version,omitempty"`

	// (string) State of the cluster.
	State *string `json:"state,omitempty" tf:"state,omitempty"`

	// URL for the Docker image
	URL *string `json:"url,omitempty" tf:"url,omitempty"`

	WorkloadType []WorkloadTypeObservation `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type ClusterParameters struct {

	// Whether to use policy default values for missing cluster attributes.
	// +kubebuilder:validation:Optional
	ApplyPolicyDefaultValues *bool `json:"applyPolicyDefaultValues,omitempty" tf:"apply_policy_default_values,omitempty"`

	// +kubebuilder:validation:Optional
	Autoscale []AutoscaleParameters `json:"autoscale,omitempty" tf:"autoscale,omitempty"`

	// Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters.
	// +kubebuilder:validation:Optional
	AutoterminationMinutes *float64 `json:"autoterminationMinutes,omitempty" tf:"autotermination_minutes,omitempty"`

	// +kubebuilder:validation:Optional
	AwsAttributes []AwsAttributesParameters `json:"awsAttributes,omitempty" tf:"aws_attributes,omitempty"`

	// +kubebuilder:validation:Optional
	AzureAttributes []AzureAttributesParameters `json:"azureAttributes,omitempty" tf:"azure_attributes,omitempty"`

	// Canonical unique identifier for the cluster.
	// +kubebuilder:validation:Optional
	ClusterID *string `json:"clusterId,omitempty" tf:"cluster_id,omitempty"`

	// +kubebuilder:validation:Optional
	ClusterLogConf []ClusterLogConfParameters `json:"clusterLogConf,omitempty" tf:"cluster_log_conf,omitempty"`

	// +kubebuilder:validation:Optional
	ClusterMountInfo []ClusterMountInfoParameters `json:"clusterMountInfo,omitempty" tf:"cluster_mount_info,omitempty"`

	// Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.
	// +kubebuilder:validation:Optional
	ClusterName *string `json:"clusterName,omitempty" tf:"cluster_name,omitempty"`

	// Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated.
	// +kubebuilder:validation:Optional
	CustomTags map[string]*string `json:"customTags,omitempty" tf:"custom_tags,omitempty"`

	// Select the security features of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed Access Mode and USER_ISOLATION has been renamed Shared, but use these terms here.
	// +kubebuilder:validation:Optional
	DataSecurityMode *string `json:"dataSecurityMode,omitempty" tf:"data_security_mode,omitempty"`

	// +kubebuilder:validation:Optional
	DockerImage []DockerImageParameters `json:"dockerImage,omitempty" tf:"docker_image,omitempty"`

	// similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
	// +kubebuilder:validation:Optional
	DriverInstancePoolID *string `json:"driverInstancePoolId,omitempty" tf:"driver_instance_pool_id,omitempty"`

	// The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above.
	// +kubebuilder:validation:Optional
	DriverNodeTypeID *string `json:"driverNodeTypeId,omitempty" tf:"driver_node_type_id,omitempty"`

	// If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page.
	// +kubebuilder:validation:Optional
	EnableElasticDisk *bool `json:"enableElasticDisk,omitempty" tf:"enable_elastic_disk,omitempty"`

	// Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.
	// +kubebuilder:validation:Optional
	EnableLocalDiskEncryption *bool `json:"enableLocalDiskEncryption,omitempty" tf:"enable_local_disk_encryption,omitempty"`

	// +kubebuilder:validation:Optional
	GCPAttributes []GCPAttributesParameters `json:"gcpAttributes,omitempty" tf:"gcp_attributes,omitempty"`

	// An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
	// +kubebuilder:validation:Optional
	IdempotencyToken *string `json:"idempotencyToken,omitempty" tf:"idempotency_token,omitempty"`

	// +kubebuilder:validation:Optional
	InitScripts []InitScriptsParameters `json:"initScripts,omitempty" tf:"init_scripts,omitempty"`

	// required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
	// +kubebuilder:validation:Optional
	InstancePoolID *string `json:"instancePoolId,omitempty" tf:"instance_pool_id,omitempty"`

	// boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
	// +kubebuilder:validation:Optional
	IsPinned *bool `json:"isPinned,omitempty" tf:"is_pinned,omitempty"`

	// +kubebuilder:validation:Optional
	Library []LibraryParameters `json:"library,omitempty" tf:"library,omitempty"`

	// Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed.
	// +kubebuilder:validation:Optional
	NodeTypeID *string `json:"nodeTypeId,omitempty" tf:"node_type_id,omitempty"`

	// Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes.
	// +kubebuilder:validation:Optional
	NumWorkers *float64 `json:"numWorkers,omitempty" tf:"num_workers,omitempty"`

	// Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.
	// +kubebuilder:validation:Optional
	PolicyID *string `json:"policyId,omitempty" tf:"policy_id,omitempty"`

	// The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD.
	// +kubebuilder:validation:Optional
	RuntimeEngine *string `json:"runtimeEngine,omitempty" tf:"runtime_engine,omitempty"`

	// SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
	// +kubebuilder:validation:Optional
	SSHPublicKeys []*string `json:"sshPublicKeys,omitempty" tf:"ssh_public_keys,omitempty"`

	// The optional user name of the user to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
	// +kubebuilder:validation:Optional
	SingleUserName *string `json:"singleUserName,omitempty" tf:"single_user_name,omitempty"`

	// Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration.
	// +kubebuilder:validation:Optional
	SparkConf map[string]*string `json:"sparkConf,omitempty" tf:"spark_conf,omitempty"`

	// Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
	// +kubebuilder:validation:Optional
	SparkEnvVars map[string]*string `json:"sparkEnvVars,omitempty" tf:"spark_env_vars,omitempty"`

	// Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
	// +kubebuilder:validation:Optional
	SparkVersion *string `json:"sparkVersion,omitempty" tf:"spark_version,omitempty"`

	// +kubebuilder:validation:Optional
	WorkloadType []WorkloadTypeParameters `json:"workloadType,omitempty" tf:"workload_type,omitempty"`
}

type CranInitParameters struct {
	Package *string `json:"package,omitempty" tf:"package,omitempty"`

	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type CranObservation struct {
	Package *string `json:"package,omitempty" tf:"package,omitempty"`

	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type CranParameters struct {

	// +kubebuilder:validation:Optional
	Package *string `json:"package" tf:"package,omitempty"`

	// +kubebuilder:validation:Optional
	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type DbfsInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type DbfsObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type DbfsParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

type DockerImageInitParameters struct {

	// basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.
	BasicAuth []BasicAuthInitParameters `json:"basicAuth,omitempty" tf:"basic_auth,omitempty"`

	// URL for the Docker image
	URL *string `json:"url,omitempty" tf:"url,omitempty"`
}

type DockerImageObservation struct {

	// basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.
	BasicAuth []BasicAuthObservation `json:"basicAuth,omitempty" tf:"basic_auth,omitempty"`

	// URL for the Docker image
	URL *string `json:"url,omitempty" tf:"url,omitempty"`
}

type DockerImageParameters struct {

	// basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.
	// +kubebuilder:validation:Optional
	BasicAuth []BasicAuthParameters `json:"basicAuth,omitempty" tf:"basic_auth,omitempty"`

	// URL for the Docker image
	// +kubebuilder:validation:Optional
	URL *string `json:"url" tf:"url,omitempty"`
}

type FileInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type FileObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type FileParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

type GCPAttributesInitParameters struct {

	// , and will be removed soon.
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// Boot disk size in GB
	BootDiskSize *float64 `json:"bootDiskSize,omitempty" tf:"boot_disk_size,omitempty"`

	// Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.
	GoogleServiceAccount *string `json:"googleServiceAccount,omitempty" tf:"google_service_account,omitempty"`

	// Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
	LocalSsdCount *float64 `json:"localSsdCount,omitempty" tf:"local_ssd_count,omitempty"`

	// if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of
	UsePreemptibleExecutors *bool `json:"usePreemptibleExecutors,omitempty" tf:"use_preemptible_executors,omitempty"`

	// Identifier for the availability zone in which the cluster resides. This can be one of the following:
	ZoneID *string `json:"zoneId,omitempty" tf:"zone_id,omitempty"`
}

type GCPAttributesObservation struct {

	// , and will be removed soon.
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// Boot disk size in GB
	BootDiskSize *float64 `json:"bootDiskSize,omitempty" tf:"boot_disk_size,omitempty"`

	// Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.
	GoogleServiceAccount *string `json:"googleServiceAccount,omitempty" tf:"google_service_account,omitempty"`

	// Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
	LocalSsdCount *float64 `json:"localSsdCount,omitempty" tf:"local_ssd_count,omitempty"`

	// if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of
	UsePreemptibleExecutors *bool `json:"usePreemptibleExecutors,omitempty" tf:"use_preemptible_executors,omitempty"`

	// Identifier for the availability zone in which the cluster resides. This can be one of the following:
	ZoneID *string `json:"zoneId,omitempty" tf:"zone_id,omitempty"`
}

type GCPAttributesParameters struct {

	// , and will be removed soon.
	// +kubebuilder:validation:Optional
	Availability *string `json:"availability,omitempty" tf:"availability,omitempty"`

	// Boot disk size in GB
	// +kubebuilder:validation:Optional
	BootDiskSize *float64 `json:"bootDiskSize,omitempty" tf:"boot_disk_size,omitempty"`

	// Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.
	// +kubebuilder:validation:Optional
	GoogleServiceAccount *string `json:"googleServiceAccount,omitempty" tf:"google_service_account,omitempty"`

	// Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
	// +kubebuilder:validation:Optional
	LocalSsdCount *float64 `json:"localSsdCount,omitempty" tf:"local_ssd_count,omitempty"`

	// if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of
	// +kubebuilder:validation:Optional
	UsePreemptibleExecutors *bool `json:"usePreemptibleExecutors,omitempty" tf:"use_preemptible_executors,omitempty"`

	// Identifier for the availability zone in which the cluster resides. This can be one of the following:
	// +kubebuilder:validation:Optional
	ZoneID *string `json:"zoneId,omitempty" tf:"zone_id,omitempty"`
}

type GcsInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type GcsObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type GcsParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

type InitScriptsDbfsInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type InitScriptsDbfsObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type InitScriptsDbfsParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

type InitScriptsInitParameters struct {
	Abfss []AbfssInitParameters `json:"abfss,omitempty" tf:"abfss,omitempty"`

	Dbfs []InitScriptsDbfsInitParameters `json:"dbfs,omitempty" tf:"dbfs,omitempty"`

	File []FileInitParameters `json:"file,omitempty" tf:"file,omitempty"`

	Gcs []GcsInitParameters `json:"gcs,omitempty" tf:"gcs,omitempty"`

	S3 []InitScriptsS3InitParameters `json:"s3,omitempty" tf:"s3,omitempty"`

	Volumes []VolumesInitParameters `json:"volumes,omitempty" tf:"volumes,omitempty"`

	Workspace []WorkspaceInitParameters `json:"workspace,omitempty" tf:"workspace,omitempty"`
}

type InitScriptsObservation struct {
	Abfss []AbfssObservation `json:"abfss,omitempty" tf:"abfss,omitempty"`

	Dbfs []InitScriptsDbfsObservation `json:"dbfs,omitempty" tf:"dbfs,omitempty"`

	File []FileObservation `json:"file,omitempty" tf:"file,omitempty"`

	Gcs []GcsObservation `json:"gcs,omitempty" tf:"gcs,omitempty"`

	S3 []InitScriptsS3Observation `json:"s3,omitempty" tf:"s3,omitempty"`

	Volumes []VolumesObservation `json:"volumes,omitempty" tf:"volumes,omitempty"`

	Workspace []WorkspaceObservation `json:"workspace,omitempty" tf:"workspace,omitempty"`
}

type InitScriptsParameters struct {

	// +kubebuilder:validation:Optional
	Abfss []AbfssParameters `json:"abfss,omitempty" tf:"abfss,omitempty"`

	// +kubebuilder:validation:Optional
	Dbfs []InitScriptsDbfsParameters `json:"dbfs,omitempty" tf:"dbfs,omitempty"`

	// +kubebuilder:validation:Optional
	File []FileParameters `json:"file,omitempty" tf:"file,omitempty"`

	// +kubebuilder:validation:Optional
	Gcs []GcsParameters `json:"gcs,omitempty" tf:"gcs,omitempty"`

	// +kubebuilder:validation:Optional
	S3 []InitScriptsS3Parameters `json:"s3,omitempty" tf:"s3,omitempty"`

	// +kubebuilder:validation:Optional
	Volumes []VolumesParameters `json:"volumes,omitempty" tf:"volumes,omitempty"`

	// +kubebuilder:validation:Optional
	Workspace []WorkspaceParameters `json:"workspace,omitempty" tf:"workspace,omitempty"`
}

type InitScriptsS3InitParameters struct {

	// Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.
	CannedACL *string `json:"cannedAcl,omitempty" tf:"canned_acl,omitempty"`

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`

	// Enable server-side encryption, false by default.
	EnableEncryption *bool `json:"enableEncryption,omitempty" tf:"enable_encryption,omitempty"`

	// The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.
	EncryptionType *string `json:"encryptionType,omitempty" tf:"encryption_type,omitempty"`

	// S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint,omitempty"`

	// KMS key used if encryption is enabled and encryption type is set to sse-kms.
	KMSKey *string `json:"kmsKey,omitempty" tf:"kms_key,omitempty"`

	// S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type InitScriptsS3Observation struct {

	// Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.
	CannedACL *string `json:"cannedAcl,omitempty" tf:"canned_acl,omitempty"`

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`

	// Enable server-side encryption, false by default.
	EnableEncryption *bool `json:"enableEncryption,omitempty" tf:"enable_encryption,omitempty"`

	// The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.
	EncryptionType *string `json:"encryptionType,omitempty" tf:"encryption_type,omitempty"`

	// S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint,omitempty"`

	// KMS key used if encryption is enabled and encryption type is set to sse-kms.
	KMSKey *string `json:"kmsKey,omitempty" tf:"kms_key,omitempty"`

	// S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type InitScriptsS3Parameters struct {

	// Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.
	// +kubebuilder:validation:Optional
	CannedACL *string `json:"cannedAcl,omitempty" tf:"canned_acl,omitempty"`

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`

	// Enable server-side encryption, false by default.
	// +kubebuilder:validation:Optional
	EnableEncryption *bool `json:"enableEncryption,omitempty" tf:"enable_encryption,omitempty"`

	// The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.
	// +kubebuilder:validation:Optional
	EncryptionType *string `json:"encryptionType,omitempty" tf:"encryption_type,omitempty"`

	// S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.
	// +kubebuilder:validation:Optional
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint,omitempty"`

	// KMS key used if encryption is enabled and encryption type is set to sse-kms.
	// +kubebuilder:validation:Optional
	KMSKey *string `json:"kmsKey,omitempty" tf:"kms_key,omitempty"`

	// S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.
	// +kubebuilder:validation:Optional
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type LibraryInitParameters struct {
	Cran []CranInitParameters `json:"cran,omitempty" tf:"cran,omitempty"`

	Egg *string `json:"egg,omitempty" tf:"egg,omitempty"`

	Jar *string `json:"jar,omitempty" tf:"jar,omitempty"`

	Maven []MavenInitParameters `json:"maven,omitempty" tf:"maven,omitempty"`

	Pypi []PypiInitParameters `json:"pypi,omitempty" tf:"pypi,omitempty"`

	Whl *string `json:"whl,omitempty" tf:"whl,omitempty"`
}

type LibraryObservation struct {
	Cran []CranObservation `json:"cran,omitempty" tf:"cran,omitempty"`

	Egg *string `json:"egg,omitempty" tf:"egg,omitempty"`

	Jar *string `json:"jar,omitempty" tf:"jar,omitempty"`

	Maven []MavenObservation `json:"maven,omitempty" tf:"maven,omitempty"`

	Pypi []PypiObservation `json:"pypi,omitempty" tf:"pypi,omitempty"`

	Whl *string `json:"whl,omitempty" tf:"whl,omitempty"`
}

type LibraryParameters struct {

	// +kubebuilder:validation:Optional
	Cran []CranParameters `json:"cran,omitempty" tf:"cran,omitempty"`

	// +kubebuilder:validation:Optional
	Egg *string `json:"egg,omitempty" tf:"egg,omitempty"`

	// +kubebuilder:validation:Optional
	Jar *string `json:"jar,omitempty" tf:"jar,omitempty"`

	// +kubebuilder:validation:Optional
	Maven []MavenParameters `json:"maven,omitempty" tf:"maven,omitempty"`

	// +kubebuilder:validation:Optional
	Pypi []PypiParameters `json:"pypi,omitempty" tf:"pypi,omitempty"`

	// +kubebuilder:validation:Optional
	Whl *string `json:"whl,omitempty" tf:"whl,omitempty"`
}

type LogAnalyticsInfoInitParameters struct {
	LogAnalyticsPrimaryKey *string `json:"logAnalyticsPrimaryKey,omitempty" tf:"log_analytics_primary_key,omitempty"`

	// Canonical unique identifier for the cluster.
	LogAnalyticsWorkspaceID *string `json:"logAnalyticsWorkspaceId,omitempty" tf:"log_analytics_workspace_id,omitempty"`
}

type LogAnalyticsInfoObservation struct {
	LogAnalyticsPrimaryKey *string `json:"logAnalyticsPrimaryKey,omitempty" tf:"log_analytics_primary_key,omitempty"`

	// Canonical unique identifier for the cluster.
	LogAnalyticsWorkspaceID *string `json:"logAnalyticsWorkspaceId,omitempty" tf:"log_analytics_workspace_id,omitempty"`
}

type LogAnalyticsInfoParameters struct {

	// +kubebuilder:validation:Optional
	LogAnalyticsPrimaryKey *string `json:"logAnalyticsPrimaryKey,omitempty" tf:"log_analytics_primary_key,omitempty"`

	// Canonical unique identifier for the cluster.
	// +kubebuilder:validation:Optional
	LogAnalyticsWorkspaceID *string `json:"logAnalyticsWorkspaceId,omitempty" tf:"log_analytics_workspace_id,omitempty"`
}

type MavenInitParameters struct {
	Coordinates *string `json:"coordinates,omitempty" tf:"coordinates,omitempty"`

	Exclusions []*string `json:"exclusions,omitempty" tf:"exclusions,omitempty"`

	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type MavenObservation struct {
	Coordinates *string `json:"coordinates,omitempty" tf:"coordinates,omitempty"`

	Exclusions []*string `json:"exclusions,omitempty" tf:"exclusions,omitempty"`

	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type MavenParameters struct {

	// +kubebuilder:validation:Optional
	Coordinates *string `json:"coordinates" tf:"coordinates,omitempty"`

	// +kubebuilder:validation:Optional
	Exclusions []*string `json:"exclusions,omitempty" tf:"exclusions,omitempty"`

	// +kubebuilder:validation:Optional
	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type NetworkFilesystemInfoInitParameters struct {

	// string that will be passed as options passed to the mount command.
	MountOptions *string `json:"mountOptions,omitempty" tf:"mount_options,omitempty"`

	// host name.
	ServerAddress *string `json:"serverAddress,omitempty" tf:"server_address,omitempty"`
}

type NetworkFilesystemInfoObservation struct {

	// string that will be passed as options passed to the mount command.
	MountOptions *string `json:"mountOptions,omitempty" tf:"mount_options,omitempty"`

	// host name.
	ServerAddress *string `json:"serverAddress,omitempty" tf:"server_address,omitempty"`
}

type NetworkFilesystemInfoParameters struct {

	// string that will be passed as options passed to the mount command.
	// +kubebuilder:validation:Optional
	MountOptions *string `json:"mountOptions,omitempty" tf:"mount_options,omitempty"`

	// host name.
	// +kubebuilder:validation:Optional
	ServerAddress *string `json:"serverAddress" tf:"server_address,omitempty"`
}

type PypiInitParameters struct {
	Package *string `json:"package,omitempty" tf:"package,omitempty"`

	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type PypiObservation struct {
	Package *string `json:"package,omitempty" tf:"package,omitempty"`

	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type PypiParameters struct {

	// +kubebuilder:validation:Optional
	Package *string `json:"package" tf:"package,omitempty"`

	// +kubebuilder:validation:Optional
	Repo *string `json:"repo,omitempty" tf:"repo,omitempty"`
}

type S3InitParameters struct {

	// Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.
	CannedACL *string `json:"cannedAcl,omitempty" tf:"canned_acl,omitempty"`

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`

	// Enable server-side encryption, false by default.
	EnableEncryption *bool `json:"enableEncryption,omitempty" tf:"enable_encryption,omitempty"`

	// The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.
	EncryptionType *string `json:"encryptionType,omitempty" tf:"encryption_type,omitempty"`

	// S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint,omitempty"`

	// KMS key used if encryption is enabled and encryption type is set to sse-kms.
	KMSKey *string `json:"kmsKey,omitempty" tf:"kms_key,omitempty"`

	// S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type S3Observation struct {

	// Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.
	CannedACL *string `json:"cannedAcl,omitempty" tf:"canned_acl,omitempty"`

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`

	// Enable server-side encryption, false by default.
	EnableEncryption *bool `json:"enableEncryption,omitempty" tf:"enable_encryption,omitempty"`

	// The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.
	EncryptionType *string `json:"encryptionType,omitempty" tf:"encryption_type,omitempty"`

	// S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint,omitempty"`

	// KMS key used if encryption is enabled and encryption type is set to sse-kms.
	KMSKey *string `json:"kmsKey,omitempty" tf:"kms_key,omitempty"`

	// S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type S3Parameters struct {

	// Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.
	// +kubebuilder:validation:Optional
	CannedACL *string `json:"cannedAcl,omitempty" tf:"canned_acl,omitempty"`

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`

	// Enable server-side encryption, false by default.
	// +kubebuilder:validation:Optional
	EnableEncryption *bool `json:"enableEncryption,omitempty" tf:"enable_encryption,omitempty"`

	// The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.
	// +kubebuilder:validation:Optional
	EncryptionType *string `json:"encryptionType,omitempty" tf:"encryption_type,omitempty"`

	// S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.
	// +kubebuilder:validation:Optional
	Endpoint *string `json:"endpoint,omitempty" tf:"endpoint,omitempty"`

	// KMS key used if encryption is enabled and encryption type is set to sse-kms.
	// +kubebuilder:validation:Optional
	KMSKey *string `json:"kmsKey,omitempty" tf:"kms_key,omitempty"`

	// S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.
	// +kubebuilder:validation:Optional
	Region *string `json:"region,omitempty" tf:"region,omitempty"`
}

type VolumesInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type VolumesObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type VolumesParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

type WorkloadTypeInitParameters struct {
	Clients []ClientsInitParameters `json:"clients,omitempty" tf:"clients,omitempty"`
}

type WorkloadTypeObservation struct {
	Clients []ClientsObservation `json:"clients,omitempty" tf:"clients,omitempty"`
}

type WorkloadTypeParameters struct {

	// +kubebuilder:validation:Optional
	Clients []ClientsParameters `json:"clients" tf:"clients,omitempty"`
}

type WorkspaceInitParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type WorkspaceObservation struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	Destination *string `json:"destination,omitempty" tf:"destination,omitempty"`
}

type WorkspaceParameters struct {

	// S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
	// +kubebuilder:validation:Optional
	Destination *string `json:"destination" tf:"destination,omitempty"`
}

// ClusterSpec defines the desired state of Cluster
type ClusterSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     ClusterParameters `json:"forProvider"`
	// THIS IS A BETA FIELD. It will be honored
	// unless the Management Policies feature flag is disabled.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	InitProvider ClusterInitParameters `json:"initProvider,omitempty"`
}

// ClusterStatus defines the observed state of Cluster.
type ClusterStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        ClusterObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true

// Cluster is the Schema for the Clusters API.
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:subresource:status
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,databricks}
type Cluster struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.sparkVersion) || (has(self.initProvider) && has(self.initProvider.sparkVersion))",message="spec.forProvider.sparkVersion is a required parameter"
	Spec   ClusterSpec   `json:"spec"`
	Status ClusterStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// ClusterList contains a list of Clusters
type ClusterList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Cluster `json:"items"`
}

// Repository type metadata.
var (
	Cluster_Kind             = "Cluster"
	Cluster_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: Cluster_Kind}.String()
	Cluster_KindAPIVersion   = Cluster_Kind + "." + CRDGroupVersion.String()
	Cluster_GroupVersionKind = CRDGroupVersion.WithKind(Cluster_Kind)
)

func init() {
	SchemeBuilder.Register(&Cluster{}, &ClusterList{})
}
