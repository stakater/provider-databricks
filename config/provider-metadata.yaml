name: databricks/databricks
resources:
    aws_iam_instance_profile:
        subCategory: Deployment
        name: aws_iam_instance_profile
        title: ""
        examples:
            - name: shared
              manifest: |-
                {
                  "name": "shared-instance-profile",
                  "role": "${aws_iam_role.role_for_s3_access.name}"
                }
              references:
                role: aws_iam_role.role_for_s3_access.name
              dependencies:
                aws_iam_policy.pass_role_for_s3_access: |-
                    {
                      "name": "shared-pass-role-for-s3-access",
                      "path": "/",
                      "policy": "${data.aws_iam_policy_document.pass_role_for_s3_access.json}"
                    }
                aws_iam_role.role_for_s3_access: |-
                    {
                      "assume_role_policy": "${data.aws_iam_policy_document.assume_role_for_ec2.json}",
                      "description": "Role for shared access",
                      "name": "shared-ec2-role-for-s3"
                    }
                aws_iam_role_policy_attachment.cross_account: |-
                    {
                      "policy_arn": "${aws_iam_policy.pass_role_for_s3_access.arn}",
                      "role": "${var.crossaccount_role_name}"
                    }
                databricks_cluster.this: |-
                    {
                      "autoscale": [
                        {
                          "max_workers": 50,
                          "min_workers": 1
                        }
                      ],
                      "autotermination_minutes": 20,
                      "aws_attributes": [
                        {
                          "availability": "SPOT",
                          "first_on_demand": 1,
                          "instance_profile_arn": "${databricks_instance_profile.shared.id}",
                          "spot_bid_price_percent": 100,
                          "zone_id": "us-east-1"
                        }
                      ],
                      "cluster_name": "Shared Autoscaling",
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                databricks_instance_profile.shared: |-
                    {
                      "instance_profile_arn": "${aws_iam_instance_profile.shared.arn}"
                    }
            - name: this
              manifest: |-
                {
                  "name": "my-databricks-sql-serverless-instance-profile",
                  "role": "${aws_iam_role.this.name}"
                }
              references:
                role: aws_iam_role.this.name
              dependencies:
                aws_iam_role.this: |-
                    {
                      "assume_role_policy": "${data.aws_iam_policy_document.sql_serverless_assume_role.json}",
                      "name": "my-databricks-sql-serverless-role"
                    }
                databricks_instance_profile.this: |-
                    {
                      "iam_role_arn": "${aws_iam_role.this.arn}",
                      "instance_profile_arn": "${aws_iam_instance_profile.this.arn}"
                    }
        argumentDocs:
            iam_role_arn: '- (Optional) The AWS IAM role ARN of the role associated with the instance profile. It must have the form arn:aws:iam::<account-id>:role/<name>. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.'
            id: '- ARN for EC2 Instance Profile, that is registered with Databricks.'
            instance_profile_arn: '- (Required) ARN attribute of aws_iam_instance_profile output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.'
            is_meta_instance_profile: '- (Optional) Whether the instance profile is a meta instance profile. Used only in IAM credential passthrough.'
            skip_validation: '- (Optional) For advanced usage only. If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. "Your requested instance type is not supported in your requested availability zone"), you can pass this flag to skip the validation and forcibly add the instance profile.'
        importStatements: []
    aws_vpc_endpoint:
        subCategory: Deployment
        name: aws_vpc_endpoint
        title: ""
        examples:
            - name: workspace
              manifest: |-
                {
                  "depends_on": [
                    "${aws_subnet.pl_subnet}"
                  ],
                  "private_dns_enabled": true,
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "service_name": "${local.private_link.workspace_service}",
                  "subnet_ids": [
                    "${aws_subnet.pl_subnet.id}"
                  ],
                  "vpc_endpoint_type": "Interface",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                service_name: local.private_link.workspace_service
                vpc_id: module.vpc.vpc_id
            - name: relay
              manifest: |-
                {
                  "depends_on": [
                    "${aws_subnet.pl_subnet}"
                  ],
                  "private_dns_enabled": true,
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "service_name": "${local.private_link.relay_service}",
                  "subnet_ids": [
                    "${aws_subnet.pl_subnet.id}"
                  ],
                  "vpc_endpoint_type": "Interface",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                service_name: local.private_link.relay_service
                vpc_id: module.vpc.vpc_id
            - name: s3
              manifest: |-
                {
                  "depends_on": [
                    "${module.vpc}"
                  ],
                  "route_table_ids": "${module.vpc.private_route_table_ids}",
                  "service_name": "com.amazonaws.${var.region}.s3",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                route_table_ids: module.vpc.private_route_table_ids
                vpc_id: module.vpc.vpc_id
            - name: sts
              manifest: |-
                {
                  "depends_on": [
                    "${module.vpc}"
                  ],
                  "private_dns_enabled": true,
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "service_name": "com.amazonaws.${var.region}.sts",
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_endpoint_type": "Interface",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
            - name: kinesis-streams
              manifest: |-
                {
                  "depends_on": [
                    "${module.vpc}"
                  ],
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "service_name": "com.amazonaws.${var.region}.kinesis-streams",
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_endpoint_type": "Interface",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
        argumentDocs:
            account_id: '- Account Id that could be found in the Accounts Console for AWS or GCP'
            aws_endpoint_service_id: '- (AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the Databricks PrivateLink documentation'
            aws_vpc_endpoint_id: '- (AWS only) ID of configured aws_vpc_endpoint'
            endpoint_region: '- Region of the PSC endpoint.'
            gcp_vpc_endpoint_info: '- (GCP only) a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:'
            id: '- the ID of VPC Endpoint in form of account_id/vpc_endpoint_id'
            project_id: '- The Google Cloud project ID of the VPC network where the PSC connection resides.'
            psc_connection_id: '- The unique ID of this PSC connection.'
            psc_endpoint_name: '- The name of the PSC endpoint in the Google Cloud project.'
            region: '- (AWS only) Region of AWS VPC'
            service_attachment_id: '- The service attachment this PSC connection connects to.'
            state: '- (AWS Only) State of VPC Endpoint'
            vpc_endpoint_id: '- Canonical unique identifier of VPC Endpoint in Databricks Account'
            vpc_endpoint_name: '- Name of VPC Endpoint in Databricks Account'
        importStatements: []
    databricks_access_control_rule_set:
        subCategory: Security
        name: databricks_access_control_rule_set
        title: ""
        examples:
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "application_id": "00000000-0000-0000-0000-000000000000",
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: automation_sp_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.user"
                    }
                  ],
                  "name": "accounts/${local.account_id}/servicePrincipals/${databricks_service_principal.automation_sp.application_id}/ruleSets/default"
                }
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_service_principal.automation_sp: |-
                    {
                      "display_name": "SP_FOR_AUTOMATION"
                    }
            - name: ds_group_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/group.manager"
                    }
                  ],
                  "name": "accounts/${local.account_id}/groups/${databricks_group.ds.id}/ruleSets/default"
                }
            - name: account_rule_set
              manifest: |-
                {
                  "grant_rules": [
                    {
                      "principals": [
                        "${data.databricks_user.john.acl_principal_id}"
                      ],
                      "role": "roles/group.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.ds.acl_principal_id}"
                      ],
                      "role": "roles/servicePrincipal.manager"
                    },
                    {
                      "principals": [
                        "${data.databricks_group.marketplace_admins.acl_principal_id}"
                      ],
                      "role": "roles/marketplace.admin"
                    }
                  ],
                  "name": "accounts/${local.account_id}/ruleSets/default"
                }
        argumentDocs:
            grant_rules: '- (Required) The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.'
            grant_rules.principals: '- (Required) a list of principals who are granted a role. The following format is supported:'
            grant_rules.role: '- (Required) Role to be granted. The supported roles are listed below. For more information about these roles, refer to service principal roles, group roles or marketplace roles.'
            groups/{groupname}: (also exposed as acl_principal_id attribute of databricks_group resource).
            id: '- ID of the access control rule set - the same as name.'
            name: '- (Required) Unique identifier of a rule set. The name determines the resource to which the rule set applies. Currently, only default rule sets are supported. The following rule set formats are supported:'
            roles/group.manager: '- Manager of a group.'
            roles/marketplace.admin: '- Admin of marketplace.'
            roles/servicePrincipal.manager: '- Manager of a service principal.'
            roles/servicePrincipal.user: '- User of a service principal.'
            servicePrincipals/{applicationId}: (also exposed as acl_principal_id attribute of databricks_service_principal resource).
            users/{username}: (also exposed as acl_principal_id attribute of databricks_user resource).
        importStatements: []
    databricks_artifact_allowlist:
        subCategory: Unity Catalog
        name: databricks_artifact_allowlist
        title: ""
        examples:
            - name: init_scripts
              manifest: |-
                {
                  "artifact_matcher": [
                    {
                      "artifact": "/Volumes/inits",
                      "match_type": "PREFIX_MATCH"
                    }
                  ],
                  "artifact_type": "INIT_SCRIPT"
                }
        argumentDocs:
            artifact_matcher.artifact: '- The artifact path or maven coordinate.'
            artifact_matcher.match_type: '- The pattern matching type of the artifact. Only PREFIX_MATCH is supported.'
            artifact_type: '- The artifact type of the allowlist. Can be INIT_SCRIPT, LIBRARY_JAR or LIBRARY_MAVEN. Change forces creation of a new resource.'
            created_at: '-  Time at which this artifact allowlist was set.'
            created_by: '-  Identity that set the artifact allowlist.'
            id: '- ID of the artifact allow list in form of metastore_id|artifact_type.'
            metastore_id: '- ID of the parent metastore.'
        importStatements: []
    databricks_catalog:
        subCategory: Unity Catalog
        name: databricks_catalog
        title: ""
        examples:
            - name: sandbox
              manifest: |-
                {
                  "comment": "this catalog is managed by terraform",
                  "name": "sandbox",
                  "properties": {
                    "purpose": "testing"
                  }
                }
        argumentDocs:
            comment: '- (Optional) User-supplied free-form text.'
            connection_name: '- (Optional) For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.'
            force_destroy: '- (Optional) Delete catalog regardless of its contents.'
            id: '- ID of this catalog - same as the name.'
            isolation_mode: '- (Optional) Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be ISOLATED or OPEN. Setting the catalog to ISOLATED will automatically allow access from the current workspace.'
            metastore_id: '- ID of the parent metastore.'
            name: '- Name of Catalog relative to parent metastore.'
            options: '- (Optional) For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.'
            owner: '- (Optional) Username/groupname/sp application_id of the catalog owner.'
            properties: '- (Optional) Extensible Catalog properties.'
            provider_name: '- (Optional) For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.'
            share_name: '- (Optional) For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.'
            storage_root: '- (Optional if storage_root is specified for the metastore) Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.'
        importStatements: []
    databricks_catalog_workspace_binding:
        subCategory: Unity Catalog
        name: databricks_catalog_workspace_binding
        title: ""
        examples:
            - name: sandbox
              manifest: |-
                {
                  "securable_name": "${databricks_catalog.sandbox.name}",
                  "workspace_id": "${databricks_mws_workspaces.other.workspace_id}"
                }
              references:
                securable_name: databricks_catalog.sandbox.name
                workspace_id: databricks_mws_workspaces.other.workspace_id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "isolation_mode": "ISOLATED",
                      "name": "sandbox"
                    }
        argumentDocs:
            binding_type: '- Binding mode. Default to BINDING_TYPE_READ_WRITE. Possible values are BINDING_TYPE_READ_ONLY, BINDING_TYPE_READ_WRITE'
            securable_name: '- Name of securable. Change forces creation of a new resource.'
            securable_type: '- Type of securable. Default to catalog. Change forces creation of a new resource.'
            workspace_id: '- ID of the workspace. Change forces creation of a new resource.'
        importStatements: []
    databricks_cluster:
        subCategory: Compute
        name: databricks_cluster
        title: ""
        examples:
            - name: shared_autoscaling
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: shared_autoscaling
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.io.cache.enabled": true,
                    "spark.databricks.io.cache.maxDiskUsage": "50g",
                    "spark.databricks.io.cache.maxMetaDataCache": "1g"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: single_node
              manifest: |-
                {
                  "autotermination_minutes": 20,
                  "cluster_name": "Single Node",
                  "custom_tags": {
                    "ResourceClass": "SingleNode"
                  },
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "singleNode",
                    "spark.master": "local[*]"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: cluster_with_table_access_control
              manifest: |-
                {
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared High-Concurrency",
                  "custom_tags": {
                    "ResourceClass": "Serverless"
                  },
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_conf": {
                    "spark.databricks.cluster.profile": "serverless",
                    "spark.databricks.repl.allowedLanguages": "python,sql"
                  },
                  "spark_version": "${data.databricks_spark_version.latest_lts.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest_lts.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "aws_attributes": [
                    {
                      "availability": "SPOT",
                      "first_on_demand": 1,
                      "spot_bid_price_percent": 100,
                      "zone_id": "us-east-1"
                    }
                  ],
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "azure_attributes": [
                    {
                      "availability": "SPOT_WITH_FALLBACK_AZURE",
                      "first_on_demand": 1,
                      "spot_bid_max_price": 100
                    }
                  ],
                  "cluster_name": "Shared Autoscaling",
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "autoscale": [
                    {
                      "max_workers": 50,
                      "min_workers": 1
                    }
                  ],
                  "autotermination_minutes": 20,
                  "cluster_name": "Shared Autoscaling",
                  "gcp_attributes": [
                    {
                      "availability": "PREEMPTIBLE_WITH_FALLBACK_GCP",
                      "zone_id": "AUTO"
                    }
                  ],
                  "node_type_id": "${data.databricks_node_type.smallest.id}",
                  "spark_version": "${data.databricks_spark_version.latest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
                spark_version: data.databricks_spark_version.latest.id
            - name: this
              manifest: |-
                {
                  "docker_image": [
                    {
                      "basic_auth": [
                        {
                          "password": "${azurerm_container_registry.this.admin_password}",
                          "username": "${azurerm_container_registry.this.admin_username}"
                        }
                      ],
                      "url": "${docker_registry_image.this.name}"
                    }
                  ]
                }
              references:
                docker_image.basic_auth.password: azurerm_container_registry.this.admin_password
                docker_image.basic_auth.username: azurerm_container_registry.this.admin_username
                docker_image.url: docker_registry_image.this.name
              dependencies:
                docker_registry_image.this: |-
                    {
                      "build": [
                        {}
                      ],
                      "name": "${azurerm_container_registry.this.login_server}/sample:latest"
                    }
            - name: with_nfs
              manifest: |-
                {
                  "cluster_mount_info": [
                    {
                      "local_mount_dir_path": "/mnt/nfs-test",
                      "network_filesystem_info": [
                        {
                          "mount_options": "sec=sys,vers=3,nolock,proto=tcp",
                          "server_address": "${local.storage_account}.blob.core.windows.net"
                        }
                      ],
                      "remote_mount_dir_path": "${local.storage_account}/${local.storage_container}"
                    }
                  ]
                }
            - name: with_nfs
              manifest: |-
                {
                  "workload_type": [
                    {
                      "clients": [
                        {
                          "jobs": false,
                          "notebooks": true
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            AUTO: ': Databricks picks an availability zone to schedule the cluster on.'
            HA: '(default): High availability, spread nodes across availability zones for a Databricks deployment region.'
            allow_cluster_create: argument set would still be able to create clusters, but within the boundary of the policy.
            apply_policy_default_values: '- (Optional) Whether to use policy default values for missing cluster attributes.'
            autoscale.max_workers: '- (Optional) The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.'
            autoscale.min_workers: '- (Optional) The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.'
            autoscale.spark.databricks.cluster.profile: must have value singleNode
            autoscale.spark.master: must have prefix local, like local[*]
            autotermination_minutes: '- (Optional) Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to 60.  We highly recommend having this setting present for Interactive/BI clusters.'
            aws_attributes.availability: '- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT, SPOT_WITH_FALLBACK and ON_DEMAND. Note: If first_on_demand is zero, this availability type will be used for the entire cluster. Backend default value is SPOT_WITH_FALLBACK and could change in the future'
            aws_attributes.ebs_volume_count: '- (Optional) The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.'
            aws_attributes.ebs_volume_size: '- (Optional) The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).'
            aws_attributes.ebs_volume_type: '- (Optional) The type of EBS volumes that will be launched with this cluster. Valid values are GENERAL_PURPOSE_SSD or THROUGHPUT_OPTIMIZED_HDD. Use this option only if you''re not picking Delta Optimized  node types.'
            aws_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is 1 and could change in the future'
            aws_attributes.instance_profile_arn: '- (Optional) Nodes for this cluster will only be placed on AWS instances with this instance profile. Please see databricks_instance_profile resource documentation for extended examples on adding a valid instance profile using Terraform.'
            aws_attributes.spot_bid_price_percent: '- (Optional) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than 10000.'
            aws_attributes.zone_id: '- (Required) Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like us-west-2a. The provided availability zone must be in the same region as the Databricks deployment. For example, us-west-2a is not a valid zone ID if the Databricks deployment resides in the us-east-1 region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value auto. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.'
            azure_attributes.availability: '- (Optional) Availability type used for all subsequent nodes past the first_on_demand ones. Valid values are SPOT_AZURE, SPOT_WITH_FALLBACK_AZURE, and ON_DEMAND_AZURE. Note: If first_on_demand is zero, this availability type will be used for the entire cluster.'
            azure_attributes.first_on_demand: '- (Optional) The first first_on_demand nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.'
            azure_attributes.spot_bid_max_price: '- (Optional) The max price for Azure spot instances.  Use -1 to specify the lowest price.'
            canned_acl: '- (Optional) Set canned access control list, e.g. bucket-owner-full-control. If canned_cal is set, the cluster instance profile must have s3:PutObjectAcl permission on the destination bucket and prefix. The full list of possible canned ACLs can be found here. By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set bucket-owner-full-control to make bucket owners able to read the logs.'
            cluster_mount_info.local_mount_dir_path: '- (Required) path inside the Spark container.'
            cluster_mount_info.network_filesystem_info: '- block specifying connection. It consists of:'
            cluster_mount_info.remote_mount_dir_path: '- (Optional) string specifying path to mount on the remote service.'
            cluster_name: '- (Optional) Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.'
            custom_tags: '- (Optional) Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to default_tags. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an x_ when it is propagated.'
            data_security_mode: '- (Optional) Select the security features of the cluster. Unity Catalog requires SINGLE_USER or USER_ISOLATION mode. LEGACY_PASSTHROUGH for passthrough cluster and LEGACY_TABLE_ACL for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed Access Mode and USER_ISOLATION has been renamed Shared, but use these terms here.'
            dbfs:/mnt/name: .
            default_tags: '- (map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: , and any workspace and pool tags.'
            destination: '- S3 destination, e.g., s3://my-bucket/some-prefix You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.'
            docker_image.basic_auth: '- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.'
            docker_image.url: '- URL for the Docker image'
            driver_instance_pool_id: (Optional) - similar to instance_pool_id, but for driver node. If omitted, and instance_pool_id is specified, then the driver will be allocated from that pool.
            driver_node_type_id: '- (Optional) The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as node_type_id defined above.'
            enable_elastic_disk: '- (Optional) If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have autotermination_minutes and autoscale attributes set. More documentation available at cluster configuration page.'
            enable_encryption: '- (Optional) Enable server-side encryption, false by default.'
            enable_local_disk_encryption: '- (Optional) Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.'
            encryption_type: '- (Optional) The encryption type, it could be sse-s3 or sse-kms. It is used only when encryption is enabled, and the default type is sse-s3.'
            endpoint: '- (Optional) S3 endpoint, e.g. https://s3-us-west-2.amazonaws.com. Either region or endpoint needs to be set. If both are set, the endpoint is used.'
            gcp_attributes.availability: ', and will be removed soon.'
            gcp_attributes.boot_disk_size: (optional, int) Boot disk size in GB
            gcp_attributes.google_service_account: '- (Optional, string) Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.'
            gcp_attributes.local_ssd_count: (optional, int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            gcp_attributes.use_preemptible_executors: '- (Optional, bool) if we should use preemptible executors (GCP documentation). Warning: this field is deprecated in favor of'
            gcp_attributes.zone_id: '(optional)  Identifier for the availability zone in which the cluster resides. This can be one of the following:'
            id: '- Canonical unique identifier for the cluster.'
            idempotency_token: '- (Optional) An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster''s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.'
            instance_pool_id: (Optional - required if node_type_id is not given) - To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to TERMINATED, the instances it used are returned to the pool and reused by a different cluster.
            instance_profile_arn: (AWS only) can control which data a given cluster can access through cloud-native controls.
            is_pinned: '- (Optional) boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters'' maximum number is limited to 100, so apply may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).'
            kms_key: '- (Optional) KMS key used if encryption is enabled and encryption type is set to sse-kms.'
            mount_options: '- (Optional) string that will be passed as options passed to the mount command.'
            node_type_id: '- (Required - optional if instance_pool_id is given) Any supported databricks_node_type id. If instance_pool_id is specified, this field is not needed.'
            num_workers: '- (Optional) Number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes.'
            policy_id: '- (Optional) Identifier of Cluster Policy to validate cluster and preset certain defaults. The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters. For example, when you specify policy_id of external metastore policy, you still have to fill in relevant keys for spark_conf.  If relevant fields aren''t filled in, then it will cause the configuration drift detected on each plan/apply, and Terraform will try to apply the detected changes.'
            region: '- (Optional) S3 region, e.g. us-west-2. Either region or endpoint must be set. If both are set, the endpoint is used.'
            runtime_engine: '- (Optional) The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: PHOTON, STANDARD.'
            server_address: '- (Required) host name.'
            single_user_name: '- (Optional) The optional user name of the user to assign to an interactive cluster. This field is required when using data_security_mode set to SINGLE_USER or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).'
            spark.databricks.cluster.profile: set to serverless
            spark.databricks.repl.allowedLanguages: 'set to a list of supported languages, for example: python,sql, or python,sql,r.  Scala is not supported!'
            spark_conf: '- (Optional) Map with key-value pairs to fine-tune Spark clusters, where you can provide custom Spark configuration properties in a cluster configuration.'
            spark_env_vars: '- (Optional) Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=''Y'') while launching the driver and workers.'
            spark_version: '- (Required) Runtime version of the cluster. Any supported databricks_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.'
            ssh_public_keys: '- (Optional) SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.'
            state: '- (string) State of the cluster.'
            workload_type.jobs: '- (Optional) boolean flag defining if it''s possible to run Databricks Jobs on this cluster. Default: true.'
            workload_type.notebooks: '- (Optional) boolean flag defining if it''s possible to run notebooks on this cluster. Default: true.'
        importStatements: []
    databricks_cluster_policy:
        subCategory: Compute
        name: databricks_cluster_policy
        title: ""
        examples:
            - name: fair_use
              manifest: |-
                {
                  "definition": "${jsonencode(merge(local.default_policy, var.policy_overrides))}",
                  "libraries": [
                    {
                      "pypi": [
                        {
                          "package": "databricks-sdk==0.12.0"
                        }
                      ]
                    }
                  ],
                  "name": "${var.team} cluster policy"
                }
              dependencies:
                databricks_permissions.can_use_cluster_policyinstance_profile: |-
                    {
                      "access_control": [
                        {
                          "group_name": "${var.team}",
                          "permission_level": "CAN_USE"
                        }
                      ],
                      "cluster_policy_id": "${databricks_cluster_policy.fair_use.id}"
                    }
            - name: personal_vm
              manifest: |-
                {
                  "name": "Personal Compute",
                  "policy_family_definition_overrides": "${jsonencode(personal_vm_override)}",
                  "policy_family_id": "personal-vm"
                }
        argumentDocs:
            Free form: policy and create fully-configurable clusters.
            definition: '- Policy definition: JSON document expressed in Databricks Policy Definition Language. Cannot be used with policy_family_id'
            description: '- (Optional) Additional human-readable description of the cluster policy.'
            id: '- Canonical unique identifier for the cluster policy. This is equal to policy_id.'
            libraries: (Optional) blocks defining individual libraries that will be installed on the cluster that uses a given cluster policy. See databricks_cluster for more details about supported library types.
            max_clusters_per_user: '- (Optional, integer) Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.'
            name: '- the name of the built-in cluster policy.'
            policy_family_definition_overrides: '- settings to override in the built-in cluster policy.'
            policy_family_id: '- the ID of the cluster policy family used for built-in cluster policy.'
            policy_id: '- Canonical unique identifier for the cluster policy.'
            spark_version: parameter in databricks_cluster and other resources.
        importStatements: []
    databricks_connection:
        subCategory: Unity Catalog
        name: databricks_connection
        title: ""
        examples:
            - name: mysql
              manifest: |-
                {
                  "comment": "this is a connection to mysql db",
                  "connection_type": "MYSQL",
                  "name": "mysql_connection",
                  "options": {
                    "host": "test.mysql.database.azure.com",
                    "password": "password",
                    "port": "3306",
                    "user": "user"
                  },
                  "properties": {
                    "purpose": "testing"
                  }
                }
            - name: bigquery
              manifest: |-
                {
                  "comment": "this is a connection to BQ",
                  "connection_type": "BIGQUERY",
                  "name": "bq_connection",
                  "options": {
                    "GoogleServiceAccountKeyJson": "${jsonencode({\n      \"type\" : \"service_account\",\n      \"project_id\" : \"PROJECT_ID\",\n      \"private_key_id\" : \"KEY_ID\",\n      \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\" : \"SERVICE_ACCOUNT_EMAIL\",\n      \"client_id\" : \"CLIENT_ID\",\n      \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\",\n      \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n      \"universe_domain\" : \"googleapis.com\"\n    })}"
                  },
                  "properties": {
                    "purpose": "testing"
                  }
                }
        argumentDocs:
            comment: '- (Optional) Free-form text.'
            connection_type: '- Connection type. BIGQUERY MYSQL POSTGRESQL SNOWFLAKE REDSHIFT SQLDW SQLSERVER or DATABRICKS are supported. Up-to-date list of connection type supported'
            id: '- ID of this connection in form of <metastore_id>|<name>.'
            name: '- Name of the Connection.'
            options: '- The key value of options required by the connection, e.g. host, port, user, password or GoogleServiceAccountKeyJson. Please consult the documentation for the required option.'
            owner: '- (Optional) Name of the connection owner.'
            properties: '-  (Optional) Free-form connection properties.'
        importStatements: []
    databricks_dbfs_file:
        subCategory: Storage
        name: databricks_dbfs_file
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "path": "/tmp/main.tf",
                  "source": "${path.module}/main.tf"
                }
            - name: this
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    Hello, world!\n    Module is ${abspath(path.module)}\n    EOT\n  )}",
                  "path": "/tmp/this.txt"
                }
            - name: app
              manifest: |-
                {
                  "path": "/FileStore/baz.whl",
                  "source": "${path.module}/baz.whl"
                }
              dependencies:
                databricks_library.app: |-
                    {
                      "cluster_id": "${each.key}",
                      "for_each": "${data.databricks_clusters.all.ids}",
                      "whl": "${databricks_dbfs_file.app.dbfs_path}"
                    }
        argumentDocs:
            content_base64: '- Encoded file contents. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a data pipeline configuration file.'
            dbfs:/mnt/name: .
            dbfs_path: '- Path, but with dbfs: prefix.'
            file_size: '- The file size of the file that is being tracked by this resource in bytes.'
            id: '- Same as path.'
            path: '- (Required) The path of the file in which you wish to save.'
            source: '- The full absolute path to the file. Conflicts with content_base64.'
        importStatements: []
    databricks_directory:
        subCategory: Workspace
        name: databricks_directory
        title: ""
        examples:
            - name: my_custom_directory
              manifest: |-
                {
                  "path": "/my_custom_directory"
                }
        argumentDocs:
            delete_recursive: '- Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Terraform. Defaults to false'
            id: '- Path of directory on workspace'
            object_id: '- Unique identifier for a DIRECTORY'
            path: '- (Required) The absolute path of the directory, beginning with "/", e.g. "/Demo".'
            spark_version: parameter in databricks_cluster and other resources.
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_entitlements:
        subCategory: Security
        name: databricks_entitlements
        title: ""
        examples:
            - name: me
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "user_id": "${data.databricks_user.me.id}"
                }
              references:
                user_id: data.databricks_user.me.id
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "service_principal_id": "${data.databricks_service_principal.this.sp_id}"
                }
              references:
                service_principal_id: data.databricks_service_principal.this.sp_id
            - name: workspace-users
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "group_id": "${data.databricks_group.users.id}"
                }
              references:
                group_id: data.databricks_group.users.id
        argumentDocs:
            allow_cluster_create: '-  (Optional) Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the principal to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            group/group_id: '- group group_id.'
            group_id: '- Canonical unique identifier for the group.'
            service_principal_id: '- Canonical unique identifier for the service principal.'
            spn/spn_id: '- service principal spn_id.'
            user/user_id: '- user user_id.'
            user_id: '-  Canonical unique identifier for the user.'
            workspace_access: '- (Optional) This is a field to allow the principal to have access to Databricks Workspace.'
        importStatements: []
    databricks_external_location:
        subCategory: Unity Catalog
        name: databricks_external_location
        title: ""
        examples:
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.external.id}",
                  "name": "external",
                  "url": "s3://${aws_s3_bucket.external.id}/some"
                }
              references:
                credential_name: databricks_storage_credential.external.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.external.id}",
                  "depends_on": [
                    "${databricks_metastore_assignment.this}"
                  ],
                  "name": "external",
                  "url": "${format(\"abfss://%s@%s.dfs.core.windows.net\",\n    azurerm_storage_container.ext_storage.name,\n  azurerm_storage_account.ext_storage.name)}"
                }
              references:
                credential_name: databricks_storage_credential.external.id
              dependencies:
                databricks_grants.some: |-
                    {
                      "external_location": "${databricks_external_location.some.id}",
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE",
                            "READ_FILES"
                          ]
                        }
                      ]
                    }
                databricks_storage_credential.external: |-
                    {
                      "azure_service_principal": [
                        {
                          "application_id": "${azuread_application.ext_cred.application_id}",
                          "client_secret": "${azuread_application_password.ext_cred.value}",
                          "directory_id": "${var.tenant_id}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "depends_on": [
                        "${databricks_metastore_assignment.this}"
                      ],
                      "name": "${azuread_application.ext_cred.display_name}"
                    }
            - name: some
              manifest: |-
                {
                  "comment": "Managed by TF",
                  "credential_name": "${databricks_storage_credential.ext.id}",
                  "name": "the-ext-location",
                  "url": "gs://${google_storage_bucket.ext_bucket.name}"
                }
              references:
                credential_name: databricks_storage_credential.ext.id
              dependencies:
                databricks_storage_credential.ext: |-
                    {
                      "databricks_gcp_service_account": [
                        {}
                      ],
                      "name": "the-creds"
                    }
        argumentDocs:
            access_point: '- (Optional) The ARN of the s3 access point to use with the external location (AWS).'
            comment: '- (Optional) User-supplied free-form text.'
            credential_name: '- Name of the databricks_storage_credential to use with this external location.'
            databricks_external_location: are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.
            encryption_details: '- (Optional) The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).'
            force_destroy: '- (Optional) Destroy external location regardless of its dependents.'
            force_update: '- (Optional) Update external location regardless of its dependents.'
            id: '- ID of this external location - same as name.'
            name: '- Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the external location owner.'
            read_only: '- (Optional) Indicates whether the external location is read-only.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the external location'
            url: '- Path URL in cloud storage, of the form: s3://[bucket-host]/[bucket-dir] (AWS), abfss://[user]@[host]/[path] (Azure), gs://[bucket-host]/[bucket-dir] (GCP).'
        importStatements: []
    databricks_file:
        subCategory: Storage
        name: databricks_file
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "path": "${databricks_volume.this.volume_path}/fileName",
                  "source": "/full/path/on/local/system"
                }
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "metastore_id": "${databricks_metastore.this.id}",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "volume_type": "MANAGED"
                    }
            - name: init_script
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"Hello World\"\n    EOT\n  )}",
                  "path": "${databricks_volume.this.volume_path}/fileName"
                }
        argumentDocs:
            content_base64: '- Contents in base 64 format. Conflicts with source.'
            file_size: '- The file size of the file that is being tracked by this resource in bytes.'
            id: '- Same as path.'
            modification_time: '- The last time stamp when the file was modified'
            path: '- The path of the file in which you wish to save. For example, /Volumes/main/default/volume1/file.txt.'
            source: '- The full absolute path to the file. Conflicts with content_base64.'
        importStatements: []
    databricks_git_credential:
        subCategory: Workspace
        name: databricks_git_credential
        title: ""
        examples:
            - name: ado
              manifest: |-
                {
                  "git_provider": "azureDevOpsServices",
                  "git_username": "myuser",
                  "personal_access_token": "sometoken"
                }
        argumentDocs:
            force: '- (Optional) specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it''s already configured, the apply operation will fail.'
            git_provider: '-  (Required) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Git Credentials API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit.'
            git_username: '- (Required) user name at Git provider.'
            id: '- identifier of specific Git credential'
            personal_access_token: '- (Required) The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it''s sourced from the first environment variable of GITHUB_TOKEN, GITLAB_TOKEN, or AZDO_PERSONAL_ACCESS_TOKEN, that has a non-empty value.'
        importStatements: []
    databricks_global_init_script:
        subCategory: Workspace
        name: databricks_global_init_script
        title: ""
        examples:
            - name: init1
              manifest: |-
                {
                  "name": "my init script",
                  "source": "${path.module}/init.sh"
                }
            - name: init2
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"hello world\"\n    EOT\n  )}",
                  "name": "hello script"
                }
        argumentDocs:
            content_base64: '- The base64-encoded source code global init script. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances'
            dbfs:/mnt/name: .
            enabled: '(bool, optional default: false) specifies if the script is enabled for execution, or not'
            id: '- ID assigned to a global init script by API'
            name: (string, required) - the name of the script.  It should be unique
            position: '(integer, optional default: null) - the position of a global init script, where 0 represents the first global init script to run, 1 is the second global init script to run, and so on. When omitted, the script gets the last position.'
            source: '- Path to script''s source code on local filesystem. Conflicts with content_base64'
        importStatements: []
    databricks_grant:
        subCategory: Unity Catalog
        name: databricks_grant
        title: ""
        examples:
            - name: sandbox_data_engineers
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_CATALOG",
                    "CREATE_EXTERNAL_LOCATION"
                  ]
                }
            - name: sandbox_data_sharer
              manifest: |-
                {
                  "principal": "Data Sharer",
                  "privileges": [
                    "CREATE_RECIPIENT",
                    "CREATE_SHARE"
                  ]
                }
            - name: sandbox_data_scientists
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Scientists",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "CREATE_TABLE",
                    "SELECT"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "metastore_id": "${databricks_metastore.this.id}",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: sandbox_data_engineers
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "CREATE_SCHEMA",
                    "CREATE_TABLE",
                    "MODIFY"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "metastore_id": "${databricks_metastore.this.id}",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: sandbox_data_analyst
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "principal": "Data Analyst",
                  "privileges": [
                    "USE_CATALOG",
                    "USE_SCHEMA",
                    "SELECT"
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "metastore_id": "${databricks_metastore.this.id}",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: things
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ],
                  "schema": "${databricks_schema.things.id}"
                }
              references:
                schema: databricks_schema.things.id
              dependencies:
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: customers_data_engineers
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "MODIFY",
                    "SELECT"
                  ],
                  "table": "main.reporting.customers"
                }
            - name: customers_data_analysts
              manifest: |-
                {
                  "principal": "Data Analysts",
                  "privileges": [
                    "SELECT"
                  ],
                  "table": "main.reporting.customers"
                }
            - name: things
              manifest: |-
                {
                  "for_each": "${data.databricks_tables.things.ids}",
                  "principal": "sensitive",
                  "privileges": [
                    "SELECT",
                    "MODIFY"
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_tables.things.ids
                table: each.value
            - name: customer360
              manifest: |-
                {
                  "principal": "Data Analysts",
                  "privileges": [
                    "SELECT"
                  ],
                  "table": "main.reporting.customer360"
                }
            - name: customers
              manifest: |-
                {
                  "for_each": "${data.databricks_views.customers.ids}",
                  "principal": "sensitive",
                  "privileges": [
                    "SELECT",
                    "MODIFY"
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_views.customers.ids
                table: each.value
            - name: volume
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "WRITE_VOLUME"
                  ],
                  "volume": "${databricks_volume.this.id}"
                }
              references:
                volume: databricks_volume.this.id
              dependencies:
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "storage_location": "${databricks_external_location.some.url}",
                      "volume_type": "EXTERNAL"
                    }
            - name: customers_data_engineers
              manifest: |-
                {
                  "model": "main.reporting.customer_model",
                  "principal": "Data Engineers",
                  "privileges": [
                    "APPLY_TAG",
                    "EXECUTE"
                  ]
                }
            - name: customers_data_analysts
              manifest: |-
                {
                  "model": "main.reporting.customer_model",
                  "principal": "Data Analysts",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: udf_data_engineers
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "principal": "Data Engineers",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: udf_data_analysts
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "principal": "Data Analysts",
                  "privileges": [
                    "EXECUTE"
                  ]
                }
            - name: external_creds
              manifest: |-
                {
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_EXTERNAL_TABLE"
                  ],
                  "storage_credential": "${databricks_storage_credential.external.id}"
                }
              references:
                storage_credential: databricks_storage_credential.external.id
              dependencies:
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some_data_engineers
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_EXTERNAL_TABLE",
                    "READ_FILES"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_service_principal
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_service_principal.my_sp.application_id}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_service_principal.my_sp.application_id
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_group
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_group.my_group.display_name}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_group.my_group.display_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some_user
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "principal": "${databricks_group.my_user.user_name}",
                  "privileges": [
                    "USE_SCHEMA",
                    "MODIFY"
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                principal: databricks_group.my_user.user_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some
              manifest: |-
                {
                  "foreign_connection": "${databricks_connection.mysql.name}",
                  "principal": "Data Engineers",
                  "privileges": [
                    "CREATE_FOREIGN_CATALOG",
                    "USE_CONNECTION"
                  ]
                }
              references:
                foreign_connection: databricks_connection.mysql.name
              dependencies:
                databricks_connection.mysql: |-
                    {
                      "comment": "this is a connection to mysql db",
                      "connection_type": "MYSQL",
                      "name": "mysql_connection",
                      "options": {
                        "host": "test.mysql.database.azure.com",
                        "password": "password",
                        "port": "3306",
                        "user": "user"
                      },
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: some
              manifest: |-
                {
                  "principal": "${databricks_recipient.some.name}",
                  "privileges": [
                    "SELECT"
                  ],
                  "share": "${databricks_share.some.name}"
                }
              references:
                principal: databricks_recipient.some.name
                share: databricks_share.some.name
              dependencies:
                databricks_recipient.some: |-
                    {
                      "name": "my_recipient"
                    }
                databricks_share.some: |-
                    {
                      "name": "my_share"
                    }
        argumentDocs:
            principal: '- User name, group name or service principal application ID.'
            privileges: '- One or more privileges that are specific to a securable type.'
        importStatements: []
    databricks_grants:
        subCategory: Unity Catalog
        name: databricks_grants
        title: ""
        examples:
            - name: sandbox
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_CATALOG",
                        "CREATE_EXTERNAL_LOCATION"
                      ]
                    },
                    {
                      "principal": "Data Sharer",
                      "privileges": [
                        "CREATE_RECIPIENT",
                        "CREATE_SHARE"
                      ]
                    }
                  ],
                  "metastore": "metastore_id"
                }
            - name: sandbox
              manifest: |-
                {
                  "catalog": "${databricks_catalog.sandbox.name}",
                  "grant": [
                    {
                      "principal": "Data Scientists",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "CREATE_TABLE",
                        "SELECT"
                      ]
                    },
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "CREATE_SCHEMA",
                        "CREATE_TABLE",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "Data Analyst",
                      "privileges": [
                        "USE_CATALOG",
                        "USE_SCHEMA",
                        "SELECT"
                      ]
                    }
                  ]
                }
              references:
                catalog: databricks_catalog.sandbox.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: things
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "USE_SCHEMA",
                        "MODIFY"
                      ]
                    }
                  ],
                  "schema": "${databricks_schema.things.id}"
                }
              references:
                schema: databricks_schema.things.id
              dependencies:
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: customers
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "MODIFY",
                        "SELECT"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "main.reporting.customers"
                }
            - name: things
              manifest: |-
                {
                  "for_each": "${data.databricks_tables.things.ids}",
                  "grant": [
                    {
                      "principal": "sensitive",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    }
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_tables.things.ids
                table: each.value
            - name: customer360
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "main.reporting.customer360"
                }
            - name: customers
              manifest: |-
                {
                  "for_each": "${data.databricks_views.customers.ids}",
                  "grant": [
                    {
                      "principal": "sensitive",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    }
                  ],
                  "table": "${each.value}"
                }
              references:
                for_each: data.databricks_views.customers.ids
                table: each.value
            - name: volume
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "WRITE_VOLUME"
                      ]
                    }
                  ],
                  "volume": "${databricks_volume.this.id}"
                }
              references:
                volume: databricks_volume.this.id
              dependencies:
                databricks_volume.this: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this volume is managed by terraform",
                      "name": "quickstart_volume",
                      "schema_name": "${databricks_schema.things.name}",
                      "storage_location": "${databricks_external_location.some.url}",
                      "volume_type": "EXTERNAL"
                    }
            - name: customers
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "APPLY_TAG",
                        "EXECUTE"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "EXECUTE"
                      ]
                    }
                  ],
                  "model": "main.reporting.customer_model"
                }
            - name: udf
              manifest: |-
                {
                  "function": "main.reporting.udf",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "EXECUTE"
                      ]
                    },
                    {
                      "principal": "Data Analysts",
                      "privileges": [
                        "EXECUTE"
                      ]
                    }
                  ]
                }
            - name: external_creds
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE"
                      ]
                    }
                  ],
                  "storage_credential": "${databricks_storage_credential.external.id}"
                }
              references:
                storage_credential: databricks_storage_credential.external.id
              dependencies:
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "comment": "Managed by TF",
                      "name": "${aws_iam_role.external_data_access.name}"
                    }
            - name: some
              manifest: |-
                {
                  "external_location": "${databricks_external_location.some.id}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_EXTERNAL_TABLE",
                        "READ_FILES"
                      ]
                    },
                    {
                      "principal": "${databricks_service_principal.my_sp.application_id}",
                      "privileges": [
                        "USE_SCHEMA",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "${databricks_group.my_group.display_name}",
                      "privileges": [
                        "USE_SCHEMA",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "${databricks_group.my_user.user_name}",
                      "privileges": [
                        "USE_SCHEMA",
                        "MODIFY"
                      ]
                    }
                  ]
                }
              references:
                external_location: databricks_external_location.some.id
                grant.principal: databricks_group.my_user.user_name
              dependencies:
                databricks_external_location.some: |-
                    {
                      "comment": "Managed by TF",
                      "credential_name": "${databricks_storage_credential.external.id}",
                      "name": "external",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
            - name: some
              manifest: |-
                {
                  "foreign_connection": "${databricks_connection.mysql.name}",
                  "grant": [
                    {
                      "principal": "Data Engineers",
                      "privileges": [
                        "CREATE_FOREIGN_CATALOG",
                        "USE_CONNECTION"
                      ]
                    }
                  ]
                }
              references:
                foreign_connection: databricks_connection.mysql.name
              dependencies:
                databricks_connection.mysql: |-
                    {
                      "comment": "this is a connection to mysql db",
                      "connection_type": "MYSQL",
                      "name": "mysql_connection",
                      "options": {
                        "host": "test.mysql.database.azure.com",
                        "password": "password",
                        "port": "3306",
                        "user": "user"
                      },
                      "properties": {
                        "purpose": "testing"
                      }
                    }
            - name: some
              manifest: |-
                {
                  "grant": [
                    {
                      "principal": "${databricks_recipient.some.name}",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "share": "${databricks_share.some.name}"
                }
              references:
                grant.principal: databricks_recipient.some.name
                share: databricks_share.some.name
              dependencies:
                databricks_recipient.some: |-
                    {
                      "name": "my_recipient"
                    }
                databricks_share.some: |-
                    {
                      "name": "my_share"
                    }
        argumentDocs:
            databricks_grants.principal: '- User name, group name or service principal application ID.'
            databricks_grants.privileges: '- One or more privileges that are specific to a securable type.'
        importStatements: []
    databricks_group:
        subCategory: Security
        name: databricks_group
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "display_name": "Some Group"
                }
            - name: this
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "allow_instance_pool_create": true,
                  "display_name": "Some Group"
                }
              dependencies:
                databricks_group_member.vip_member: |-
                    {
                      "group_id": "${databricks_group.this.id}",
                      "member_id": "${databricks_user.this.id}"
                    }
                databricks_user.this: |-
                    {
                      "user_name": "someone@example.com"
                    }
            - name: this
              manifest: |-
                {
                  "display_name": "Some Group",
                  "provider": "${databricks.mws}"
                }
              references:
                provider: databricks.mws
            - name: this
              manifest: |-
                {
                  "display_name": "Some Group",
                  "provider": "${databricks.azure_account}"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. groups/Some Group.'
            allow_cluster_create: '-  (Optional) This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            display_name: '-  (Required) This is the display name for the given group.'
            external_id: '- (Optional) ID of the group in an external identity provider.'
            force: '- (Optional) Ignore cannot create group: Group with name X already exists. errors and implicitly import the specific group into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            id: '-  The id for the group object.'
            workspace_access: '- (Optional) This is a field to allow the group to have access to Databricks Workspace.'
        importStatements: []
    databricks_group_member:
        subCategory: Security
        name: databricks_group_member
        title: ""
        examples:
            - name: ab
              manifest: |-
                {
                  "group_id": "${databricks_group.a.id}",
                  "member_id": "${databricks_group.b.id}"
                }
              references:
                group_id: databricks_group.a.id
                member_id: databricks_group.b.id
              dependencies:
                databricks_group.a: |-
                    {
                      "display_name": "A"
                    }
                databricks_group.b: |-
                    {
                      "display_name": "B"
                    }
                databricks_user.bradley: |-
                    {
                      "user_name": "bradley@example.com"
                    }
            - name: bb
              manifest: |-
                {
                  "group_id": "${databricks_group.b.id}",
                  "member_id": "${databricks_user.bradley.id}"
                }
              references:
                group_id: databricks_group.b.id
                member_id: databricks_user.bradley.id
              dependencies:
                databricks_group.a: |-
                    {
                      "display_name": "A"
                    }
                databricks_group.b: |-
                    {
                      "display_name": "B"
                    }
                databricks_user.bradley: |-
                    {
                      "user_name": "bradley@example.com"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id for the databricks_group_member object which is in the format <group_id>|<member_id>.'
            member_id: '- (Required) This is the id of the group, service principal, or user.'
        importStatements: []
    databricks_group_role:
        subCategory: Security
        name: databricks_group_role
        title: ""
        examples:
            - name: my_group_instance_profile
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "role": "${databricks_instance_profile.instance_profile.id}"
                }
              references:
                group_id: databricks_group.my_group.id
                role: databricks_instance_profile.instance_profile.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
            - name: my_group_account_admin
              manifest: |-
                {
                  "group_id": "${databricks_group.my_group.id}",
                  "role": "account_admin"
                }
              references:
                group_id: databricks_group.my_group.id
              dependencies:
                databricks_group.my_group: |-
                    {
                      "display_name": "my_group_name"
                    }
        argumentDocs:
            group_id: '- (Required) This is the id of the group resource.'
            id: '- The id for the databricks_group_role object which is in the format <group_id>|<role>.'
            role: '- (Required) Either a role name or the ARN/ID of the instance profile resource.'
        importStatements: []
    databricks_instance_pool:
        subCategory: Compute
        name: databricks_instance_pool
        title: ""
        examples:
            - name: smallest_nodes
              manifest: |-
                {
                  "aws_attributes": [
                    {
                      "availability": "ON_DEMAND",
                      "spot_bid_price_percent": "100",
                      "zone_id": "us-east-1a"
                    }
                  ],
                  "disk_spec": [
                    {
                      "disk_count": 1,
                      "disk_size": 80,
                      "disk_type": [
                        {
                          "ebs_volume_type": "GENERAL_PURPOSE_SSD"
                        }
                      ]
                    }
                  ],
                  "idle_instance_autotermination_minutes": 10,
                  "instance_pool_name": "Smallest Nodes",
                  "max_capacity": 300,
                  "min_idle_instances": 0,
                  "node_type_id": "${data.databricks_node_type.smallest.id}"
                }
              references:
                node_type_id: data.databricks_node_type.smallest.id
            - name: this
              manifest: |-
                {
                  "preloaded_docker_image": [
                    {
                      "basic_auth": [
                        {
                          "password": "${azurerm_container_registry.this.admin_password}",
                          "username": "${azurerm_container_registry.this.admin_username}"
                        }
                      ],
                      "url": "${docker_registry_image.this.name}"
                    }
                  ]
                }
              references:
                preloaded_docker_image.basic_auth.password: azurerm_container_registry.this.admin_password
                preloaded_docker_image.basic_auth.username: azurerm_container_registry.this.admin_username
                preloaded_docker_image.url: docker_registry_image.this.name
              dependencies:
                docker_registry_image.this: |-
                    {
                      "build": [
                        {}
                      ],
                      "name": "${azurerm_container_registry.this.login_server}/sample:latest"
                    }
        argumentDocs:
            1 - 1023: GiB
            1- 1023: GiB
            100 - 4096: GiB
            500 - 4096: GiB
            availability: '- (Optional) (String) Availability type used for all instances in the pool. Only ON_DEMAND and SPOT are supported.'
            azure_attributes.availability: '- (Optional) Availability type used for all nodes. Valid values are SPOT_AZURE and ON_DEMAND_AZURE.'
            azure_attributes.spot_bid_max_price: '- (Optional) The max price for Azure spot instances.  Use -1 to specify the lowest price.'
            custom_tags: '- (Optional) (Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS & Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the official documentation). Attempting to set the same tags in both cluster and instance pool will raise an error. Databricks allows at most 43 custom tags.'
            disk_count: '- (Optional) (Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.'
            disk_size: '- (Optional) (Integer) The size of each disk (in GiB) to attach.'
            enable_elastic_disk: '- (Optional) (Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.'
            gcp_attributes.gcp_availability: '- (Optional) Availability type used for all nodes. Valid values are PREEMPTIBLE_GCP, PREEMPTIBLE_WITH_FALLBACK_GCP and ON_DEMAND_GCP, default: ON_DEMAND_GCP.'
            gcp_attributes.local_ssd_count: (optional, int) Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
            id: '- Canonical unique identifier for the instance pool.'
            idle_instance_autotermination_minutes: '- (Required) (Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.'
            instance_pool_name: '- (Required) (String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.'
            max_capacity: '- (Optional) (Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a best practice, this should be set based on anticipated usage.'
            min_idle_instances: '- (Optional) (Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.'
            node_type_id: '- (Required) (String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the List Node Types API call.'
            preloaded_docker_image.basic_auth: '- (Optional) basic_auth.username and basic_auth.password for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.'
            preloaded_docker_image.url: '- URL for the Docker image'
            preloaded_spark_versions: '- (Optional) (List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks_spark_version data source or via  Runtime Versions API call.'
            spot_bid_price_percent: '- (Optional) (Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the default value is 100. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. For safety, this field cannot be greater than 10000.'
            zone_id: '- (Optional) (String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like "us-west-2a". The provided availability zone must be in the same region as the Databricks deployment. For example, "us-west-2a" is not a valid zone ID if the Databricks deployment resides in the "us-east-1" region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the List Zones API.'
        importStatements: []
    databricks_instance_profile:
        subCategory: Security
        name: databricks_instance_profile
        title: ""
        examples:
            - name: instance_profile
              manifest: |-
                {
                  "instance_profile_arn": "my_instance_profile_arn"
                }
              dependencies:
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
                databricks_user_instance_profile.my_user_instance_profile: |-
                    {
                      "instance_profile_id": "${databricks_instance_profile.instance_profile.id}",
                      "user_id": "${databricks_user.my_user.id}"
                    }
        argumentDocs:
            id: '- The id in the format <user_id>|<instance_profile_id>.'
            instance_profile_id: '-  (Required) This is the id of the instance profile resource.'
            user_id: '- (Required) This is the id of the user resource.'
        importStatements: []
    databricks_ip_access_list:
        subCategory: Security
        name: databricks_ip_access_list
        title: ""
        examples:
            - name: allowed-list
              manifest: |-
                {
                  "depends_on": [
                    "${databricks_workspace_conf.this}"
                  ],
                  "ip_addresses": [
                    "1.1.1.1",
                    "1.2.3.0/24",
                    "1.2.5.0/24"
                  ],
                  "label": "allow_in",
                  "list_type": "ALLOW"
                }
              dependencies:
                databricks_workspace_conf.this: |-
                    {
                      "custom_config": {
                        "enableIpAccessLists": true
                      }
                    }
        argumentDocs:
            enabled: '- (Optional) Boolean true or false indicating whether this list should be active.  Defaults to true'
            id: '- Canonical unique identifier for the IP Access List, same as list_id.'
            ip_addresses: '- A string list of IP addresses and CIDR ranges.'
            label: '-  This is the display name for the given IP ACL List.'
            list_id: '- Canonical unique identifier for the IP Access List.'
            list_type: '-  Can only be "ALLOW" or "BLOCK".'
        importStatements: []
    databricks_job:
        subCategory: Compute
        name: databricks_job
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "description": "This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.",
                  "job_cluster": [
                    {
                      "job_cluster_key": "j",
                      "new_cluster": [
                        {
                          "node_type_id": "${data.databricks_node_type.smallest.id}",
                          "num_workers": 2,
                          "spark_version": "${data.databricks_spark_version.latest.id}"
                        }
                      ]
                    }
                  ],
                  "name": "Job with multiple tasks",
                  "task": [
                    {
                      "new_cluster": [
                        {
                          "node_type_id": "${data.databricks_node_type.smallest.id}",
                          "num_workers": 1,
                          "spark_version": "${data.databricks_spark_version.latest.id}"
                        }
                      ],
                      "notebook_task": [
                        {
                          "notebook_path": "${databricks_notebook.this.path}"
                        }
                      ],
                      "task_key": "a"
                    },
                    {
                      "depends_on": [
                        {
                          "task_key": "a"
                        }
                      ],
                      "existing_cluster_id": "${databricks_cluster.shared.id}",
                      "spark_jar_task": [
                        {
                          "main_class_name": "com.acme.data.Main"
                        }
                      ],
                      "task_key": "b"
                    },
                    {
                      "job_cluster_key": "j",
                      "notebook_task": [
                        {
                          "notebook_path": "${databricks_notebook.this.path}"
                        }
                      ],
                      "task_key": "c"
                    },
                    {
                      "pipeline_task": [
                        {
                          "pipeline_id": "${databricks_pipeline.this.id}"
                        }
                      ],
                      "task_key": "d"
                    }
                  ]
                }
              references:
                job_cluster.new_cluster.node_type_id: data.databricks_node_type.smallest.id
                job_cluster.new_cluster.spark_version: data.databricks_spark_version.latest.id
                task.existing_cluster_id: databricks_cluster.shared.id
                task.new_cluster.node_type_id: data.databricks_node_type.smallest.id
                task.new_cluster.spark_version: data.databricks_spark_version.latest.id
                task.notebook_task.notebook_path: databricks_notebook.this.path
                task.pipeline_task.pipeline_id: databricks_pipeline.this.id
            - name: this
              manifest: |-
                {
                  "tags": {
                    "environment": "dev",
                    "owner": "dream-team"
                  }
                }
            - name: this
              manifest: |-
                {
                  "run_as": [
                    {
                      "service_principal_name": "8d23ae77-912e-4a19-81e4-b9c3f5cc9349"
                    }
                  ]
                }
            - name: sql_aggregation_job
              manifest: |-
                {
                  "name": "Example SQL Job",
                  "task": [
                    {
                      "sql_task": [
                        {
                          "query": [
                            {
                              "query_id": "${databricks_sql_query.agg_query.id}"
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_agg_query"
                    },
                    {
                      "sql_task": [
                        {
                          "dashboard": [
                            {
                              "dashboard_id": "${databricks_sql_dashboard.dash.id}",
                              "subscriptions": [
                                {
                                  "user_name": "user@domain.com"
                                }
                              ]
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_dashboard"
                    },
                    {
                      "sql_task": [
                        {
                          "alert": [
                            {
                              "alert_id": "${databricks_sql_alert.alert.id}",
                              "subscriptions": [
                                {
                                  "user_name": "user@domain.com"
                                }
                              ]
                            }
                          ],
                          "warehouse_id": "${databricks_sql_endpoint.sql_job_warehouse.id}"
                        }
                      ],
                      "task_key": "run_alert"
                    }
                  ]
                }
              references:
                task.sql_task.alert.alert_id: databricks_sql_alert.alert.id
                task.sql_task.dashboard.dashboard_id: databricks_sql_dashboard.dash.id
                task.sql_task.query.query_id: databricks_sql_query.agg_query.id
                task.sql_task.warehouse_id: databricks_sql_endpoint.sql_job_warehouse.id
            - name: this
              manifest: |-
                {
                  "name": "Terraform Demo (${data.databricks_current_user.me.alphanumeric})",
                  "new_cluster": [
                    {
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "num_workers": 1,
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                  ],
                  "notebook_task": [
                    {
                      "notebook_path": "${databricks_notebook.this.path}"
                    }
                  ]
                }
              references:
                new_cluster.node_type_id: data.databricks_node_type.smallest.id
                new_cluster.spark_version: data.databricks_spark_version.latest.id
                notebook_task.notebook_path: databricks_notebook.this.path
              dependencies:
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/Terraform"
                    }
        argumentDocs:
            '*_task': '- (Required) one of the specific task blocks described below:'
            alert: '- (Optional) block consisting of following fields:'
            alert_id: '- (Required) (String) identifier of the Databricks SQL Alert.'
            alert_on_last_attempt: '- (Optional) (Bool) do not send notifications to recipients specified in on_start for the retried runs and do not send notifications to recipients specified in on_failure until the last retry of the run.'
            always_running: '- (Optional, Deprecated) (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with parameters specified in spark_jar_task or spark_submit_task or spark_python_task or notebook_task blocks.'
            base_parameters: '- (Optional) (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using dbutils.widgets.get.'
            branch: '- name of the Git branch to use. Conflicts with tag and commit.'
            catalog: '- (Optional) The name of the catalog to use inside Unity Catalog.'
            commands: '- (Required) (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".'
            commit: '- hash of Git commit to use. Conflicts with branch and tag.'
            concurrency: '- (Optional) Controls the number of active iteration task runs. Default is 20, maximum allowed is 100.'
            control_run_state: '- (Optional) (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the pause_status by stopping the current active run. This flag cannot be set for non-continuous jobs.'
            custom_subject: '- (Optional) string specifying a custom subject of email sent.'
            dashboard: '- (Optional) block consisting of following fields:'
            dashboard_id: '- (Required) (String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.'
            default: '- (Required) Default value of the parameter.'
            depends_on: '- (Optional) block specifying dependency(-ies) for a given task.'
            description: '- (Optional) An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.'
            email_notifications: '- (Optional) (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.'
            email_notifications.no_alert_for_skipped_runs: '- (Optional) (Bool) don''t send alert for skipped runs. (It''s recommended to use the corresponding setting in the notification_settings configuration block).'
            email_notifications.on_duration_warning_threshold_exceeded: '- (Optional) (List) list of emails to notify when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block.'
            email_notifications.on_failure: '- (Optional) (List) list of emails to notify when the run fails.'
            email_notifications.on_start: '- (Optional) (List) list of emails to notify when the run starts.'
            email_notifications.on_success: '- (Optional) (List) list of emails to notify when the run completes successfully.'
            enabled: '- (Required) If true, enable queueing for the job.'
            entry_point: '- (Optional) Python function as entry point for the task'
            file: '- (Optional) block consisting of single string fields:'
            file_arrival: '- (Required) configuration block to define a trigger for File Arrival events consisting of following attributes:'
            full_refresh: '- (Optional) (Bool) Specifies if there should be full refresh of the pipeline.'
            health: '- (Optional) An optional block that specifies the health conditions for the job (described below).'
            id: '- ID of the system notification that is notified when an event defined in webhook_notifications is triggered.'
            inputs: '- (Required) (String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter.'
            job_cluster: '- (Optional) A list of job databricks_cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. Multi-task syntax'
            job_cluster.job_cluster_key: '- (Required) Identifier that can be referenced in task block, so that cluster is shared between tasks'
            job_cluster.new_cluster: '- Same set of parameters as for databricks_cluster resource.'
            job_id: '- (Required)(String) ID of the job'
            job_parameters: '- (Optional)(Map) Job parameters for the task'
            left: '- The left operand of the condition task. It could be a string value, job state, or a parameter reference.'
            library: '- (Optional) (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks_cluster resource.'
            main_class_name: '- (Optional) The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use SparkContext.getOrCreate to obtain a Spark context; otherwise, runs of the job will fail.'
            max_concurrent_runs: '- (Optional) (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to 1.'
            max_retries: '- (Optional) (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR.'
            metric: '- (Optional) string specifying the metric to check.  The only supported metric is RUN_DURATION_SECONDS (check Jobs REST API documentation for the latest information).'
            min_retry_interval_millis: '- (Optional) (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.'
            min_time_between_triggers_seconds: '- (Optional) If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.'
            name: '- (Optional) An optional name for the job. The default value is Untitled.'
            name.existing_cluster_id: '- (Optional) If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use new_cluster for greater reliability.'
            name.new_cluster: '- (Optional) Same set of parameters as for databricks_cluster resource.'
            named_parameters: '- (Optional) Named parameters for the task'
            no_alert_for_canceled_runs: '- (Optional) (Bool) don''t send alert for cancelled runs.'
            no_alert_for_skipped_runs: '- (Optional) (Bool) don''t send alert for skipped runs.'
            notebook_path: '- (Required) The path of the databricks_notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.'
            notification_settings: '- (Optional) An optional block controlling the notification settings on the job level (described below).'
            op: '- (Optional) string specifying the operation used to evaluate the given metric. The only supported operation is GREATER_THAN.'
            outcome: '- (Optional, string) Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are "true" or "false".'
            package_name: '- (Optional) Name of Python package'
            parameters: '- (Optional) (List) Parameters passed to the main method.'
            path: '- If source is GIT: Relative path to the file in the repository specified in the git_source block with SQL commands to execute. If source is WORKSPACE: Absolute path to the file in the workspace with SQL commands to execute.'
            pause_status: '- (Optional) Indicate whether this continuous job is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted in the block, the server will default to using UNPAUSED as a value for pause_status.'
            pause_subscriptions: '- (Optional) flag that specifies if subscriptions are paused or not.'
            pipeline_id: '- (Required) The pipeline''s unique ID.'
            profiles_directory: '- (Optional) The relative path to the directory in the repository specified by git_source where dbt should look in for the profiles.yml file. If not specified, defaults to the repository''s root directory. Equivalent to passing --profile-dir to a dbt command.'
            project_directory: '- (Required when source is WORKSPACE) The path where dbt should look for dbt_project.yml. Equivalent to passing --project-dir to the dbt CLI.'
            provider: '- (Optional, if it''s possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition.'
            python_file: '- (Required) The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. s3:/, abfss:/, gs:/), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with /Repos. For files stored in a remote repository, the path must be relative. This field is required.'
            query: '- (Optional) block consisting of single string field: query_id - identifier of the Databricks SQL Query (databricks_sql_query).'
            retry_on_timeout: '- (Optional) (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.'
            right: '- The right operand of the condition task. It could be a string value, job state, or parameter reference.'
            rules: '- (List) list of rules that are represented as objects with the following attributes:'
            run_as.service_principal_name: '- (Optional) The application ID of an active service principal. Setting this field requires the servicePrincipal/user role.'
            run_as.user_name: '- (Optional) The email of an active workspace user. Non-admin users can only set this field to their own email.'
            run_if: '- (Optional) An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to ALL_SUCCESS.'
            schedule: '- (Optional) (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.'
            schedule.pause_status: '- (Optional) Indicate whether this schedule is paused or not. Either PAUSED or UNPAUSED. When the pause_status field is omitted and a schedule is provided, the server will default to using UNPAUSED as a value for pause_status.'
            schedule.quartz_cron_expression: '- (Required) A Cron expression using Quartz syntax that describes the schedule for a job. This field is required.'
            schedule.timezone_id: '- (Required) A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.'
            schema: '- (Optional) The name of the schema dbt should run in. Defaults to default.'
            source: '- (Optional) Location type of the Python file, can only be GIT. When set to GIT, the Python file will be retrieved from a Git repository defined in git_source.'
            spark_version: parameter in databricks_cluster and other resources.
            subscriptions: '- (Optional) a list of subscription blocks consisting out of one of the required fields: user_name for user emails or destination_id - for Alert destination''s identifier.'
            tag: '- name of the Git branch to use. Conflicts with branch and commit.'
            task: '- (Required) Task to run against the inputs list.'
            task_key: '- (Required) string specifying an unique key for a given task.'
            timeout_seconds: '- (Optional) (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.'
            url: '- (Required) string with URL under the Unity Catalog external location that will be monitored for new files. Please note that have a trailing slash character (/).'
            value: '- (Optional) integer value used to compare to the given metric.'
            wait_after_last_change_seconds: '- (Optional) If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.'
            warehouse_id: '- (Optional) The ID of the SQL warehouse that dbt should execute against.'
            webhook_notification.on_duration_warning_threshold_exceeded: '- (Optional) (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the RUN_DURATION_SECONDS metric in the health block.'
            webhook_notification.on_failure: '- (Optional) (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.'
            webhook_notification.on_start: '- (Optional) (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.'
            webhook_notification.on_success: '- (Optional) (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.'
            webhook_notifications: '- (Optional) (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.'
        importStatements: []
    databricks_library:
        subCategory: Compute
        name: databricks_library
        title: ""
        examples:
            - name: cli
              manifest: |-
                {
                  "cluster_id": "${each.key}",
                  "for_each": "${data.databricks_clusters.all.ids}",
                  "pypi": [
                    {
                      "package": "databricks-cli"
                    }
                  ]
                }
              references:
                cluster_id: each.key
                for_each: data.databricks_clusters.all.ids
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "jar": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                jar: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/app-0.0.1.jar",
                      "source": "${path.module}/app-0.0.1.jar"
                    }
            - name: deequ
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "maven": [
                    {
                      "coordinates": "com.amazon.deequ:deequ:1.0.4",
                      "exclusions": [
                        "org.apache.avro:avro"
                      ]
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "whl": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                whl: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/baz.whl",
                      "source": "${path.module}/baz.whl"
                    }
            - name: fbprophet
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "pypi": [
                    {
                      "package": "fbprophet==0.6"
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
            - name: app
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "egg": "${databricks_dbfs_file.app.dbfs_path}"
                }
              references:
                cluster_id: databricks_cluster.this.id
                egg: databricks_dbfs_file.app.dbfs_path
              dependencies:
                databricks_dbfs_file.app: |-
                    {
                      "path": "/FileStore/foo.egg",
                      "source": "${path.module}/foo.egg"
                    }
            - name: rkeops
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.this.id}",
                  "cran": [
                    {
                      "package": "rkeops"
                    }
                  ]
                }
              references:
                cluster_id: databricks_cluster.this.id
        argumentDocs:
            dbfs:/mnt/name: .
        importStatements: []
    databricks_metastore:
        subCategory: Unity Catalog
        name: databricks_metastore
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "us-east-1",
                  "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                }
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "eastus",
                  "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                }
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
            - name: this
              manifest: |-
                {
                  "force_destroy": true,
                  "name": "primary",
                  "owner": "uc admins",
                  "region": "${us-east1}",
                  "storage_root": "gs://${google_storage_bucket.unity_metastore.name}"
                }
              references:
                region: us-east1
              dependencies:
                databricks_metastore_assignment.this: |-
                    {
                      "metastore_id": "${databricks_metastore.this.id}",
                      "workspace_id": "${local.workspace_id}"
                    }
        argumentDocs:
            delta_sharing_organization_name: '- (Optional) The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.'
            delta_sharing_recipient_token_lifetime_in_seconds: '- (Optional) Required along with delta_sharing_scope. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.'
            delta_sharing_scope: '- (Optional) Required along with delta_sharing_recipient_token_lifetime_in_seconds. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.'
            force_destroy: '- (Optional) Destroy metastore regardless of its contents.'
            id: '- system-generated ID of this Unity Catalog Metastore.'
            name: '- Name of metastore.'
            owner: '- (Optional) Username/groupname/sp application_id of the metastore owner.'
            region: '- (Mandatory for account-level) The region of the metastore'
            storage_root: '- (Optional) Path on cloud storage account, where managed databricks_table are stored. Change forces creation of a new resource. If no storage_root is defined for the metastore, each catalog must have a storage_root defined.'
        importStatements: []
    databricks_metastore_assignment:
        subCategory: Unity Catalog
        name: databricks_metastore_assignment
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "metastore_id": "${databricks_metastore.this.id}",
                  "workspace_id": "${local.workspace_id}"
                }
              references:
                metastore_id: databricks_metastore.this.id
                workspace_id: local.workspace_id
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "us-east-1",
                      "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                    }
        argumentDocs:
            default_catalog_name: '- (Optional) Default catalog used for this assignment, default to hive_metastore'
            id: '- ID of this metastore assignment in form of <metastore_id>|<metastore_id>.'
            metastore_id: '- Unique identifier of the parent Metastore'
            workspace_id: '- id of the workspace for the assignment'
        importStatements: []
    databricks_metastore_data_access:
        subCategory: Unity Catalog
        name: databricks_metastore_data_access
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.metastore_data_access.arn}"
                    }
                  ],
                  "is_default": true,
                  "metastore_id": "${databricks_metastore.this.id}",
                  "name": "${aws_iam_role.metastore_data_access.name}"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.metastore_data_access.arn
                metastore_id: databricks_metastore.this.id
                name: aws_iam_role.metastore_data_access.name
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "us-east-1",
                      "storage_root": "s3://${aws_s3_bucket.metastore.id}/metastore"
                    }
            - name: this
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${var.access_connector_id}"
                    }
                  ],
                  "is_default": true,
                  "metastore_id": "${databricks_metastore.this.id}",
                  "name": "mi_dac"
                }
              references:
                azure_managed_identity.access_connector_id: var.access_connector_id
                metastore_id: databricks_metastore.this.id
              dependencies:
                databricks_metastore.this: |-
                    {
                      "force_destroy": true,
                      "name": "primary",
                      "owner": "uc admins",
                      "region": "eastus",
                      "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                    }
        argumentDocs:
            id: '- ID of this data access configuration in form of <metastore_id>|<name>.'
            is_default: '-  whether to set this credential as the default for the metastore. In practice, this should always be true.'
        importStatements: []
    databricks_mlflow_experiment:
        subCategory: MLflow
        name: databricks_mlflow_experiment
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "artifact_location": "dbfs:/tmp/my-experiment",
                  "description": "My MLflow experiment description",
                  "name": "${data.databricks_current_user.me.home}/Sample"
                }
        argumentDocs:
            artifact_location: '- Path to dbfs:/ or s3:// artifact location of the MLflow experiment.'
            description: '- The description of the MLflow experiment.'
            id: '- ID of the MLflow experiment.'
            name: '- (Required) Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. /Users/<some-username>/my-experiment. For more information about changes to experiment naming conventions, see mlflow docs.'
        importStatements: []
    databricks_mlflow_model:
        subCategory: MLflow
        name: databricks_mlflow_model
        title: ""
        examples:
            - name: test
              manifest: |-
                {
                  "description": "My MLflow model description",
                  "name": "My MLflow Model",
                  "tags": [
                    {
                      "key": "key1",
                      "value": "value1"
                    },
                    {
                      "key": "key2",
                      "value": "value2"
                    }
                  ]
                }
        argumentDocs:
            description: '- The description of the MLflow model.'
            id: '- ID of the MLflow model, the same as name.'
            name: '- (Required) Name of MLflow model. Change of name triggers new resource.'
            tags: '- Tags for the MLflow model.'
        importStatements: []
    databricks_mlflow_webhook:
        subCategory: MLflow
        name: databricks_mlflow_webhook
        title: ""
        examples:
            - name: job
              manifest: |-
                {
                  "description": "Databricks Job webhook trigger",
                  "events": [
                    "TRANSITION_REQUEST_CREATED"
                  ],
                  "job_spec": [
                    {
                      "access_token": "${databricks_token.pat_for_webhook.token_value}",
                      "job_id": "${databricks_job.this.id}",
                      "workspace_url": "${data.databricks_current_user.me.workspace_url}"
                    }
                  ],
                  "status": "ACTIVE"
                }
              references:
                job_spec.access_token: databricks_token.pat_for_webhook.token_value
                job_spec.job_id: databricks_job.this.id
                job_spec.workspace_url: data.databricks_current_user.me.workspace_url
              dependencies:
                databricks_job.this: |-
                    {
                      "name": "Terraform MLflowWebhook Demo (${data.databricks_current_user.me.alphanumeric})",
                      "task": [
                        {
                          "new_cluster": [
                            {
                              "node_type_id": "${data.databricks_node_type.smallest.id}",
                              "num_workers": 1,
                              "spark_version": "${data.databricks_spark_version.latest.id}"
                            }
                          ],
                          "notebook_task": [
                            {
                              "notebook_path": "${databricks_notebook.this.path}"
                            }
                          ],
                          "task_key": "task1"
                        }
                      ]
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    import json\n \n    event_message = dbutils.widgets.get(\"event_message\")\n    event_message_dict = json.loads(event_message)\n    print(f\"event data={event_message_dict}\")\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/MLFlowWebhook"
                    }
                databricks_token.pat_for_webhook: |-
                    {
                      "comment": "MLflow Webhook",
                      "lifetime_seconds": 86400000
                    }
            - name: url
              manifest: |-
                {
                  "description": "URL webhook trigger",
                  "events": [
                    "TRANSITION_REQUEST_CREATED"
                  ],
                  "http_url_spec": [
                    {
                      "url": "https://my_cool_host/webhook"
                    }
                  ]
                }
        argumentDocs:
            access_token: '- (Required) The personal access token used to authorize webhook''s job runs.'
            authorization: '- (Optional) Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form <auth type> <credentials>, e.g. Bearer <access_token>. If set to an empty string, no authorization header will be included in the request.'
            description: '- Optional description of the MLflow webhook.'
            enable_ssl_verification: '- (Optional) Enable/disable SSL certificate validation. Default is true. For self-signed certificates, this field must be false AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.'
            events: '- (Required) The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, MODEL_VERSION_CREATED, MODEL_VERSION_TRANSITIONED_STAGE, TRANSITION_REQUEST_CREATED, etc.  Refer to the Webhooks API documentation for a full list of supported events.'
            id: '- Unique ID of the MLflow Webhook.'
            job_id: '- (Required) ID of the Databricks job that the webhook runs.'
            model_name: '- (Optional) Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.'
            secret: '- (Optional) Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as X-Databricks-Signature: encoded_payload.'
            status: '- Optional status of webhook. Possible values are ACTIVE, TEST_MODE, DISABLED. Default is ACTIVE.'
            url: '- (Required) External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to documentation for more details.'
            workspace_url: '- (Optional) URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.'
        importStatements: []
    databricks_model_serving:
        subCategory: Serving
        name: databricks_model_serving
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "config": [
                    {
                      "served_models": [
                        {
                          "model_name": "ads-model",
                          "model_version": "2",
                          "name": "prod_model",
                          "scale_to_zero_enabled": true,
                          "workload_size": "Small"
                        },
                        {
                          "model_name": "ads-model",
                          "model_version": "4",
                          "name": "candidate_model",
                          "scale_to_zero_enabled": false,
                          "workload_size": "Small"
                        }
                      ],
                      "traffic_config": [
                        {
                          "routes": [
                            {
                              "served_model_name": "prod_model",
                              "traffic_percentage": 90
                            },
                            {
                              "served_model_name": "candidate_model",
                              "traffic_percentage": 10
                            }
                          ]
                        }
                      ]
                    }
                  ],
                  "name": "ads-serving-endpoint"
                }
        argumentDocs:
            config: '- (Required) The model serving endpoint configuration.'
            config.served_models: '- (Required) Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.'
            config.traffic_config: '- A single block represents the traffic split configuration amongst the served models.'
            environment_vars: '- (Optional) a map of environment variable name/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: {{secrets/secret_scope/secret_key}}.'
            id: '- Equal to the name argument and used to identify the serving endpoint.'
            instance_profile_arn: '- (Optional) ARN of the instance profile that the served model will use to access AWS resources.'
            model_name: '- (Required) The name of the model in Databricks Model Registry to be served.'
            model_version: '- (Required) The version of the model in Databricks Model Registry to be served.'
            name: '- (Required) The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the update name.'
            routes: '- (Required) Each block represents a route that defines traffic to each served model. Each served_models block needs to have a corresponding routes block'
            routes.served_model_name: '- (Required) The name of the served model this route configures traffic for. This needs to match the name of a served_models block'
            routes.traffic_percentage: '- (Required) The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.'
            scale_to_zero_enabled: '- Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is true.'
            serving_endpoint_id: '- Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.'
            workload_size: '- (Required) The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are "Small" (4 - 4 provisioned concurrency), "Medium" (8 - 16 provisioned concurrency), and "Large" (16 - 64 provisioned concurrency).'
            workload_type: '- The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like GPU_SMALL and others. See documentation for all options. The default value is CPU.'
        importStatements: []
    databricks_mount:
        subCategory: Storage
        name: databricks_mount
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "extra_configs": {
                    "fs.azure.account.auth.type": "OAuth",
                    "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/${local.tenant_id}/oauth2/token",
                    "fs.azure.account.oauth2.client.id": "${local.client_id}",
                    "fs.azure.account.oauth2.client.secret": "{{secrets/${local.secret_scope}/${local.secret_key}}}",
                    "fs.azure.createRemoteFileSystemDuringInitialization": "false"
                  },
                  "name": "tf-abfss",
                  "uri": "abfss://${local.container}@${local.storage_acc}.dfs.core.windows.net"
                }
            - name: passthrough
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.shared_passthrough.id}",
                  "extra_configs": {
                    "fs.azure.account.auth.type": "CustomAccessToken",
                    "fs.azure.account.custom.token.provider.class": "{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}"
                  },
                  "name": "passthrough-test",
                  "uri": "abfss://${var.container}@${var.storage_acc}.dfs.core.windows.net"
                }
              references:
                cluster_id: databricks_cluster.shared_passthrough.id
              dependencies:
                databricks_cluster.shared_passthrough: |-
                    {
                      "autotermination_minutes": 10,
                      "cluster_name": "Shared Passthrough for mount",
                      "custom_tags": {
                        "ResourceClass": "Serverless"
                      },
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "num_workers": 1,
                      "spark_conf": {
                        "spark.databricks.cluster.profile": "serverless",
                        "spark.databricks.passthrough.enabled": "true",
                        "spark.databricks.pyspark.enableProcessIsolation": "true",
                        "spark.databricks.repl.allowedLanguages": "python,sql"
                      },
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
            - name: this
              manifest: |-
                {
                  "name": "experiments",
                  "s3": [
                    {
                      "bucket_name": "${aws_s3_bucket.this.bucket}",
                      "instance_profile": "${databricks_instance_profile.ds.id}"
                    }
                  ]
                }
              references:
                s3.bucket_name: aws_s3_bucket.this.bucket
                s3.instance_profile: databricks_instance_profile.ds.id
            - name: marketing
              manifest: |-
                {
                  "abfs": [
                    {
                      "client_id": "${data.azurerm_client_config.current.client_id}",
                      "client_secret_key": "${databricks_secret.service_principal_key.key}",
                      "client_secret_scope": "${databricks_secret_scope.terraform.name}",
                      "initialize_file_system": true
                    }
                  ],
                  "name": "marketing",
                  "resource_id": "${azurerm_storage_container.this.resource_manager_id}"
                }
              references:
                abfs.client_id: data.azurerm_client_config.current.client_id
                abfs.client_secret_key: databricks_secret.service_principal_key.key
                abfs.client_secret_scope: databricks_secret_scope.terraform.name
                resource_id: azurerm_storage_container.this.resource_manager_id
              dependencies:
                azurerm_role_assignment.this: |-
                    {
                      "principal_id": "${data.azurerm_client_config.current.object_id}",
                      "role_definition_name": "Storage Blob Data Contributor",
                      "scope": "${azurerm_storage_account.this.id}"
                    }
                azurerm_storage_account.this: |-
                    {
                      "account_kind": "StorageV2",
                      "account_replication_type": "GRS",
                      "account_tier": "Standard",
                      "is_hns_enabled": true,
                      "location": "${var.resource_group_location}",
                      "name": "${var.prefix}datalake",
                      "resource_group_name": "${var.resource_group_name}"
                    }
                azurerm_storage_container.this: |-
                    {
                      "container_access_type": "private",
                      "name": "marketing",
                      "storage_account_name": "${azurerm_storage_account.this.name}"
                    }
                databricks_secret.service_principal_key: |-
                    {
                      "key": "service_principal_key",
                      "scope": "${databricks_secret_scope.terraform.name}",
                      "string_value": "${var.ARM_CLIENT_SECRET}"
                    }
                databricks_secret_scope.terraform: |-
                    {
                      "initial_manage_principal": "users",
                      "name": "application"
                    }
            - name: this_gs
              manifest: |-
                {
                  "gs": [
                    {
                      "bucket_name": "mybucket",
                      "service_account": "acc@company.iam.gserviceaccount.com"
                    }
                  ],
                  "name": "gs-mount"
                }
            - name: mount
              manifest: |-
                {
                  "adl": [
                    {
                      "client_id": "${data.azurerm_client_config.current.client_id}",
                      "client_secret_key": "${databricks_secret.service_principal_key.key}",
                      "client_secret_scope": "${databricks_secret_scope.terraform.name}",
                      "spark_conf_prefix": "fs.adl",
                      "storage_resource_name": "{env.TEST_STORAGE_ACCOUNT_NAME}",
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
                  ],
                  "name": "{var.RANDOM}"
                }
              references:
                adl.client_id: data.azurerm_client_config.current.client_id
                adl.client_secret_key: databricks_secret.service_principal_key.key
                adl.client_secret_scope: databricks_secret_scope.terraform.name
                adl.tenant_id: data.azurerm_client_config.current.tenant_id
            - name: marketing
              manifest: |-
                {
                  "name": "marketing",
                  "wasb": [
                    {
                      "auth_type": "ACCESS_KEY",
                      "container_name": "${azurerm_storage_container.marketing.name}",
                      "storage_account_name": "${azurerm_storage_account.blobaccount.name}",
                      "token_secret_key": "${databricks_secret.storage_key.key}",
                      "token_secret_scope": "${databricks_secret_scope.terraform.name}"
                    }
                  ]
                }
              references:
                wasb.container_name: azurerm_storage_container.marketing.name
                wasb.storage_account_name: azurerm_storage_account.blobaccount.name
                wasb.token_secret_key: databricks_secret.storage_key.key
                wasb.token_secret_scope: databricks_secret_scope.terraform.name
              dependencies:
                azurerm_storage_account.blobaccount: |-
                    {
                      "account_kind": "StorageV2",
                      "account_replication_type": "LRS",
                      "account_tier": "Standard",
                      "location": "${var.resource_group_location}",
                      "name": "${var.prefix}blob",
                      "resource_group_name": "${var.resource_group_name}"
                    }
                azurerm_storage_container.marketing: |-
                    {
                      "container_access_type": "private",
                      "name": "marketing",
                      "storage_account_name": "${azurerm_storage_account.blobaccount.name}"
                    }
                databricks_secret.storage_key: |-
                    {
                      "key": "blob_storage_key",
                      "scope": "${databricks_secret_scope.terraform.name}",
                      "string_value": "${azurerm_storage_account.blobaccount.primary_access_key}"
                    }
                databricks_secret_scope.terraform: |-
                    {
                      "initial_manage_principal": "users",
                      "name": "application"
                    }
        argumentDocs:
            abfs: '- to mount ADLS Gen2 using Azure Blob Filesystem (ABFS) driver'
            abfs.client_id: '- (Required) (String) This is the client_id (Application Object ID) for the enterprise application for the service principal.'
            abfs.client_secret_key: '- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.'
            abfs.client_secret_scope: '- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.'
            abfs.container_name: '- (Required) (String) ADLS gen2 container name. (Could be omitted if resource_id is provided)'
            abfs.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            abfs.initialize_file_system: '- (Required) (Bool) either or not initialize FS for the first use'
            abfs.storage_account_name: '- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)'
            abfs.tenant_id: '- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it).'
            adl: '- to mount ADLS Gen1 using Azure Data Lake (ADL) driver'
            adl.client_id: '- (Required) (String) This is the client_id for the enterprise application for the service principal.'
            adl.client_secret_key: '- (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.'
            adl.client_secret_scope: '- (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.'
            adl.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            adl.spark_conf_prefix: '- (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are fs.adl, dfs.adls. Use fs.adl for runtime 6.0 and above for the clusters. Otherwise use dfs.adls. The default value is: fs.adl.'
            adl.storage_resource_name: '- (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if resource_id is provided)'
            adl.tenant_id: '- (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract tenant_id from it)'
            bucket_name: for AWS S3 and Google Cloud Storage
            cluster_id: '- (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it''s going to be started, so be aware to set auto-termination rules on it.'
            container_name: for ADLS Gen2 and Azure Blob Storage
            encryption_type: '- (Optional, String) encryption type. Currently used only for AWS S3 mounts'
            extra_configs: '- (Optional, String map) configuration parameters that are necessary for mounting of specific storage'
            gs: '- to mount Google Cloud Storage'
            gs.bucket_name: '- (Required) (String) GCS bucket name to be mounted.'
            gs.service_account: '- (Optional) (String) email of registered Google Service Account for data access.  If it''s not specified, then the cluster_id should be provided, and the cluster should have a Google service account attached to it.'
            id: '- mount name'
            mount_name: to name
            name: '- (Optional, String) Name, under which mount will be accessible in dbfs:/mnt/<MOUNT_NAME>. If not specified, provider will try to infer it from depending on the resource type:'
            resource_id: '- (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account & container names on Azure.'
            s3: '- to mount AWS S3'
            s3.bucket_name: '- (Required) (String) S3 bucket name to be mounted.'
            s3.instance_profile: '- (Optional) (String) ARN of registered instance profile for data access.  If it''s not specified, then the cluster_id should be provided, and the cluster should have an instance profile attached to it. If both cluster_id & instance_profile are specified, then cluster_id takes precedence.'
            s3_bucket_name: to bucket_name
            source: '- (String) HDFS-compatible url'
            storage_resource_name: for ADLS Gen1
            uri: '- (Optional, String) the URI for accessing specific storage (s3a://...., abfss://...., gs://...., etc.)'
            wasb: '- to mount Azure Blob Storage using Windows Azure Storage Blob (WASB) driver'
            wasb.auth_type: '- (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (SAS) or account access keys (ACCESS_KEY).'
            wasb.container_name: '- (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if resource_id is provided)'
            wasb.directory: '- (Computed) (String) This is optional if you don''t want to add an additional directory that you wish to mount. This must start with a "/".'
            wasb.storage_account_name: '- (Required) (String) The name of the storage resource in which the data is. (Could be omitted if resource_id is provided)'
            wasb.token_secret_key: '- (Required) (String) This is the secret key in which your auth type token is stored.'
            wasb.token_secret_scope: '- (Required) (String) This is the secret scope in which your auth type token is stored.'
        importStatements: []
    databricks_mws_credentials:
        subCategory: Deployment
        name: databricks_mws_credentials
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "credentials_name": "${local.prefix}-creds",
                  "provider": "${databricks.mws}",
                  "role_arn": "${aws_iam_role.cross_account_role.arn}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                role_arn: aws_iam_role.cross_account_role.arn
              dependencies:
                aws_iam_role.cross_account_role: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.this.json}",
                      "name": "${local.prefix}-crossaccount",
                      "tags": "${var.tags}"
                    }
                aws_iam_role_policy.this: |-
                    {
                      "name": "${local.prefix}-policy",
                      "policy": "${data.databricks_aws_crossaccount_policy.this.json}",
                      "role": "${aws_iam_role.cross_account_role.id}"
                    }
        argumentDocs:
            account_id: '- (Optional) Account Id that could be found in the top right corner of Accounts Console'
            creation_time: '- (Integer) time of credentials registration'
            credentials_id: '- (String) identifier of credentials'
            credentials_name: '- (Required) name of credentials to register'
            id: '- Canonical unique identifier for the mws credentials.'
            role_arn: '- (Required) ARN of cross-account role'
        importStatements: []
    databricks_mws_customer_managed_keys:
        subCategory: Deployment
        name: databricks_mws_customer_managed_keys
        title: ""
        examples:
            - name: managed_services
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_key_info": [
                    {
                      "key_alias": "${aws_kms_alias.managed_services_customer_managed_key_alias.name}",
                      "key_arn": "${aws_kms_key.managed_services_customer_managed_key.arn}"
                    }
                  ],
                  "use_cases": [
                    "MANAGED_SERVICES"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                aws_key_info.key_alias: aws_kms_alias.managed_services_customer_managed_key_alias.name
                aws_key_info.key_arn: aws_kms_key.managed_services_customer_managed_key.arn
              dependencies:
                aws_kms_alias.managed_services_customer_managed_key_alias: |-
                    {
                      "name": "alias/managed-services-customer-managed-key-alias",
                      "target_key_id": "${aws_kms_key.managed_services_customer_managed_key.key_id}"
                    }
                aws_kms_key.managed_services_customer_managed_key: |-
                    {
                      "policy": "${data.aws_iam_policy_document.databricks_managed_services_cmk.json}"
                    }
            - name: managed_services
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_key_info": [
                    {
                      "kms_key_id": "${var.cmek_resource_id}"
                    }
                  ],
                  "use_cases": [
                    "MANAGED_SERVICES"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_key_info.kms_key_id: var.cmek_resource_id
            - name: storage
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_key_info": [
                    {
                      "key_alias": "${aws_kms_alias.storage_customer_managed_key_alias.name}",
                      "key_arn": "${aws_kms_key.storage_customer_managed_key.arn}"
                    }
                  ],
                  "use_cases": [
                    "STORAGE"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                aws_key_info.key_alias: aws_kms_alias.storage_customer_managed_key_alias.name
                aws_key_info.key_arn: aws_kms_key.storage_customer_managed_key.arn
              dependencies:
                aws_kms_alias.storage_customer_managed_key_alias: |-
                    {
                      "name": "alias/storage-customer-managed-key-alias",
                      "target_key_id": "${aws_kms_key.storage_customer_managed_key.key_id}"
                    }
                aws_kms_key.storage_customer_managed_key: |-
                    {
                      "policy": "${data.aws_iam_policy_document.databricks_storage_cmk.json}"
                    }
            - name: storage
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_key_info": [
                    {
                      "kms_key_id": "${var.cmek_resource_id}"
                    }
                  ],
                  "use_cases": [
                    "STORAGE"
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_key_info.kms_key_id: var.cmek_resource_id
        argumentDocs:
            MANAGED_SERVICES: '- for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane'
            STORAGE: '- for encryption of the DBFS Storage & Cluster EBS Volumes'
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            aws_key_info: '- This field is a block and is documented below. This conflicts with gcp_key_info'
            aws_key_info.key_alias: '- (Optional) The AWS KMS key alias.'
            aws_key_info.key_arn: '- The AWS KMS key''s Amazon Resource Name (ARN).'
            aws_key_info.key_region: '- (Optional) (Computed) The AWS region in which KMS key is deployed to. This is not required.'
            creation_time: '- (Integer) Time in epoch milliseconds when the customer key was created.'
            customer_managed_key_id: '- (String) ID of the encryption key configuration object.'
            gcp_key_info: '- This field is a block and is documented below. This conflicts with aws_key_info'
            gcp_key_info.kms_key_id: '- The GCP KMS key''s resource name.'
            id: '- Canonical unique identifier for the mws customer managed keys.'
            use_cases: '- (since v0.3.4) List of use cases for which this key will be used. If you''ve used the resource before, please add  Possible values are:'
            use_cases = ["MANAGED_SERVICES"]: to keep the previous behaviour.
        importStatements: []
    databricks_mws_log_delivery:
        subCategory: Log Delivery
        name: databricks_mws_log_delivery
        title: ""
        examples:
            - name: usage_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Usage Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "billable-usage",
                  "log_type": "BILLABLE_USAGE",
                  "output_format": "CSV",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
              dependencies:
                aws_iam_role.logdelivery: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.logdelivery.json}",
                      "description": "(${var.prefix}) UsageDelivery role",
                      "name": "${var.prefix}-logdelivery",
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket.logdelivery: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-logdelivery",
                      "force_destroy": true,
                      "tags": "${merge(var.tags, {\n    Name = \"${var.prefix}-logdelivery\"\n  })}"
                    }
                aws_s3_bucket_policy.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "policy": "${data.databricks_aws_bucket_policy.logdelivery.json}"
                    }
                aws_s3_bucket_public_access_block.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "ignore_public_acls": true
                    }
                aws_s3_bucket_versioning.logdelivery_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.log_writer: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "Usage Delivery",
                      "role_arn": "${aws_iam_role.logdelivery.arn}"
                    }
                databricks_mws_storage_configurations.log_bucket: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.logdelivery.bucket}",
                      "storage_configuration_name": "Usage Logs"
                    }
            - name: audit_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Audit Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "audit-logs",
                  "log_type": "AUDIT_LOGS",
                  "output_format": "JSON",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
              dependencies:
                aws_iam_role.logdelivery: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.logdelivery.json}",
                      "description": "(${var.prefix}) UsageDelivery role",
                      "name": "${var.prefix}-logdelivery",
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket.logdelivery: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-logdelivery",
                      "force_destroy": true,
                      "tags": "${merge(var.tags, {\n    Name = \"${var.prefix}-logdelivery\"\n  })}"
                    }
                aws_s3_bucket_policy.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "policy": "${data.databricks_aws_bucket_policy.logdelivery.json}"
                    }
                aws_s3_bucket_public_access_block.logdelivery: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "ignore_public_acls": true
                    }
                aws_s3_bucket_versioning.logdelivery_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.logdelivery.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.log_writer: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "Usage Delivery",
                      "role_arn": "${aws_iam_role.logdelivery.arn}"
                    }
                databricks_mws_storage_configurations.log_bucket: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.logdelivery.bucket}",
                      "storage_configuration_name": "Usage Logs"
                    }
            - name: usage_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Usage Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "billable-usage",
                  "log_type": "BILLABLE_USAGE",
                  "output_format": "CSV",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
            - name: audit_logs
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "config_name": "Audit Logs",
                  "credentials_id": "${databricks_mws_credentials.log_writer.credentials_id}",
                  "delivery_path_prefix": "audit-logs",
                  "log_type": "AUDIT_LOGS",
                  "output_format": "JSON",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.log_writer.credentials_id
                storage_configuration_id: databricks_mws_storage_configurations.log_bucket.storage_configuration_id
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console.'
            config_id: '- Databricks log delivery configuration ID.'
            config_name: '- The optional human-readable name of the log delivery configuration. Defaults to empty.'
            credentials_id: '- The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.'
            delivery_path_prefix: '- (Optional) Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.'
            delivery_start_time: '- (Optional) The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.'
            id: '- the ID of log delivery configuration in form of account_id|config_id.'
            log_type: '- The type of log delivery. BILLABLE_USAGE and AUDIT_LOGS are supported.'
            output_format: '- The file type of log delivery. Currently CSV (for BILLABLE_USAGE) and JSON (for AUDIT_LOGS) are supported.'
            status: '- Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.'
            storage_configuration_id: '- The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.'
            workspace_ids_filter: '- (Optional) By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.'
        importStatements: []
    databricks_mws_networks:
        subCategory: Deployment
        name: databricks_mws_networks
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "network_name": "${local.prefix}-network",
                  "provider": "${databricks.mws}",
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "depends_on": [
                    "${aws_vpc_endpoint.workspace}",
                    "${aws_vpc_endpoint.relay}"
                  ],
                  "network_name": "${local.prefix}-network",
                  "provider": "${databricks.mws}",
                  "security_group_ids": [
                    "${module.vpc.default_security_group_id}"
                  ],
                  "subnet_ids": "${module.vpc.private_subnets}",
                  "vpc_endpoints": [
                    {
                      "dataplane_relay": [
                        "${databricks_mws_vpc_endpoint.relay.vpc_endpoint_id}"
                      ],
                      "rest_api": [
                        "${databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id}"
                      ]
                    }
                  ],
                  "vpc_id": "${module.vpc.vpc_id}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                subnet_ids: module.vpc.private_subnets
                vpc_id: module.vpc.vpc_id
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_network_info": [
                    {
                      "network_project_id": "${var.google_project}",
                      "pod_ip_range_name": "pods",
                      "service_ip_range_name": "svc",
                      "subnet_id": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.name}",
                      "subnet_region": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.region}",
                      "vpc_id": "${google_compute_network.dbx_private_vpc.name}"
                    }
                  ],
                  "network_name": "test-demo-${random_string.suffix.result}"
                }
              references:
                account_id: var.databricks_account_id
                gcp_network_info.network_project_id: var.google_project
                gcp_network_info.subnet_id: google_compute_subnetwork.network_with_private_secondary_ip_ranges.name
                gcp_network_info.subnet_region: google_compute_subnetwork.network_with_private_secondary_ip_ranges.region
                gcp_network_info.vpc_id: google_compute_network.dbx_private_vpc.name
              dependencies:
                google_compute_network.dbx_private_vpc: |-
                    {
                      "auto_create_subnetworks": false,
                      "name": "tf-network-${random_string.suffix.result}",
                      "project": "${var.google_project}"
                    }
                google_compute_router.router: |-
                    {
                      "name": "my-router-${random_string.suffix.result}",
                      "network": "${google_compute_network.dbx_private_vpc.id}",
                      "region": "${google_compute_subnetwork.network-with-private-secondary-ip-ranges.region}"
                    }
                google_compute_router_nat.nat: |-
                    {
                      "name": "my-router-nat-${random_string.suffix.result}",
                      "nat_ip_allocate_option": "AUTO_ONLY",
                      "region": "${google_compute_router.router.region}",
                      "router": "${google_compute_router.router.name}",
                      "source_subnetwork_ip_ranges_to_nat": "ALL_SUBNETWORKS_ALL_IP_RANGES"
                    }
                google_compute_subnetwork.network-with-private-secondary-ip-ranges: |-
                    {
                      "ip_cidr_range": "10.0.0.0/16",
                      "name": "test-dbx-${random_string.suffix.result}",
                      "network": "${google_compute_network.dbx_private_vpc.id}",
                      "private_ip_google_access": true,
                      "region": "us-central1",
                      "secondary_ip_range": [
                        {
                          "ip_cidr_range": "10.1.0.0/16",
                          "range_name": "pods"
                        },
                        {
                          "ip_cidr_range": "10.2.0.0/20",
                          "range_name": "svc"
                        }
                      ]
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "gcp_network_info": [
                    {
                      "network_project_id": "${var.google_project}",
                      "pod_ip_range_name": "pods",
                      "service_ip_range_name": "svc",
                      "subnet_id": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.name}",
                      "subnet_region": "${google_compute_subnetwork.network_with_private_secondary_ip_ranges.region}",
                      "vpc_id": "${google_compute_network.dbx_private_vpc.name}"
                    }
                  ],
                  "network_name": "test-demo-${random_string.suffix.result}",
                  "vpc_endpoints": [
                    {
                      "dataplane_relay": [
                        "${databricks_mws_vpc_endpoint.relay.vpc_endpoint_id}"
                      ],
                      "rest_api": [
                        "${databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id}"
                      ]
                    }
                  ]
                }
              references:
                account_id: var.databricks_account_id
                gcp_network_info.network_project_id: var.google_project
                gcp_network_info.subnet_id: google_compute_subnetwork.network_with_private_secondary_ip_ranges.name
                gcp_network_info.subnet_region: google_compute_subnetwork.network_with_private_secondary_ip_ranges.region
                gcp_network_info.vpc_id: google_compute_network.dbx_private_vpc.name
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            gcp_network_info: '- (GCP only) a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:'
            id: '- Canonical unique identifier for the mws networks.'
            network_id: '- (String) id of network to be used for databricks_mws_workspaces resource.'
            network_name: '- name under which this network is registered'
            network_project_id: '- The Google Cloud project ID of the VPC network.'
            pod_ip_range_name: '- The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.'
            security_group_ids: '- (AWS only) ids of aws_security_group'
            service_ip_range_name: '- The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.'
            subnet_id: '- The ID of the subnet associated with this network.'
            subnet_ids: '- (AWS only) ids of aws_subnet'
            subnet_region: '- The Google Cloud region of the workspace data plane. For example, us-east4.'
            vpc_endpoints: '- (Optional) mapping of databricks_mws_vpc_endpoint for PrivateLink or Private Service Connect connections'
            vpc_id: '- (AWS only) aws_vpc id'
            vpc_status: '- (String) VPC attachment status'
            workspace_id: '- (Integer) id of associated workspace'
        importStatements: []
    databricks_mws_permission_assignment:
        subCategory: Security
        name: databricks_mws_permission_assignment
        title: ""
        examples:
            - name: add_admin_group
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${databricks_group.data_eng.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_group.data_eng.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_group.data_eng: |-
                    {
                      "display_name": "Data Engineering"
                    }
            - name: add_user
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${databricks_user.me.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_user.me.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_user.me: |-
                    {
                      "user_name": "me@example.com"
                    }
            - name: add_admin_spn
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${databricks_service_principal.sp.id}",
                  "workspace_id": "${databricks_mws_workspaces.this.workspace_id}"
                }
              references:
                principal_id: databricks_service_principal.sp.id
                workspace_id: databricks_mws_workspaces.this.workspace_id
              dependencies:
                databricks_service_principal.sp: |-
                    {
                      "display_name": "Automation-only SP"
                    }
        argumentDocs:
            '"ADMIN"': '- Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.'
            '"USER"': '- Can access the workspace with basic privileges.'
            id: '- ID of the permission assignment in form of workspace_id|principal_id.'
            permissions: '- The list of workspace permissions to assign to the principal:'
            principal_id: '- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources.'
            workspace_id: '- Databricks workspace ID.'
        importStatements: []
    databricks_mws_private_access_settings:
        subCategory: Deployment
        name: databricks_mws_private_access_settings
        title: ""
        examples:
            - name: pas
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "private_access_settings_name": "Private Access Settings for ${local.prefix}",
                  "provider": "${databricks.mws}",
                  "public_access_enabled": true,
                  "region": "${var.region}"
                }
              references:
                account_id: var.databricks_account_id
                provider: databricks.mws
                region: var.region
        argumentDocs:
            account_id: '- Account Id that could be found in the Accounts Console for AWS or GCP'
            allowed_vpc_endpoint_ids: '- (Optional) An array of databricks_mws_vpc_endpoint vpc_endpoint_id (not id). Only used when private_access_level is set to ENDPOINT. This is an allow list of databricks_mws_vpc_endpoint that in your account that can connect to your databricks_mws_workspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting public_access_enabled to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.'
            id: '- the ID of the Private Access Settings in form of account_id/private_access_settings_id.'
            private_access_level: '- (Optional) The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. ACCOUNT level access (default) lets only databricks_mws_vpc_endpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. ENDPOINT level access lets only specified databricks_mws_vpc_endpoint connect to your workspace. Please see the allowed_vpc_endpoint_ids documentation for more details.'
            private_access_settings_id: '- Canonical unique identifier of Private Access Settings in Databricks Account'
            private_access_settings_name: '- Name of Private Access Settings in Databricks Account'
            public_access_enabled: (Boolean, Optional, false by default on AWS, true by default on GCP) - If true, the databricks_mws_workspaces can be accessed over the databricks_mws_vpc_endpoint as well as over the public network. In such a case, you could also configure an databricks_ip_access_list for the workspace, to restrict the source networks that could be used to access it over the public network. If false, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.
            region: '- Region of AWS VPC or the Google Cloud VPC network'
            status: '- (AWS only) Status of Private Access Settings'
        importStatements: []
    databricks_mws_storage_configurations:
        subCategory: Deployment
        name: databricks_mws_storage_configurations
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "bucket_name": "${aws_s3_bucket.root_storage_bucket.bucket}",
                  "provider": "${databricks.mws}",
                  "storage_configuration_name": "${var.prefix}-storage"
                }
              references:
                account_id: var.databricks_account_id
                bucket_name: aws_s3_bucket.root_storage_bucket.bucket
                provider: databricks.mws
              dependencies:
                aws_s3_bucket.root_storage_bucket: |-
                    {
                      "acl": "private",
                      "bucket": "${var.prefix}-rootbucket"
                    }
                aws_s3_bucket_versioning.root_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console'
            bucket_name: '- name of AWS S3 bucket'
            id: '- Canonical unique identifier for the mws storage configurations.'
            storage_configuration_id: '- (String) id of storage config to be used for databricks_mws_workspace resource.'
            storage_configuration_name: '- name under which this storage configuration is stored'
        importStatements: []
    databricks_mws_workspaces:
        subCategory: Deployment
        name: databricks_mws_workspaces
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_region": "${var.region}",
                  "credentials_id": "${databricks_mws_credentials.this.credentials_id}",
                  "network_id": "${databricks_mws_networks.this.network_id}",
                  "provider": "${databricks.mws}",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.this.storage_configuration_id}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                aws_region: var.region
                credentials_id: databricks_mws_credentials.this.credentials_id
                network_id: databricks_mws_networks.this.network_id
                provider: databricks.mws
                storage_configuration_id: databricks_mws_storage_configurations.this.storage_configuration_id
                workspace_name: var.prefix
              dependencies:
                databricks_mws_credentials.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "${var.prefix}-creds",
                      "provider": "${databricks.mws}",
                      "role_arn": "${var.crossaccount_arn}"
                    }
                databricks_mws_networks.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "network_name": "${var.prefix}-network",
                      "provider": "${databricks.mws}",
                      "security_group_ids": [
                        "${var.security_group}"
                      ],
                      "subnet_ids": "${var.subnets_private}",
                      "vpc_id": "${var.vpc_id}"
                    }
                databricks_mws_storage_configurations.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${var.root_bucket}",
                      "provider": "${databricks.mws}",
                      "storage_configuration_name": "${var.prefix}-storage"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "aws_region": "us-east-1",
                  "credentials_id": "${databricks_mws_credentials.this.credentials_id}",
                  "custom_tags": {
                    "SoldToCode": "1234"
                  },
                  "provider": "${databricks.mws}",
                  "storage_configuration_id": "${databricks_mws_storage_configurations.this.storage_configuration_id}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${local.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                credentials_id: databricks_mws_credentials.this.credentials_id
                provider: databricks.mws
                storage_configuration_id: databricks_mws_storage_configurations.this.storage_configuration_id
                workspace_name: local.prefix
              dependencies:
                aws_iam_role.cross_account_role: |-
                    {
                      "assume_role_policy": "${data.databricks_aws_assume_role_policy.this.json}",
                      "name": "${local.prefix}-crossaccount",
                      "tags": "${var.tags}"
                    }
                aws_iam_role_policy.this: |-
                    {
                      "name": "${local.prefix}-policy",
                      "policy": "${data.databricks_aws_crossaccount_policy.this.json}",
                      "role": "${aws_iam_role.cross_account_role.id}"
                    }
                aws_s3_bucket.root_storage_bucket: |-
                    {
                      "acl": "private",
                      "bucket": "${local.prefix}-rootbucket",
                      "force_destroy": true,
                      "tags": "${var.tags}"
                    }
                aws_s3_bucket_policy.root_bucket_policy: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "depends_on": [
                        "${aws_s3_bucket_public_access_block.root_storage_bucket}"
                      ],
                      "policy": "${data.databricks_aws_bucket_policy.this.json}"
                    }
                aws_s3_bucket_public_access_block.root_storage_bucket: |-
                    {
                      "block_public_acls": true,
                      "block_public_policy": true,
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "depends_on": [
                        "${aws_s3_bucket.root_storage_bucket}"
                      ],
                      "ignore_public_acls": true,
                      "restrict_public_buckets": true
                    }
                aws_s3_bucket_server_side_encryption_configuration.root_storage_bucket: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.bucket}",
                      "rule": [
                        {
                          "apply_server_side_encryption_by_default": [
                            {
                              "sse_algorithm": "AES256"
                            }
                          ]
                        }
                      ]
                    }
                aws_s3_bucket_versioning.root_versioning: |-
                    {
                      "bucket": "${aws_s3_bucket.root_storage_bucket.id}",
                      "versioning_configuration": [
                        {
                          "status": "Disabled"
                        }
                      ]
                    }
                databricks_mws_credentials.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "credentials_name": "${local.prefix}-creds",
                      "provider": "${databricks.mws}",
                      "role_arn": "${aws_iam_role.cross_account_role.arn}"
                    }
                databricks_mws_storage_configurations.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "bucket_name": "${aws_s3_bucket.root_storage_bucket.bucket}",
                      "provider": "${databricks.mws}",
                      "storage_configuration_name": "${local.prefix}-storage"
                    }
                random_string.naming: |-
                    {
                      "length": 6,
                      "special": false,
                      "upper": false
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "cloud_resource_container": [
                    {
                      "gcp": [
                        {
                          "project_id": "${var.google_project}"
                        }
                      ]
                    }
                  ],
                  "gke_config": [
                    {
                      "connectivity_type": "PRIVATE_NODE_PUBLIC_MASTER",
                      "master_ip_range": "10.3.0.0/28"
                    }
                  ],
                  "location": "${var.subnet_region}",
                  "network_id": "${databricks_mws_networks.this.network_id}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                cloud_resource_container.gcp.project_id: var.google_project
                location: var.subnet_region
                network_id: databricks_mws_networks.this.network_id
                workspace_name: var.prefix
              dependencies:
                databricks_mws_networks.this: |-
                    {
                      "account_id": "${var.databricks_account_id}",
                      "gcp_network_info": [
                        {
                          "network_project_id": "${var.google_project}",
                          "pod_ip_range_name": "pods",
                          "service_ip_range_name": "svc",
                          "subnet_id": "${var.subnet_id}",
                          "subnet_region": "${var.subnet_region}",
                          "vpc_id": "${var.vpc_id}"
                        }
                      ],
                      "network_name": "${var.prefix}-network"
                    }
            - name: this
              manifest: |-
                {
                  "account_id": "${var.databricks_account_id}",
                  "cloud_resource_container": [
                    {
                      "gcp": [
                        {
                          "project_id": "${data.google_client_config.current.project}"
                        }
                      ]
                    }
                  ],
                  "gke_config": [
                    {
                      "connectivity_type": "PRIVATE_NODE_PUBLIC_MASTER",
                      "master_ip_range": "10.3.0.0/28"
                    }
                  ],
                  "location": "${data.google_client_config.current.region}",
                  "provider": "${databricks.accounts}",
                  "token": [
                    {}
                  ],
                  "workspace_name": "${var.prefix}"
                }
              references:
                account_id: var.databricks_account_id
                cloud_resource_container.gcp.project_id: data.google_client_config.current.project
                location: data.google_client_config.current.region
                provider: databricks.accounts
                workspace_name: var.prefix
        argumentDocs:
            account_id: '- Account Id that could be found in the top right corner of Accounts Console.'
            aws_region: '- (AWS only) region of VPC.'
            cloud_resource_container: '- (GCP only) A block that specifies GCP workspace configurations, consisting of following blocks:'
            connectivity_type: ': Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: PRIVATE_NODE_PUBLIC_MASTER, PUBLIC_NODE_PUBLIC_MASTER.'
            creation_time: '- (Integer) time when workspace was created'
            custom_tags: '- (Optional / AWS only) - The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any default_tags or custom_tags on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.'
            deployment_name: '- (Optional) part of URL as in https://<prefix>-<deployment-name>.cloud.databricks.com. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.'
            gcp: '- A block that consists of the following field:'
            gke_config: '- (GCP only) A block that specifies GKE configuration for the Databricks workspace:'
            id: '- (String) Canonical unique identifier for the workspace, of the format <account-id>/<workspace-id>'
            location: '- (GCP only) region of the subnet.'
            managed_services_customer_managed_key_id: '- (Optional) customer_managed_key_id from customer managed keys with use_cases set to MANAGED_SERVICES. This is used to encrypt the workspace''s notebook and secret data in the control plane.'
            master_ip_range: ': The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as /28.'
            network_id: '- (Optional) network_id from networks.'
            private_access_settings_id: '- (Optional) Canonical unique identifier of databricks_mws_private_access_settings in Databricks Account.'
            project_id: '- The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.'
            storage_configuration_id: '- (AWS only)storage_configuration_id from storage configuration.'
            storage_customer_managed_key_id: '- (Optional) customer_managed_key_id from customer managed keys with use_cases set to STORAGE. This is used to encrypt the DBFS Storage & Cluster Volumes.'
            token {}.comment: '- (Optional) Comment, that will appear in "User Settings / Access Tokens" page on Workspace UI. By default it''s "Terraform PAT".'
            token {}.lifetime_seconds: '- (Optional) Token expiry lifetime. By default its 2592000 (30 days).'
            workspace_id: '- (String) workspace id'
            workspace_name: '- name of the workspace, will appear on UI.'
            workspace_status: '- (String) workspace status'
            workspace_status_message: '- (String) updates on workspace status'
            workspace_url: '- (String) URL of the workspace'
        importStatements: []
    databricks_notebook:
        subCategory: Workspace
        name: databricks_notebook
        title: ""
        examples:
            - name: ddl
              manifest: |-
                {
                  "path": "${data.databricks_current_user.me.home}/AA/BB/CC",
                  "source": "${path.module}/DDLgen.py"
                }
            - name: notebook
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )}",
                  "language": "PYTHON",
                  "path": "/Shared/Demo"
                }
            - name: lesson
              manifest: |-
                {
                  "path": "/Shared/Intro",
                  "source": "${path.module}/IntroNotebooks.dbc"
                }
        argumentDocs:
            content_base64: '- The base64-encoded notebook source code. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline.'
            id: '-  Path of notebook on workspace'
            language: '-  (required with content_base64) One of SCALA, PYTHON, SQL, R.'
            object_id: '-  Unique identifier for a NOTEBOOK'
            path: '-  (Required) The absolute path of the notebook or directory, beginning with "/", e.g. "/Demo".'
            source: '- Path to notebook in source code format on local filesystem. Conflicts with content_base64.'
            url: '- Routable URL of the notebook'
        importStatements: []
    databricks_obo_token:
        subCategory: Security
        name: databricks_obo_token
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "application_id": "${databricks_service_principal.this.application_id}",
                  "comment": "PAT on behalf of ${databricks_service_principal.this.display_name}",
                  "depends_on": [
                    "${databricks_permissions.token_usage}"
                  ],
                  "lifetime_seconds": 3600
                }
              references:
                application_id: databricks_service_principal.this.application_id
              dependencies:
                databricks_permissions.token_usage: |-
                    {
                      "access_control": [
                        {
                          "permission_level": "CAN_USE",
                          "service_principal_name": "${databricks_service_principal.this.application_id}"
                        }
                      ],
                      "authorization": "tokens"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "Automation-only SP"
                    }
            - name: this
              manifest: |-
                {
                  "application_id": "${databricks_service_principal.this.application_id}",
                  "comment": "PAT on behalf of ${databricks_service_principal.this.display_name}",
                  "depends_on": [
                    "${databricks_group_member.this}"
                  ],
                  "lifetime_seconds": 3600
                }
              references:
                application_id: databricks_service_principal.this.application_id
              dependencies:
                databricks_group_member.this: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_service_principal.this.id}"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "Terraform"
                    }
        argumentDocs:
            application_id: '- Application ID of databricks_service_principal to create a PAT token for.'
            comment: '- (String, Optional) Comment that describes the purpose of the token.'
            id: '- Canonical unique identifier for the token.'
            lifetime_seconds: '- (Integer, Optional) The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.'
            token_value: '- Sensitive value of the newly-created token.'
        importStatements: []
    databricks_permission_assignment:
        subCategory: Security
        name: databricks_permission_assignment
        title: ""
        examples:
            - name: add_user
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${data.databricks_user.me.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_user.me.id
                provider: databricks.workspace
            - name: add_admin_spn
              manifest: |-
                {
                  "permissions": [
                    "ADMIN"
                  ],
                  "principal_id": "${data.databricks_service_principal.sp.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_service_principal.sp.id
                provider: databricks.workspace
            - name: this
              manifest: |-
                {
                  "permissions": [
                    "USER"
                  ],
                  "principal_id": "${data.databricks_group.account_level.id}",
                  "provider": "${databricks.workspace}"
                }
              references:
                principal_id: data.databricks_group.account_level.id
                provider: databricks.workspace
        argumentDocs:
            '"ADMIN"': '- Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.'
            '"USER"': '- Can access the workspace with basic privileges.'
            id: '- ID of the permission assignment - same as principal_id.'
            permissions: '- The list of workspace permissions to assign to the principal:'
            principal_id: '- Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks_service_principal or databricks_group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of principal_id as outputs from another Terraform stack.'
        importStatements: []
    databricks_permissions:
        subCategory: Security
        name: databricks_permissions
        title: ""
        examples:
            - name: cluster_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_ATTACH_TO"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_RESTART"
                    },
                    {
                      "group_name": "${databricks_group.ds.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "cluster_id": "${databricks_cluster.shared_autoscaling.id}"
                }
              references:
                access_control.group_name: databricks_group.ds.display_name
                cluster_id: databricks_cluster.shared_autoscaling.id
              dependencies:
                databricks_cluster.shared_autoscaling: |-
                    {
                      "autoscale": [
                        {
                          "max_workers": 10,
                          "min_workers": 1
                        }
                      ],
                      "autotermination_minutes": 60,
                      "cluster_name": "Shared Autoscaling",
                      "node_type_id": "${data.databricks_node_type.smallest.id}",
                      "spark_version": "${data.databricks_spark_version.latest.id}"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: policy_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.ds.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "cluster_policy_id": "${databricks_cluster_policy.something_simple.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                cluster_policy_id: databricks_cluster_policy.something_simple.id
              dependencies:
                databricks_cluster_policy.something_simple: |-
                    {
                      "definition": "${jsonencode({\n    \"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\" : {\n      \"type\" : \"forbidden\"\n    },\n    \"spark_conf.spark.secondkey\" : {\n      \"type\" : \"forbidden\"\n    }\n  })}",
                      "name": "Some simple policy"
                    }
                databricks_group.ds: |-
                    {
                      "display_name": "Data Science"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: pool_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_ATTACH_TO"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "instance_pool_id": "${databricks_instance_pool.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                instance_pool_id: databricks_instance_pool.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_instance_pool.this: |-
                    {
                      "idle_instance_autotermination_minutes": 60,
                      "instance_pool_name": "Reserved Instances",
                      "max_capacity": 10,
                      "min_idle_instances": 0,
                      "node_type_id": "${data.databricks_node_type.smallest.id}"
                    }
            - name: job_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "permission_level": "IS_OWNER",
                      "service_principal_name": "${databricks_service_principal.aws_principal.application_id}"
                    }
                  ],
                  "job_id": "${databricks_job.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                access_control.service_principal_name: databricks_service_principal.aws_principal.application_id
                job_id: databricks_job.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_job.this: |-
                    {
                      "max_concurrent_runs": 1,
                      "name": "Featurization",
                      "task": [
                        {
                          "new_cluster": [
                            {
                              "node_type_id": "${data.databricks_node_type.smallest.id}",
                              "num_workers": 300,
                              "spark_version": "${data.databricks_spark_version.latest.id}"
                            }
                          ],
                          "notebook_task": [
                            {
                              "notebook_path": "/Production/MakeFeatures"
                            }
                          ],
                          "task_key": "task1"
                        }
                      ]
                    }
                databricks_service_principal.aws_principal: |-
                    {
                      "display_name": "main"
                    }
            - name: dlt_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "pipeline_id": "${databricks_pipeline.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                pipeline_id: databricks_pipeline.this.id
              dependencies:
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.dlt_demo: |-
                    {
                      "content_base64": "${base64encode(\u003c\u003c-EOT\n    import dlt\n    json_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n    @dlt.table(\n       comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n    )\n    def clickstream_raw():\n        return (spark.read.format(\"json\").load(json_path))\n    EOT\n  )}",
                      "language": "PYTHON",
                      "path": "${data.databricks_current_user.me.home}/DLT_Demo"
                    }
                databricks_pipeline.this: |-
                    {
                      "configuration": {
                        "key1": "value1",
                        "key2": "value2"
                      },
                      "continuous": false,
                      "filters": [
                        {
                          "exclude": [
                            "com.databricks.exclude"
                          ],
                          "include": [
                            "com.databricks.include"
                          ]
                        }
                      ],
                      "library": [
                        {
                          "notebook": [
                            {
                              "path": "${databricks_notebook.dlt_demo.id}"
                            }
                          ]
                        }
                      ],
                      "name": "DLT Demo Pipeline (${data.databricks_current_user.me.alphanumeric})",
                      "storage": "/test/tf-pipeline"
                    }
            - name: notebook_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "notebook_path": "${databricks_notebook.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                notebook_path: databricks_notebook.this.path
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_notebook.this: |-
                    {
                      "content_base64": "${base64encode(\"# Welcome to your Python notebook\")}",
                      "language": "PYTHON",
                      "path": "/Production/ETL/Features"
                    }
            - name: workspace_file_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "workspace_file_path": "${databricks_workspace_file.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                workspace_file_path: databricks_workspace_file.this.path
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_workspace_file.this: |-
                    {
                      "content_base64": "${base64encode(\"print('Hello World')\")}",
                      "path": "/Production/ETL/Features.py"
                    }
            - name: folder_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "depends_on": [
                    "${databricks_directory.this}"
                  ],
                  "directory_path": "${databricks_directory.this.path}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                directory_path: databricks_directory.this.path
              dependencies:
                databricks_directory.this: |-
                    {
                      "path": "/Production/ETL"
                    }
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: repo_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "repo_id": "${databricks_repo.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                repo_id: databricks_repo.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_repo.this: |-
                    {
                      "url": "https://github.com/user/demo.git"
                    }
            - name: experiment_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_EDIT"
                    }
                  ],
                  "experiment_id": "${databricks_mlflow_experiment.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                experiment_id: databricks_mlflow_experiment.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_mlflow_experiment.this: |-
                    {
                      "artifact_location": "dbfs:/tmp/my-experiment",
                      "description": "My MLflow experiment description",
                      "name": "${data.databricks_current_user.me.home}/Sample"
                    }
            - name: model_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE_PRODUCTION_VERSIONS"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE_STAGING_VERSIONS"
                    }
                  ],
                  "registered_model_id": "${databricks_mlflow_model.this.registered_model_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                registered_model_id: databricks_mlflow_model.this.registered_model_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_mlflow_model.this: |-
                    {
                      "name": "SomePredictions"
                    }
            - name: ml_serving_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_VIEW"
                    },
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_MANAGE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_QUERY"
                    }
                  ],
                  "serving_endpoint_id": "${databricks_model_serving.this.serving_endpoint_id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                serving_endpoint_id: databricks_model_serving.this.serving_endpoint_id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_model_serving.this: |-
                    {
                      "config": [
                        {
                          "served_models": [
                            {
                              "model_name": "test",
                              "model_version": "1",
                              "name": "prod_model",
                              "scale_to_zero_enabled": true,
                              "workload_size": "Small"
                            }
                          ]
                        }
                      ],
                      "name": "tf-test"
                    }
            - name: password_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.guests.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "authorization": "passwords"
                }
              references:
                access_control.group_name: databricks_group.guests.display_name
              dependencies:
                databricks_group.guests: |-
                    {
                      "display_name": "Guest Users"
                    }
            - name: token_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_USE"
                    }
                  ],
                  "authorization": "tokens"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_USE"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_endpoint_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
                sql_endpoint_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "Small",
                      "max_num_clusters": 1,
                      "name": "Endpoint of ${data.databricks_current_user.me.alphanumeric}",
                      "tags": [
                        {
                          "custom_tags": [
                            {
                              "key": "City",
                              "value": "Amsterdam"
                            }
                          ]
                        }
                      ]
                    }
            - name: endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_dashboard_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_query_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: endpoint_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "${databricks_group.auto.display_name}",
                      "permission_level": "CAN_RUN"
                    },
                    {
                      "group_name": "${databricks_group.eng.display_name}",
                      "permission_level": "CAN_MANAGE"
                    }
                  ],
                  "sql_alert_id": "3244325"
                }
              references:
                access_control.group_name: databricks_group.eng.display_name
              dependencies:
                databricks_group.auto: |-
                    {
                      "display_name": "Automation"
                    }
                databricks_group.eng: |-
                    {
                      "display_name": "Engineering"
                    }
            - name: model_usage
              manifest: |-
                {
                  "access_control": [
                    {
                      "group_name": "users",
                      "permission_level": "CAN_READ"
                    }
                  ],
                  "registered_model_id": "${databricks_mlflow_model.model.registered_model_id}"
                }
              references:
                registered_model_id: databricks_mlflow_model.model.registered_model_id
              dependencies:
                databricks_mlflow_model.model: |-
                    {
                      "description": "MLflow registered model",
                      "name": "example_model"
                    }
        argumentDocs:
            CAN_MANAGE: permission for items in the Workspace > Shared Icon Shared folder. You can grant CAN_MANAGE permission to notebooks and folders by moving them to the Shared Icon Shared folder.
            IS_OWNER: permission. Destroying databricks_permissions resource for a job would revert ownership to the creator.
            admins.group_name: '- (Optional) name of the group. We recommend setting permissions on groups.'
            admins.permission_level: '- (Required) permission level according to specific resource. See examples above for the reference.'
            admins.service_principal_name: '- (Optional) Application ID of the service_principal.'
            admins.user_name: '- (Optional) name of the user.'
            authorization: '- either tokens or passwords.'
            cluster_id: '- cluster id'
            cluster_policy_id: '- cluster policy id'
            directory_id: '- directory id'
            directory_path: '- path of directory'
            experiment_id: '- MLflow experiment id'
            id: '- Canonical unique identifier for the permissions in form of /object_type/object_id.'
            instance_pool_id: '- instance pool id'
            job_id: '- job id'
            notebook_id: '- ID of notebook within workspace'
            notebook_path: '- path of notebook'
            object_type: '- type of permissions.'
            pipeline_id: '- pipeline id'
            registered_model_id: '- MLflow registered model id'
            repo_id: '- repo id'
            repo_path: '- path of databricks repo directory(/Repos/<username>/...)'
            serving_endpoint_id: '- Model Serving endpoint id.'
            sql_alert_id: '- SQL alert id'
            sql_dashboard_id: '- SQL dashboard id'
            sql_endpoint_id: '- SQL warehouse id'
            sql_query_id: '- SQL query id'
        importStatements: []
    databricks_pipeline:
        subCategory: Compute
        name: databricks_pipeline
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "cluster": [
                    {
                      "custom_tags": {
                        "cluster_type": "default"
                      },
                      "label": "default",
                      "num_workers": 2
                    },
                    {
                      "custom_tags": {
                        "cluster_type": "maintenance"
                      },
                      "label": "maintenance",
                      "num_workers": 1
                    }
                  ],
                  "configuration": {
                    "key1": "value1",
                    "key2": "value2"
                  },
                  "continuous": false,
                  "library": [
                    {
                      "notebook": [
                        {
                          "path": "${databricks_notebook.dlt_demo.id}"
                        }
                      ]
                    },
                    {
                      "file": [
                        {
                          "path": "${databricks_repo.dlt_demo.path}/pipeline.sql"
                        }
                      ]
                    }
                  ],
                  "name": "Pipeline Name",
                  "notification": [
                    {
                      "alerts": [
                        "on-update-failure",
                        "on-update-fatal-failure",
                        "on-update-success",
                        "on-flow-failure"
                      ],
                      "email_recipients": [
                        "user@domain.com",
                        "user1@domain.com"
                      ]
                    }
                  ],
                  "storage": "/test/first-pipeline"
                }
              references:
                library.notebook.path: databricks_notebook.dlt_demo.id
              dependencies:
                databricks_notebook.dlt_demo: '{}'
                databricks_repo.dlt_demo: '{}'
        argumentDocs:
            alerts: (Required) non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list
            catalog: '- The name of catalog in Unity Catalog. Change of this parameter forces recreation of the pipeline. (Conflicts with storage).'
            channel: '- optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: CURRENT (default) and PREVIEW.'
            cluster: blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. Please note that DLT pipeline clusters are supporting only subset of attributes as described in   Also, note that autoscale block is extended with the mode parameter that controls the autoscaling algorithm (possible values are ENHANCED for new, enhanced autoscaling algorithm, or LEGACY for old algorithm).
            configuration: '- An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.'
            continuous: '- A flag indicating whether to run the pipeline continuously. The default value is false.'
            development: '- A flag indicating whether to run the pipeline in development mode. The default value is true.'
            edition: '- optional name of the product edition. Supported values are: CORE, PRO, ADVANCED (default).'
            email_recipients: (Required) non-empty list of emails to notify.
            id: '- Canonical unique identifier of the DLT pipeline.'
            library: blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special notebook & file library types that should have the path attribute. Right now only the
            name: '- A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.'
            notebook: '& file types are supported.'
            on-flow-failure: '- a single data flow fails.'
            on-update-failure: '- a pipeline update fails with a retryable error.'
            on-update-fatal-failure: '- a pipeline update fails with a non-retryable (fatal) error.'
            on-update-success: '- a pipeline update completes successfully.'
            photon: '- A flag indicating whether to use Photon engine. The default value is false.'
            storage: '- A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. Change of this parameter forces recreation of the pipeline. (Conflicts with catalog).'
            target: '- The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.'
            url: '- URL of the DLT pipeline on the given workspace.'
        importStatements: []
    databricks_provider:
        subCategory: Unity Catalog
        name: databricks_provider
        title: ""
        examples:
            - name: dbprovider
              manifest: |-
                {
                  "authentication_type": "TOKEN",
                  "comment": "made by terraform 2",
                  "name": "terraform-test-provider",
                  "recipient_profile_str": "${jsonencode(\n    {\n      \"shareCredentialsVersion\" : 1,\n      \"bearerToken\" : \"token\",\n      \"endpoint\" : \"endpoint\",\n      \"expirationTime\" : \"expiration-time\"\n    }\n  )}"
                }
        argumentDocs:
            authentication_type: '- (Optional) The delta sharing authentication type. Valid values are TOKEN.'
            comment: '- (Optional) Description about the provider.'
            id: '- ID of this provider - same as the name.'
            name: '- Name of provider. Change forces creation of a new resource.'
            recipient_profile_str: '- (Optional) This is the json file that is created from a recipient url.'
        importStatements: []
    databricks_recipient:
        subCategory: Unity Catalog
        name: databricks_recipient
        title: ""
        examples:
            - name: db2open
              manifest: |-
                {
                  "authentication_type": "TOKEN",
                  "comment": "made by terraform",
                  "ip_access_list": [
                    {
                      "allowed_ip_addresses": []
                    }
                  ],
                  "name": "${data.databricks_current_user.current.alphanumeric}-recipient",
                  "sharing_code": "${random_password.db2opensharecode.result}"
                }
              references:
                sharing_code: random_password.db2opensharecode.result
              dependencies:
                random_password.db2opensharecode: |-
                    {
                      "length": 16,
                      "special": true
                    }
            - name: db2db
              manifest: |-
                {
                  "authentication_type": "DATABRICKS",
                  "comment": "made by terraform",
                  "data_recipient_global_metastore_id": "${databricks_metastore.recipient_metastore.global_metastore_id}",
                  "name": "${data.databricks_current_user.current.alphanumeric}-recipient"
                }
              references:
                data_recipient_global_metastore_id: databricks_metastore.recipient_metastore.global_metastore_id
              dependencies:
                databricks_metastore.recipient_metastore: |-
                    {
                      "delta_sharing_recipient_token_lifetime_in_seconds": "60000000",
                      "delta_sharing_scope": "INTERNAL",
                      "force_destroy": true,
                      "name": "recipient",
                      "storage_root": "${format(\"abfss://%s@%s.dfs.core.windows.net/\",\n    azurerm_storage_container.unity_catalog.name,\n  azurerm_storage_account.unity_catalog.name)}"
                    }
        argumentDocs:
            activation_url: '- Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.'
            authentication_type: '- (Optional) The delta sharing authentication type. Valid values are TOKEN and DATABRICKS.'
            comment: '- (Optional) Description about the recipient.'
            created_at: '- Time at which this recipient Token was created, in epoch milliseconds.'
            created_by: '- Username of recipient token creator.'
            data_recipient_global_metastore_id: '- Required when authentication_type is DATABRICKS.'
            expiration_time: '- Expiration timestamp of the token in epoch milliseconds.'
            id: '- the ID of the recipient - the same as the name.'
            ip_access_list: '- (Optional) Recipient IP access list.'
            ip_access_list.allowed_ip_addresses: '- Allowed IP Addresses in CIDR notation. Limit of 100.'
            name: '- Name of recipient. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the recipient owner.'
            sharing_code: '- (Optional) The one-time sharing code provided by the data recipient.'
            tokens: '- List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:'
            updated_at: '- Time at which this recipient Token was updated, in epoch milliseconds.'
            updated_by: '- Username of recipient Token updater.'
        importStatements: []
    databricks_registered_model:
        subCategory: Unity Catalog
        name: databricks_registered_model
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "catalog_name": "main",
                  "name": "my_model",
                  "schema_name": "default"
                }
        argumentDocs:
            ALL_PRIVILEGES: ', APPLY_TAG, and EXECUTE privileges.'
            catalog_name: '- (Required) The name of the catalog where the schema and the registered model reside. Change of this parameter forces recreation of the resource.'
            comment: '- The comment attached to the registered model.'
            id: '- Equal to the full name of the model (catalog_name.schema_name.name) and used to identify the model uniquely across the metastore.'
            name: '- (Required) The name of the registered model.  Change of this parameter forces recreation of the resource.'
            schema_name: '- (Required) The name of the schema where the registered model resides. Change of this parameter forces recreation of the resource.'
            storage_location: '- The storage location under which model version data files are stored. Change of this parameter forces recreation of the resource.'
        importStatements: []
    databricks_repo:
        subCategory: Workspace
        name: databricks_repo
        title: ""
        examples:
            - name: nutter_in_home
              manifest: |-
                {
                  "url": "https://github.com/user/demo.git"
                }
        argumentDocs:
            branch: '- (Optional) name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with tag.  If branch is removed, and tag isn''t specified, then the repository will stay at the previously checked out state.'
            commit_hash: '- Hash of the HEAD commit at time of the last executed operation. It won''t change if you manually perform pull operation via UI or API'
            git_provider: '- (Optional, if it''s possible to detect Git provider by host name) case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult Repos API documentation): gitHub, gitHubEnterprise, bitbucketCloud, bitbucketServer, azureDevOpsServices, gitLab, gitLabEnterpriseEdition, awsCodeCommit.'
            id: '-  Repo identifier'
            path: '- (Optional) path to put the checked out Repo. If not specified, then repo will be created in the user''s repo directory (/Repos/<username>/...).  If the value changes, repo is re-created.'
            sparse_checkout.patterns: '- array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place.'
            tag: '- (Optional) name of the tag for initial checkout.  Conflicts with branch.'
            url: '-  (Required) The URL of the Git Repository to clone from. If the value changes, repo is re-created.'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    databricks_restrict_workspace_admins_setting:
        subCategory: Settings
        name: databricks_restrict_workspace_admins_setting
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "restrict_workspace_admins": [
                    {
                      "status": "RESTRICT_TOKENS_AND_JOB_RUN_AS"
                    }
                  ]
                }
        argumentDocs:
            restrict_workspace_admins: '- (Required) The configuration details.'
            status: '- (Required) The restrict workspace admins status for the workspace.'
        importStatements: []
    databricks_schema:
        subCategory: Unity Catalog
        name: databricks_schema
        title: ""
        examples:
            - name: things
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.id}",
                  "comment": "this database is managed by terraform",
                  "name": "things",
                  "properties": {
                    "kind": "various"
                  }
                }
              references:
                catalog_name: databricks_catalog.sandbox.id
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
        argumentDocs:
            catalog_name: '- Name of parent catalog. Change forces creation of a new resource.'
            comment: '- (Optional) User-supplied free-form text.'
            force_destroy: '- (Optional) Delete schema regardless of its contents.'
            id: '- ID of this schema in form of <catalog_name>.<name>.'
            name: '- Name of Schema relative to parent catalog. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the schema owner.'
            properties: '- (Optional) Extensible Schema properties.'
            storage_root: '- (Optional) Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.'
        importStatements: []
    databricks_secret_acl:
        subCategory: Security
        name: databricks_secret_acl
        title: ""
        examples:
            - name: my_secret_acl
              manifest: |-
                {
                  "permission": "READ",
                  "principal": "${databricks_group.ds.display_name}",
                  "scope": "${databricks_secret_scope.app.name}"
                }
              references:
                principal: databricks_group.ds.display_name
                scope: databricks_secret_scope.app.name
              dependencies:
                databricks_group.ds: |-
                    {
                      "display_name": "data-scientists"
                    }
                databricks_secret.publishing_api: |-
                    {
                      "key": "publishing_api",
                      "scope": "${databricks_secret_scope.app.name}",
                      "string_value": "${data.azurerm_key_vault_secret.example.value}"
                    }
                databricks_secret_scope.app: |-
                    {
                      "name": "app-secret-scope"
                    }
        argumentDocs:
            application_id: attribute of databricks_service_principal.
            display_name: attribute of databricks_group.  Use users to allow access for all workspace users.
            permission: '- (Required) READ, WRITE or MANAGE.'
            principal: '- (Required) principal''s identifier. It can be:'
            scope: '- (Required) name of the scope'
            user_name: attribute of databricks_user.
        importStatements: []
    databricks_secret_scope:
        subCategory: Security
        name: databricks_secret_scope
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "name": "terraform-demo-scope"
                }
            - name: kv
              manifest: |-
                {
                  "keyvault_metadata": [
                    {
                      "dns_name": "${azurerm_key_vault.this.vault_uri}",
                      "resource_id": "${azurerm_key_vault.this.id}"
                    }
                  ],
                  "name": "keyvault-managed"
                }
              references:
                keyvault_metadata.dns_name: azurerm_key_vault.this.vault_uri
                keyvault_metadata.resource_id: azurerm_key_vault.this.id
              dependencies:
                azurerm_key_vault.this: |-
                    {
                      "location": "${azurerm_resource_group.example.location}",
                      "name": "${var.prefix}-kv",
                      "purge_protection_enabled": false,
                      "resource_group_name": "${azurerm_resource_group.example.name}",
                      "sku_name": "standard",
                      "soft_delete_enabled": false,
                      "tags": "${var.tags}",
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
                azurerm_key_vault_access_policy.this: |-
                    {
                      "key_vault_id": "${azurerm_key_vault.this.id}",
                      "object_id": "${data.azurerm_client_config.current.object_id}",
                      "secret_permissions": [
                        "Delete",
                        "Get",
                        "List",
                        "Set"
                      ],
                      "tenant_id": "${data.azurerm_client_config.current.tenant_id}"
                    }
        argumentDocs:
            backend_type: '- Either DATABRICKS or AZURE_KEYVAULT'
            id: '- The id for the secret scope object.'
            initial_manage_principal: '- (Optional) The principal with the only possible value users that is initially granted MANAGE permission to the created scope.  If it''s omitted, then the databricks_secret_acl with MANAGE permission applied to the scope is assigned to the API request issuer''s user identity (see documentation). This part of the state cannot be imported.'
            name: '- (Required) Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.'
        importStatements: []
    databricks_service_principal:
        subCategory: Security
        name: databricks_service_principal
        title: ""
        examples:
            - name: sp
              manifest: |-
                {
                  "application_id": "00000000-0000-0000-0000-000000000000"
                }
            - name: sp
              manifest: |-
                {
                  "application_id": "00000000-0000-0000-0000-000000000000"
                }
              dependencies:
                databricks_group_member.i-am-admin: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_service_principal.sp.id}"
                    }
            - name: sp
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "application_id": "00000000-0000-0000-0000-000000000000",
                  "display_name": "Example service principal"
                }
            - name: sp
              manifest: |-
                {
                  "display_name": "Automation-only SP",
                  "provider": "${databricks.mws}"
                }
              references:
                provider: databricks.mws
            - name: sp
              manifest: |-
                {
                  "application_id": "00000000-0000-0000-0000-000000000000",
                  "provider": "${databricks.azure_account}"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. servicePrincipals/00000000-0000-0000-0000-000000000000.'
            active: '- (Optional) Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.'
            allow_cluster_create: '-  (Optional) Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            application_id: '- This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.'
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature through databricks_sql_endpoint.'
            disable_as_user_deletion: '- (Optional) When deleting a user, set the user''s active flag to false instead of actually deleting the user. This flag is exclusive to force_delete_repos and force_delete_home_dir flags. True by default for accounts SCIM API, false otherwise.'
            display_name: '- (Required) This is an alias for the service principal and can be the full name of the service principal.'
            external_id: '- (Optional) ID of the service principal in an external identity provider.'
            force: '- (Optional) Ignore cannot create service principal: Service principal with application ID X already exists errors and implicitly import the specified service principal into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            force_delete_home_dir: '- (Optional) This flag determines whether the service principal''s home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            force_delete_repos: '- (Optional) This flag determines whether the service principal''s repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            home: '- Home folder of the service principal, e.g. /Users/00000000-0000-0000-0000-000000000000.'
            id: '- Canonical unique identifier for the service principal.'
            repos: '- Personal Repos location of the service principal, e.g. /Repos/00000000-0000-0000-0000-000000000000.'
            workspace_access: '- (Optional) This is a field to allow the group to have access to Databricks Workspace.'
        importStatements: []
    databricks_service_principal_role:
        subCategory: Security
        name: databricks_service_principal_role
        title: ""
        examples:
            - name: my_service_principal_instance_profile
              manifest: |-
                {
                  "role": "${databricks_instance_profile.instance_profile.id}",
                  "service_principal_id": "${databricks_service_principal.this.id}"
                }
              references:
                role: databricks_instance_profile.instance_profile.id
                service_principal_id: databricks_service_principal.this.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_service_principal.this: |-
                    {
                      "display_name": "My Service Principal"
                    }
        argumentDocs:
            id: '- The id in the format <service_principal_id>|<role>.'
            role: '-  (Required) This is the id of the role or instance profile resource.'
            service_principal_id: '- (Required) This is the id of the service principal resource.'
        importStatements: []
    databricks_service_principal_secret:
        subCategory: Security
        name: databricks_service_principal_secret
        title: ""
        examples:
            - name: terraform_sp
              manifest: |-
                {
                  "service_principal_id": "${databricks_service_principal.this.id}"
                }
              references:
                service_principal_id: databricks_service_principal.this.id
        argumentDocs:
            id: '- ID of the secret'
            secret: '- Generated secret for the service principal'
            service_principal_id: '- ID of the databricks_service_principal (not application ID).'
        importStatements: []
    databricks_share:
        subCategory: Unity Catalog
        name: databricks_share
        title: ""
        examples:
            - name: some
              manifest: |-
                {
                  "dynamic": {
                    "object": [
                      {
                        "content": [
                          {
                            "data_object_type": "TABLE",
                            "name": "${object.value}"
                          }
                        ],
                        "for_each": "${data.databricks_tables.things.ids}"
                      }
                    ]
                  },
                  "name": "my_share"
                }
              references:
                dynamic.content.name: object.value
                dynamic.for_each: data.databricks_tables.things.ids
            - name: schema_share
              manifest: |-
                {
                  "name": "schema_share",
                  "object": [
                    {
                      "data_object_type": "SCHEMA",
                      "history_data_sharing_status": "ENABLED",
                      "name": "catalog_name.schema_name"
                    }
                  ]
                }
            - name: some
              manifest: |-
                {
                  "name": "my_share",
                  "object": [
                    {
                      "data_object_type": "TABLE",
                      "history_data_sharing_status": "ENABLED",
                      "name": "my_catalog.my_schema.my_table",
                      "partition": [
                        {
                          "value": [
                            {
                              "name": "year",
                              "op": "EQUAL",
                              "value": "2009"
                            },
                            {
                              "name": "month",
                              "op": "EQUAL",
                              "value": "12"
                            }
                          ]
                        },
                        {
                          "value": [
                            {
                              "name": "year",
                              "op": "EQUAL",
                              "value": "2010"
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            cdf_enabled: (Optional) - Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field history_data_sharing_status can not be set.
            comment: (Optional) -  Description about the object.
            created_at: '- Time when the share was created.'
            created_by: '- The principal that created the share.'
            data_object_type: (Required) - Type of the data object, currently TABLE, SCHEMA, VOLUME, NOTEBOOK_FILE are supported.
            history_data_sharing_status: '(Optional) - Whether to enable history sharing, one of: ENABLED, DISABLED. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. NOTE: The start_version should be less than or equal the current version of the object. When this field is set, field cdf_enabled can not be set.'
            id: '- the ID of the share, the same as name.'
            name: (Required) - Name of share. Change forces creation of a new resource.
            op: '- The operator to apply for the value, one of: EQUAL, LIKE'
            owner: (Optional) -  User name/group name/sp application_id of the share owner.
            recipient_property_key: (Optional) - The key of a Delta Sharing recipient's property. For example databricks-account-id. When this field is set, field value can not be set.
            shared_as: (Optional) - A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the shared_as name. The shared_as name must be unique within a Share. Change forces creation of a new resource.
            start_version: (Optional) -  The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.
            status: '- Status of the object, one of: ACTIVE, PERMISSION_DENIED.'
            value: (Optional) - The value of the partition column. When this value is not set, it means null value. When this field is set, field recipient_property_key can not be set.
        importStatements: []
    databricks_sql_alert:
        subCategory: Databricks SQL
        name: databricks_sql_alert
        title: ""
        examples:
            - name: alert
              manifest: |-
                {
                  "name": "My Alert",
                  "options": [
                    {
                      "column": "p1",
                      "muted": false,
                      "op": "==",
                      "value": "2"
                    }
                  ],
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "query_id": "${databricks_sql_query.this.id}",
                  "rearm": 1
                }
              references:
                query_id: databricks_sql_query.this.id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
                databricks_sql_query.this: |-
                    {
                      "data_source_id": "${databricks_sql_endpoint.example.data_source_id}",
                      "name": "My Query Name",
                      "parent": "folders/${databricks_directory.shared_dir.object_id}",
                      "query": "SELECT 1 AS p1, 2 as p2"
                    }
        argumentDocs:
            column: '- (Required, String) Name of column in the query result to compare in alert evaluation.'
            custom_body: '- (Optional, String) Custom body of alert notification, if it exists. See Alerts API reference for custom templating instructions.'
            custom_subject: '- (Optional, String) Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See Alerts API reference for custom templating instructions.'
            empty_result_state: '- (Optional, String) State that alert evaluates to when query result is empty.  Currently supported values are unknown, triggered, ok - check API documentation for full list of supported values.'
            id: '- unique ID of the SQL Alert.'
            muted: '- (Optional, bool) Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.'
            name: '- (Required, String) Name of the alert.'
            op: '- (Required, String Enum) Operator used to compare in alert evaluation. (Enum: >, >=, <, <=, ==, !=)'
            options: '- (Required) Alert configuration options.'
            parent: '- (Optional, String) The identifier of the workspace folder containing the alert. The default is ther user''s home folder. The folder identifier is formatted as folder/<folder_id>.'
            query_id: '- (Required, String) ID of the query evaluated by the alert.'
            rearm: '- (Optional, Integer) Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.'
            value: '- (Required, String) Value used to compare in alert evaluation.'
        importStatements: []
    databricks_sql_dashboard:
        subCategory: Databricks SQL
        name: databricks_sql_dashboard
        title: ""
        examples:
            - name: d1
              manifest: |-
                {
                  "name": "My Dashboard Name",
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "tags": [
                    "some-tag",
                    "another-tag"
                  ]
                }
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Dashboards"
                    }
        argumentDocs:
            id: '- the unique ID of the SQL Dashboard.'
        importStatements: []
    databricks_sql_endpoint:
        subCategory: Databricks SQL
        name: databricks_sql_endpoint
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "cluster_size": "Small",
                  "max_num_clusters": 1,
                  "name": "Endpoint of ${data.databricks_current_user.me.alphanumeric}",
                  "tags": [
                    {
                      "custom_tags": [
                        {
                          "key": "City",
                          "value": "Amsterdam"
                        }
                      ]
                    }
                  ]
                }
        argumentDocs:
            auto_stop_mins: '- Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.'
            channel: 'block, consisting of following fields:'
            channel.name: '- Name of the Databricks SQL release channel. Possible values are: CHANNEL_NAME_PREVIEW and CHANNEL_NAME_CURRENT. Default is CHANNEL_NAME_CURRENT.'
            cluster_size: '- (Required) The size of the clusters allocated to the endpoint: "2X-Small", "X-Small", "Small", "Medium", "Large", "X-Large", "2X-Large", "3X-Large", "4X-Large".'
            creator_name: '- The username of the user who created the endpoint.'
            data_source_id: '- ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.'
            databricks_sql_access: on databricks_group or databricks_user.
            enable_photon: '- Whether to enable Photon. This field is optional and is enabled by default.'
            enable_serverless_compute: '- Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.'
            "false": for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to true if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated terms of use, workspace admins are prompted in the Databricks SQL UI. A workspace must meet the requirements and might require an update to its instance profile role to add a trust relationship.
            health: '- Health status of the endpoint.'
            id: '- the unique ID of the SQL warehouse.'
            jdbc_url: '- JDBC connection string.'
            max_num_clusters: '- Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to 1.'
            min_num_clusters: '- Minimum number of clusters available when a SQL warehouse is running. The default is 1.'
            name: '- (Required) Name of the SQL warehouse. Must be unique.'
            num_active_sessions: '- The current number of clusters used by the endpoint.'
            num_clusters: '- The current number of clusters used by the endpoint.'
            odbc_params: '- ODBC connection params: odbc_params.hostname, odbc_params.path, odbc_params.protocol, and odbc_params.port.'
            spot_instance_policy: '- The spot policy to use for allocating instances to clusters: COST_OPTIMIZED or RELIABILITY_OPTIMIZED. This field is optional. Default is COST_OPTIMIZED.'
            state: '- The current state of the endpoint.'
            tags: '- Databricks tags all endpoint resources with these tags.'
            warehouse_type: '- SQL warehouse type. See for AWS or Azure. Set to PRO or CLASSIC. If the field enable_serverless_compute has the value true either explicitly or through the default logic (see that field above for details), the default is PRO, which is required for serverless SQL warehouses. Otherwise, the default is CLASSIC.'
        importStatements: []
    databricks_sql_global_config:
        subCategory: Databricks SQL
        name: databricks_sql_global_config
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "data_access_config": {
                    "spark.sql.session.timeZone": "UTC"
                  },
                  "instance_profile_arn": "arn:....",
                  "security_policy": "DATA_ACCESS_CONTROL"
                }
            - name: this
              manifest: |-
                {
                  "data_access_config": {
                    "spark.hadoop.fs.azure.account.auth.type": "OAuth",
                    "spark.hadoop.fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
                    "spark.hadoop.fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/${var.tenant_id}/oauth2/token",
                    "spark.hadoop.fs.azure.account.oauth2.client.id": "${var.application_id}",
                    "spark.hadoop.fs.azure.account.oauth2.client.secret": "{{secrets/${local.secret_scope}/${local.secret_key}}}"
                  },
                  "security_policy": "DATA_ACCESS_CONTROL",
                  "sql_config_params": {
                    "ANSI_MODE": "true"
                  }
                }
        argumentDocs:
            data_access_config: (Optional, Map) - Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the documentation for a full list.  Apply will fail if you're specifying not permitted configuration.
            google_service_account: (Optional, String) - used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.
            instance_profile_arn: (Optional, String) - databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.
            security_policy: '(Optional, String) - The policy for controlling access to datasets. Default value: DATA_ACCESS_CONTROL, consult documentation for list of possible values'
            sql_config_params: (Optional, Map) - SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.
        importStatements: []
    databricks_sql_permissions:
        subCategory: Security
        name: databricks_sql_permissions
        title: ""
        examples:
            - name: foo_table
              manifest: |-
                {
                  "cluster_id": "${databricks_cluster.cluster_name.id}"
                }
              references:
                cluster_id: databricks_cluster.cluster_name.id
            - name: foo_table
              manifest: |-
                {
                  "privilege_assignments": [
                    {
                      "principal": "serge@example.com",
                      "privileges": [
                        "SELECT",
                        "MODIFY"
                      ]
                    },
                    {
                      "principal": "special group",
                      "privileges": [
                        "SELECT"
                      ]
                    }
                  ],
                  "table": "foo"
                }
        argumentDocs:
            anonymous function/: '- anonymous function. / suffix is mandatory.'
            anonymous_function: '- (Boolean) If this access control for using anonymous function. Defaults to false.'
            any file/: '- direct access to any file. / suffix is mandatory.'
            any_file: '- (Boolean) If this access control for reading/writing any file. Defaults to false.'
            catalog: '- (Boolean) If this access control for the entire catalog. Defaults to false.'
            catalog/: '- entire catalog. / suffix is mandatory.'
            database: '- Name of the database. Has default value of default.'
            database/bar: '- bar database.'
            privilege_assignments.CREATE: '- gives the ability to create an object (for example, a table in a database).'
            privilege_assignments.CREATE_NAMED_FUNCTION: '- gives the ability to create a named UDF in an existing catalog or database.'
            privilege_assignments.MODIFY: '- gives the ability to add, delete, and modify data to or from an object.'
            privilege_assignments.MODIFY_CLASSPATH: '- gives the ability to add files to the Spark class path.'
            privilege_assignments.READ_METADATA: '- gives the ability to view an object and its metadata.'
            privilege_assignments.SELECT: '- gives read access to an object.'
            privilege_assignments.USAGE: '- do not give any abilities, but is an additional requirement to perform any action on a database object.'
            privilege_assignments.principal: '- display_name for a databricks_group or databricks_user, application_id for a databricks_service_principal.'
            privilege_assignments.privileges: '- set of available privilege names in upper case.'
            table: '- Name of the table. Can be combined with database.'
            table/default.foo: '- table foo in a default database. Database is always mandatory.'
            view: '- Name of the view. Can be combined with database.'
            view/bar.foo: '- view foo in bar database.'
        importStatements: []
    databricks_sql_query:
        subCategory: Databricks SQL
        name: databricks_sql_query
        title: ""
        examples:
            - name: q1
              manifest: |-
                {
                  "data_source_id": "${databricks_sql_endpoint.example.data_source_id}",
                  "name": "My Query Name",
                  "parameter": [
                    {
                      "name": "p1",
                      "text": [
                        {
                          "value": "default"
                        }
                      ],
                      "title": "Title for p1"
                    },
                    {
                      "enum": [
                        {
                          "multiple": [
                            {
                              "prefix": "\"",
                              "separator": ",",
                              "suffix": "\""
                            }
                          ],
                          "options": [
                            "default",
                            "foo",
                            "bar"
                          ],
                          "value": "default"
                        }
                      ],
                      "name": "p2",
                      "title": "Title for p2"
                    },
                    {
                      "date": [
                        {
                          "value": "2022-01-01"
                        }
                      ],
                      "name": "p3",
                      "title": "Title for p3"
                    }
                  ],
                  "parent": "folders/${databricks_directory.shared_dir.object_id}",
                  "query": "                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n",
                  "run_as_role": "viewer",
                  "tags": [
                    "t1",
                    "t2"
                  ]
                }
              references:
                data_source_id: databricks_sql_endpoint.example.data_source_id
              dependencies:
                databricks_directory.shared_dir: |-
                    {
                      "path": "/Shared/Queries"
                    }
        argumentDocs:
            data_source_id: '- Data source ID of a SQL warehouse'
            description: '- General description that conveys additional information about this query such as usage notes.'
            id: '- the unique ID of the SQL Query.'
            name: '- The title of this query that appears in list views, widget headings, and on the query page.'
            parent: '- The identifier of the workspace folder containing the object.'
            query: '- The text of the query to be run.'
            run_as_role: '- Run as role. Possible values are viewer, owner.'
            text.value: '- The default value for this parameter.'
            title: '- The text displayed in a parameter picking widget.'
        importStatements: []
    databricks_sql_table:
        subCategory: Unity Catalog
        name: databricks_sql_table
        title: ""
        examples:
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "data_source_format": "DELTA",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "",
                  "table_type": "MANAGED"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing_view
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "cluster_id": "0423-201305-xsrt82qn",
                  "comment": "this view is managed by terraform",
                  "name": "quickstart_table_view",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "VIEW",
                  "view_definition": "${format(\"SELECT name FROM %s WHERE id == 1\", databricks_sql_table.thing.id)}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.id}",
                      "comment": "this database is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
            - name: thing
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "column": [
                    {
                      "name": "id",
                      "type": "int"
                    },
                    {
                      "comment": "name of thing",
                      "name": "name",
                      "type": "string"
                    }
                  ],
                  "comment": "this table is managed by terraform",
                  "data_source_format": "DELTA",
                  "name": "quickstart_table",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "",
                  "table_type": "MANAGED",
                  "warehouse_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
                warehouse_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "2X-Small",
                      "max_num_clusters": 1,
                      "name": "endpoint"
                    }
            - name: thing_view
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "comment": "this view is managed by terraform",
                  "name": "quickstart_table_view",
                  "provider": "${databricks.workspace}",
                  "schema_name": "${databricks_schema.things.name}",
                  "table_type": "VIEW",
                  "view_definition": "${format(\"SELECT name FROM %s WHERE id == 1\", databricks_sql_table.thing.id)}",
                  "warehouse_id": "${databricks_sql_endpoint.this.id}"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                provider: databricks.workspace
                schema_name: databricks_schema.things.name
                warehouse_id: databricks_sql_endpoint.this.id
              dependencies:
                databricks_sql_endpoint.this: |-
                    {
                      "cluster_size": "2X-Small",
                      "max_num_clusters": 1,
                      "name": "endpoint"
                    }
        argumentDocs:
            catalog_name: '- Name of parent catalog. Change forces creation of a new resource.'
            cluster_id: '- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a cluster_id is specified, it will be used to execute SQL commands to manage this table. If empty, a cluster will be created automatically with the name terraform-sql-table.'
            cluster_keys: '- (Optional) a subset of columns to liquid cluster the table by. Conflicts with partitions.'
            comment: '- (Optional) User-supplied free-form text. Changing comment is not currently supported on VIEW table_type.'
            data_source_format: '- (Optional) External tables are supported in multiple data source formats. The string constants identifying these formats are DELTA, CSV, JSON, AVRO, PARQUET, ORC, TEXT. Change forces creation of a new resource. Not supported for MANAGED tables or VIEW.'
            id: '- ID of this table in form of <catalog_name>.<schema_name>.<name>.'
            name: '- Name of table relative to parent catalog and schema. Change forces creation of a new resource.'
            nullable: '- (Optional) Whether field is nullable (Default: true)'
            options: '- (Optional) Map of user defined table options. Change forces creation of a new resource.'
            partitions: '- (Optional) a subset of columns to partition the table by. Change forces creation of a new resource. Conflicts with cluster_keys.'
            properties: '- (Optional) Map of table properties.'
            schema_name: '- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.'
            storage_credential_name: '- (Optional) For EXTERNAL Tables only: the name of storage credential to use. Change forces creation of a new resource.'
            storage_location: '- (Optional) URL of storage location for Table data (required for EXTERNAL Tables). Not supported for VIEW or MANAGED table_type.'
            table_type: '- Distinguishes a view vs. managed/external Table. MANAGED, EXTERNAL or VIEW. Change forces creation of a new resource.'
            type: '- Column type spec (with metadata) as SQL text. Not supported for VIEW table_type.'
            view_definition: '- (Optional) SQL text defining the view (for table_type == "VIEW"). Not supported for MANAGED or EXTERNAL table_type.'
            warehouse_id: '- (Optional) All table CRUD operations must be executed on a running cluster or SQL warehouse. If a warehouse_id is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with cluster_id.'
        importStatements: []
    databricks_sql_visualization:
        subCategory: Databricks SQL
        name: databricks_sql_visualization
        title: ""
        examples:
            - name: q1v1
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Table",
                  "options": "${jsonencode(\n    {\n      \"itemsPerPage\" : 25,\n      \"columns\" : [\n        {\n          \"name\" : \"p1\",\n          \"type\" : \"string\"\n          \"title\" : \"Parameter 1\",\n          \"displayAs\" : \"string\",\n        },\n        {\n          \"name\" : \"p2\",\n          \"type\" : \"string\"\n          \"title\" : \"Parameter 2\",\n          \"displayAs\" : \"link\",\n          \"highlightLinks\" : true,\n        }\n      ]\n    }\n  )}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "table"
                }
              references:
                query_id: databricks_sql_query.q1.id
            - name: q1v1
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Table",
                  "options": "${file(\"${path.module}/visualizations/q1v1.json\")}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "table"
                }
              references:
                query_id: databricks_sql_query.q1.id
            - name: q1v2
              manifest: |-
                {
                  "description": "Some Description",
                  "name": "My Chart",
                  "options": "${file(\"${path.module}/visualizations/q1v2.json\")}",
                  "query_id": "${databricks_sql_query.q1.id}",
                  "type": "chart"
                }
              references:
                query_id: databricks_sql_query.q1.id
        argumentDocs: {}
        importStatements: []
    databricks_sql_widget:
        subCategory: Databricks SQL
        name: databricks_sql_widget
        title: ""
        examples:
            - name: d1w1
              manifest: |-
                {
                  "dashboard_id": "${databricks_sql_dashboard.d1.id}",
                  "position": [
                    {
                      "pos_x": 0,
                      "pos_y": 0,
                      "size_x": 3,
                      "size_y": 4
                    }
                  ],
                  "text": "Hello! I'm a **text widget**!"
                }
              references:
                dashboard_id: databricks_sql_dashboard.d1.id
            - name: d1w2
              manifest: |-
                {
                  "dashboard_id": "${databricks_sql_dashboard.d1.id}",
                  "position": [
                    {
                      "pos_x": 3,
                      "pos_y": 0,
                      "size_x": 3,
                      "size_y": 4
                    }
                  ],
                  "visualization_id": "${databricks_sql_visualization.q1v1.id}"
                }
              references:
                dashboard_id: databricks_sql_dashboard.d1.id
                visualization_id: databricks_sql_visualization.q1v1.id
        argumentDocs: {}
        importStatements: []
    databricks_storage_credential:
        subCategory: Unity Catalog
        name: databricks_storage_credential
        title: ""
        examples:
            - name: external
              manifest: |-
                {
                  "aws_iam_role": [
                    {
                      "role_arn": "${aws_iam_role.external_data_access.arn}"
                    }
                  ],
                  "comment": "Managed by TF",
                  "name": "${aws_iam_role.external_data_access.name}"
                }
              references:
                aws_iam_role.role_arn: aws_iam_role.external_data_access.arn
                name: aws_iam_role.external_data_access.name
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
            - name: external_mi
              manifest: |-
                {
                  "azure_managed_identity": [
                    {
                      "access_connector_id": "${azurerm_databricks_access_connector.example.id}"
                    }
                  ],
                  "comment": "Managed identity credential managed by TF",
                  "name": "mi_credential"
                }
              references:
                azure_managed_identity.access_connector_id: azurerm_databricks_access_connector.example.id
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
            - name: external
              manifest: |-
                {
                  "databricks_gcp_service_account": [
                    {}
                  ],
                  "name": "the-creds"
                }
              dependencies:
                databricks_grants.external_creds: |-
                    {
                      "grant": [
                        {
                          "principal": "Data Engineers",
                          "privileges": [
                            "CREATE_EXTERNAL_TABLE"
                          ]
                        }
                      ],
                      "storage_credential": "${databricks_storage_credential.external.id}"
                    }
        argumentDocs:
            aws_iam_role.external_id: (output only) - The external ID used in role assumption to prevent confused deputy problem.
            aws_iam_role.role_arn: '- The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF'
            aws_iam_role.unity_catalog_iam_arn: (output only) - The Amazon Resource Name (ARN) of the AWS IAM user managed by Databricks. This is the identity that is going to assume the AWS IAM role.
            azure_managed_identity.access_connector_id: '- The Resource ID of the Azure Databricks Access Connector resource, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name.'
            azure_managed_identity.managed_identity_id: '- (Optional) The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name.'
            azure_service_principal.application_id: '- The application ID of the application registration within the referenced AAD tenant'
            azure_service_principal.client_secret: '- The client secret generated for the above app ID in AAD. This field is redacted on output'
            azure_service_principal.directory_id: '- The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application'
            databricks_gcp_service_account.email: (output only) - The email of the GCP service account created, to be granted access to relevant buckets.
            databricks_storage_credential: represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.
            force_destroy: '- (Optional) Delete storage credential regardless of its dependencies.'
            force_update: '- (Optional) Update storage credential regardless of its dependents.'
            id: '- ID of this storage credential - same as the name.'
            metastore_id: '- (Required for account-level) Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.'
            name: '- Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.'
            owner: '- (Optional) Username/groupname/sp application_id of the storage credential owner.'
            read_only: '- (Optional) Indicates whether the storage credential is only usable for read operations.'
            skip_validation: '- (Optional) Suppress validation errors if any & force save the storage credential.'
        importStatements: []
    databricks_system_schema:
        subCategory: Unity Catalog
        name: databricks_system_schema
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "schema": "access"
                }
        argumentDocs:
            id: '- the ID of system schema in form of metastore_id|schema_name.'
            schema: '- (Required) Full name of the system schema.'
            state: '- The current state of enablement for the system schema.'
        importStatements: []
    databricks_token:
        subCategory: Security
        name: databricks_token
        title: ""
        examples:
            - name: pat
              manifest: |-
                {
                  "comment": "Terraform Provisioning",
                  "lifetime_seconds": 8640000,
                  "provider": "${databricks.created_workspace}"
                }
              references:
                provider: databricks.created_workspace
            - name: pat
              manifest: |-
                {
                  "comment": "Terraform (created: ${time_rotating.this.rfc3339})",
                  "lifetime_seconds": "${60 * 24 * 60 * 60}"
                }
              dependencies:
                time_rotating.this: |-
                    {
                      "rotation_days": 30
                    }
        argumentDocs:
            comment: '- (Optional) (String) Comment that will appear on the user’s settings page for this token.'
            id: '- Canonical unique identifier for the token.'
            lifetime_seconds: '- (Optional) (Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.'
            token_value: '- Sensitive value of the newly-created token.'
        importStatements: []
    databricks_user:
        subCategory: Security
        name: databricks_user
        title: ""
        examples:
            - name: me
              manifest: |-
                {
                  "user_name": "me@example.com"
                }
            - name: me
              manifest: |-
                {
                  "user_name": "me@example.com"
                }
              dependencies:
                databricks_group_member.i-am-admin: |-
                    {
                      "group_id": "${data.databricks_group.admins.id}",
                      "member_id": "${databricks_user.me.id}"
                    }
            - name: me
              manifest: |-
                {
                  "allow_cluster_create": true,
                  "display_name": "Example user",
                  "user_name": "me@example.com"
                }
            - name: account_user
              manifest: |-
                {
                  "display_name": "Example user",
                  "provider": "${databricks.mws}",
                  "user_name": "me@example.com"
                }
              references:
                provider: databricks.mws
            - name: account_user
              manifest: |-
                {
                  "display_name": "Example user",
                  "provider": "${databricks.azure_account}",
                  "user_name": "me@example.com"
                }
              references:
                provider: databricks.azure_account
        argumentDocs:
            acl_principal_id: '- identifier for use in databricks_access_control_rule_set, e.g. users/mr.foo@example.com.'
            active: '- (Optional) Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.'
            allow_cluster_create: '-  (Optional) Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and cluster_id argument. Everyone without allow_cluster_create argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.'
            allow_instance_pool_create: '-  (Optional) Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks_permissions and instance_pool_id argument.'
            databricks_sql_access: '- (Optional) This is a field to allow the group to have access to Databricks SQL feature in User Interface and through databricks_sql_endpoint.'
            disable_as_user_deletion: '- (Optional) When deleting a user, set the user''s active flag to false instead of actually deleting the user. This flag is exclusive to force_delete_repos and force_delete_home_dir flags. True by default for accounts SCIM API, false otherwise.'
            display_name: '- (Optional) This is an alias for the username that can be the full name of the user.'
            external_id: '- (Optional) ID of the user in an external identity provider.'
            force: '- (Optional) Ignore cannot create user: User with username X already exists errors and implicitly import the specific user into Terraform state, enforcing entitlements defined in the instance of resource. This functionality is experimental and is designed to simplify corner cases, like Azure Active Directory synchronisation.'
            force_delete_home_dir: '- (Optional) This flag determines whether the user''s home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.'
            force_delete_repos: '- (Optional) This flag determines whether the user''s repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.'
            home: '- Home folder of the user, e.g. /Users/mr.foo@example.com.'
            id: '- Canonical unique identifier for the user.'
            repos: '- Personal Repos location of the user, e.g. /Repos/mr.foo@example.com.'
            user_name: '- (Required) This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.'
        importStatements: []
    databricks_user_role:
        subCategory: Security
        name: databricks_user_role
        title: ""
        examples:
            - name: my_user_role
              manifest: |-
                {
                  "role": "${databricks_instance_profile.instance_profile.id}",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                role: databricks_instance_profile.instance_profile.id
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_instance_profile.instance_profile: |-
                    {
                      "instance_profile_arn": "my_instance_profile_arn"
                    }
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
            - name: my_user_account_admin
              manifest: |-
                {
                  "role": "account_admin",
                  "user_id": "${databricks_user.my_user.id}"
                }
              references:
                user_id: databricks_user.my_user.id
              dependencies:
                databricks_user.my_user: |-
                    {
                      "user_name": "me@example.com"
                    }
        argumentDocs:
            id: '- The id in the format <user_id>|<role>.'
            role: '-  (Required) Either a role name or the ARN/ID of the instance profile resource.'
            user_id: '- (Required) This is the id of the user resource.'
        importStatements: []
    databricks_vector_search_endpoint:
        subCategory: Vector Search
        name: databricks_vector_search_endpoint
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "endpoint_type": "STANDARD",
                  "name": "vector-search-test"
                }
        argumentDocs:
            creation_timestamp: '- Timestamp of endpoint creation (milliseconds).'
            creator: '- Creator of the endpoint.'
            endpoint_id: '- Unique internal identifier of the endpoint (UUID).'
            endpoint_status: '- Object describing the current status of the endpoint consisting of following fields:'
            endpoint_type: '(Required) type of Vector Search Endpoint.  Currently only accepting single value: STANDARD (See documentation for the list of currently supported values).  If it''s changed, Vector Search Endpoint is recreated.'
            id: '- The same as the name of the endpoint.'
            last_updated_timestamp: '- Timestamp of last update to the endpoint (milliseconds).'
            last_updated_user: '- User who last updated the endpoint.'
            message: '- Additional status message.'
            name: '- (Required) Name of the Vector Search Endpoint to create.  If name is changed, Vector Search Endpoint is recreated.'
            num_indexes: '- Number of indexes on the endpoint.'
            state: '- Current state of the endpoint. Currently following values are supported: PROVISIONING, ONLINE, OFFLINE.'
        importStatements: []
    databricks_volume:
        subCategory: Unity Catalog
        name: databricks_volume
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "catalog_name": "${databricks_catalog.sandbox.name}",
                  "comment": "this volume is managed by terraform",
                  "name": "quickstart_volume",
                  "schema_name": "${databricks_schema.things.name}",
                  "storage_location": "${databricks_external_location.some.url}",
                  "volume_type": "EXTERNAL"
                }
              references:
                catalog_name: databricks_catalog.sandbox.name
                schema_name: databricks_schema.things.name
                storage_location: databricks_external_location.some.url
              dependencies:
                databricks_catalog.sandbox: |-
                    {
                      "comment": "this catalog is managed by terraform",
                      "name": "sandbox",
                      "properties": {
                        "purpose": "testing"
                      }
                    }
                databricks_external_location.some: |-
                    {
                      "credential_name": "${databricks_storage_credential.external.name}",
                      "name": "external-location",
                      "url": "s3://${aws_s3_bucket.external.id}/some"
                    }
                databricks_schema.things: |-
                    {
                      "catalog_name": "${databricks_catalog.sandbox.name}",
                      "comment": "this schema is managed by terraform",
                      "name": "things",
                      "properties": {
                        "kind": "various"
                      }
                    }
                databricks_storage_credential.external: |-
                    {
                      "aws_iam_role": [
                        {
                          "role_arn": "${aws_iam_role.external_data_access.arn}"
                        }
                      ],
                      "name": "creds"
                    }
        argumentDocs:
            <catalogName>: ': The name of the catalog containing the Volume.'
            <schemaName>: ': The name of the schema containing the Volume.'
            <volumeName>: ': The name of the Volume. It identifies the volume object.'
            catalog_name: '- Name of parent Catalog. Change forces creation of a new resource.'
            comment: '- (Optional) Free-form text.'
            id: '- ID of this Unity Catalog Volume in form of <catalog>.<schema>.<name>.'
            name: '- Name of the Volume'
            owner: '- (Optional) Name of the volume owner.'
            schema_name: '- Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.'
            storage_location: '- (Optional) Path inside an External Location. Only used for EXTERNAL Volumes. Change forces creation of a new resource.'
            volume_path: '- base file path for this Unity Catalog Volume in form of /Volumes/<catalog>/<schema>/<name>.'
            volume_type: '- Volume type. EXTERNAL or MANAGED. Change forces creation of a new resource.'
        importStatements: []
    databricks_workspace_conf:
        subCategory: Workspace
        name: databricks_workspace_conf
        title: ""
        examples:
            - name: this
              manifest: |-
                {
                  "custom_config": {
                    "enableIpAccessLists": true
                  }
                }
        argumentDocs:
            custom_config: '- (Required) Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with enable or enforce will be reset to false value, regardless of initial default one.'
            enableDeprecatedClusterNamedInitScripts: '- (boolean) Enable or disable legacy cluster-named init scripts for this workspace.'
            enableDeprecatedGlobalInitScripts: '- (boolean) Enable or disable legacy global init scripts for this workspace.'
            enableIpAccessLists: '- enables the use of databricks_ip_access_list resources'
            enableTokensConfig: '- (boolean) Enable or disable personal access tokens for this workspace.'
            maxTokenLifetimeDays: '- (string) Maximum token lifetime of new tokens in days, as an integer. If zero, new tokens are permitted to have no lifetime limit. Negative numbers are unsupported. WARNING: This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set.'
        importStatements: []
    databricks_workspace_file:
        subCategory: Workspace
        name: databricks_workspace_file
        title: ""
        examples:
            - name: module
              manifest: |-
                {
                  "path": "${data.databricks_current_user.me.home}/AA/BB/CC",
                  "source": "${path.module}/module.py"
                }
            - name: init_script
              manifest: |-
                {
                  "content_base64": "${base64encode(\u003c\u003c-EOT\n    #!/bin/bash\n    echo \"Hello World\"\n    EOT\n  )}",
                  "path": "/Shared/init-script.sh"
                }
        argumentDocs:
            content_base64: '- The base64-encoded file content. Conflicts with source. Use of content_base64 is discouraged, as it''s increasing memory footprint of Terraform state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline.'
            id: '-  Path of workspace file'
            object_id: '-  Unique identifier for a workspace file'
            path: '-  (Required) The absolute path of the workspace file, beginning with "/", e.g. "/Demo".'
            source: '- Path to file on local filesystem. Conflicts with content_base64.'
            url: '- Routable URL of the workspace file'
            workspace_path: '- path on Workspace File System (WSFS) in form of /Workspace + path'
        importStatements: []
    default_namespace_settings:
        subCategory: Settings
        name: default_namespace_settings
        title: ""
        argumentDocs:
            namespace: '- (Required) The configuration details.'
            value: '- (Required) The value for the setting.'
        importStatements: []
